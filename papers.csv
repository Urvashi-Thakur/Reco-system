1582583,MARS: Link-layer rate selection for multicast transmissions in wireless mesh networks.,"IEEE 802.11 devices dynamically choose among different modulation schemes and bit-rates for frame transmissions. This rate adaptation, however, is restricted only to unicast frames. Multicast (and broadcast) frames are constrained to use a fixed low bit-rate modulation, resulting in low throughput for multicast streams. Availability of high bandwidth and efficient use of the medium is crucial to support multimedia multicast streaming applications such as IPTV, especially in multihop mesh networks. To address this problem, we propose a rate adaptation algorithm for multicast transmissions in these networks. The proposed algorithm, MARS, is distributed in nature, and relies on local network measurements to select a transmission bit-rate for a given multicast group. The algorithm also facilitates the joint use of bit-rate selection and link-layer mechanisms such as acknowledgements and retransmissions to improve reliability of high throughput multicast streams. Based on implementation and evaluation on a testbed, the algorithm provides up to 600% gain in throughput compared to traditional 802.11 networks in some scenarios. Additionally, the algorithm can support multicast streams while consuming a small fraction (20%) of the resources compared to the basic 802.11 operation.",2011,1582583,"Prashanth Aravinda Kumar Acharya,Elizabeth M. Belding",0
1582584,Optimal physical carrier sense in wireless networks.,"We investigate the problem of maximizing Medium Access Control (MAC) throughput in Carrier Sense Multiple Access (CSMA) wireless networks. By explicitly incorporating the carrier sense threshold and the transmit power into our analysis, we derive an analytical relation between MAC throughput and system parameters. In homogeneous networks, we derive the optimal carrier sense range at a given node density as a function of the ratio between the transmit power and the carrier sense threshold. The obtained optimal carrier sense range is smaller than that for covering the entire interference range, which is in sharp contrast to what has been considered to be optimal in previous studies. Only when the node density goes to infinity, the optimal carrier sense range converges to that for exactly covering the interference range, thereby eliminating all the hidden nodes. For nonhomogeneous networks, any distributed algorithm for tuning the carrier sense threshold, in which each node tries to maximize its own throughput without coordination, may significantly degrade MAC throughput. In order to properly design a distributed algorithm, each node not only considers its own throughput, but also needs to take account of its adverse impact on others. Our analysis is verified by simulation studies under various network scenarios.",2011,1582584,"Kyung-Joon Park,Jihyuk Choi,Jennifer C. Hou,Yih-Chun Hu,Hyuk Lim",0
1582589,MC-LMAC: A multi-channel MAC protocol for wireless sensor networks.,"In traditional wireless sensor network (WSN) applications, energy efficiency may be considered to be the most important concern whereas utilizing bandwidth and maximizing throughput are of secondary importance. However, recent applications, such as structural health monitoring, require high amounts of data to be collected at a faster rate. We present a multi-channel MAC protocol, MC-LMAC, designed with the objective of maximizing the throughput of WSNs by coordinating transmissions over multiple frequency channels. MC-LMAC takes advantage of interference and contention-free parallel transmissions on different channels. It is based on scheduled access which eases the coordination of nodes, dynamically switching their interfaces between channels and makes the protocol operate effectively with no collisions during peak traffic. Time is slotted and each node is assigned the control over a time slot to transmit on a particular channel. We analyze the performance of MC-LMAC with extensive simulations in Glomosim. MC-LMAC exhibits significant bandwidth utilization and high throughput while ensuring an energy-efficient operation. Moreover, MC-LMAC outperforms the contention-based multi-channel MMSN protocol, a cluster-based channel assignment method, and the single-channel CSMA in terms of data delivery ratio and throughput for high data rate, moderate-size networks of 100 nodes at different densities.",2011,1582589,"Özlem Durmaz Incel,Lodewijk van Hoesel,Pierre G. Jansen,Paul J. M. Havinga",0
1582591,Robust and flexible Internet connectivity for mobile ad hoc networks.,"An important challenge for the wider adoption of mobile ad hoc network (MANET) technologies is finding ways to efficiently interconnect them with the Internet. However, such interconnections prove difficult due to differences in mobility, addressing and routing between MANETs and existing IP networks. In this paper, we review the existing solutions to interconnect MANETs with the Internet, but find them lacking in robustness and flexibility. For instance, many solutions do not consider the presence of multiple gateways, and in such scenarios they either fail, or are less efficient due to the lack of multi-homing capabilities. A key insight of ours is that the reason for routing failure is usually an interconnection scheme's inability to express indirection (i.e., a way to enforce routing through a certain gateway on the path toward a destination in the Internet). Another problem concerns state replication where a route update fails to replicate all the routing state needed to forward packets to an Internet gateway. We analyze the above problems and suggest a solution that provides robust and flexible Internet connectivity. With minor adaptions our solution works for any MANET routing protocol, and has support for multiple gateways and multi-homing. Simulations show that, when used in combination with AODV routing, our solution provides up to 20% delivery ratio improvement over one of the main alternatives. A prototype implementation illustrates the feasibility of our solution in the real world.",2011,1582591,"Erik Nordström,Per Gunningberg,Christian F. Tschudin",0
1582593,Performance evaluation of distributed localization techniques for mobile underwater acoustic sensor networks.,"Underwater sensor networks (USN) are used for tough oceanographic missions where human operation is dangerous or impossible. In the common mobile USN architecture, sensor nodes freely float several meters below the surface and move with the force of currents. One of the significant challenges of the mobile USN is localization. In this paper, we compare the performance of three localization techniques; Dive and Rise Localization (DNRL), Proxy Localization (PL) and Large-Scale Localization (LSL). DNRL, PL and LSL are distributed, range-based localization schemes and they are suitable for large-scale, three dimensional, mobile USNs. Our simulations show that, DNRL and LSL can localize more than 90% of the underwater nodes with high accuracy while LSL has higher energy consumption and higher overhead than DNRL. The localization success and accuracy of PL is lower than the other techniques however it can localize underwater nodes faster when small number of beacons are employed.",2011,1582593,"Melike Erol-Kantarci,Sema F. Oktug,Luiz Filipe M. Vieira,Mario Gerla",0
1582602,Multipath optimized link state routing for mobile ad hoc networks.,"Multipath routing protocols for Mobile Ad hoc NETwork (MANET) address the problem of scalability, security (confidentiality and integrity), lifetime of networks, instability of wireless transmissions, and their adaptation to applications. Our protocol, called MultiPath OLSR (MP-OLSR), is a multipath routing protocol based on OLSR [1]. The Multipath Dijkstra Algorithm is proposed to obtain multiple paths. The algorithm gains great flexibility and extensibility by employing different link metrics and cost functions. In addition, route recovery and loop detection are implemented in MP-OLSR in order to improve quality of service regarding OLSR. The backward compatibility with OLSR based on IP source routing is also studied. Simulation based on Qualnet simulator is performed in different scenarios. A testbed is also set up to validate the protocol in real world. The results reveal that MP-OLSR is suitable for mobile, large and dense networks with large traffic, and could satisfy critical multimedia applications with high on time constraints.",2011,1582602,"Jiazi Yi,Asmaa Adnane,Sylvain David,Benoît Parrein",0
1582643,Lattice routing: A 4D routing scheme for multiradio multichannel ad hoc networks.,"An efficient channel assignment strategy ensures capacity maximization in a multiradio, multichannel ad hoc network. Existing mechanisms either use a static channel assignment or a centralized process intensive system that assigns channels to individual nodes. These are not effective in a dynamic environment with multiple flows that are active at different time instants. The protocol proposed in this work (Lattice routing) manages channels of the radios for the different nodes in the network using information about current channel conditions and adapts itself to varying traffic patterns in order to efficiently use the multiple channels. Further the protocol uses multipathing, a key mechanism that is found to alleviate bottlenecks present in single path routes in such an environment. Results indicate that Lattice routing consistently outperforms it closest competitor ((MCR) Kyasanur and Vaidya (2006) [1]) across a large number of experiments.",2011,1582643,"Sandeep Kakumanu,Stephan Eidenbenz,Raghupathy Sivakumar",0
1583920,Artificial Bee Colony (ABC) for multi-objective design optimization of composite structures.,"In this paper, we present a generic method/model for multi-objective design optimization of laminated composite components, based on Vector Evaluated Artificial Bee Colony (VEABC) algorithm. VEABC is a parallel vector evaluated type, swarm intelligence multi-objective variant of the Artificial Bee Colony algorithm (ABC). In the current work a modified version of VEABC algorithm for discrete variables has been developed and implemented successfully for the multi-objective design optimization of composites. The problem is formulated with multiple objectives of minimizing weight and the total cost of the composite component to achieve a specified strength. The primary optimization variables are the number of layers, its stacking sequence (the orientation of the layers) and thickness of each layer. The classical lamination theory is utilized to determine the stresses in the component and the design is evaluated based on three failure criteria: failure mechanism based failure criteria, maximum stress failure criteria and the tsai-wu failure criteria. The optimization method is validated for a number of different loading configurations-uniaxial, biaxial and bending loads. The design optimization has been carried for both variable stacking sequences, as well fixed standard stacking schemes and a comparative study of the different design configurations evolved has been presented. Finally the performance is evaluated in comparison with other nature inspired techniques which includes Particle Swarm Optimization (PSO), Artificial Immune System (AIS) and Genetic Algorithm (GA). The performance of ABC is at par with that of PSO, AIS and GA for all the loading configurations.",2011,1583920,"S. N. Omkar,J. Senthilnath,Rahul Khandelwal,G. Narayana Naik,S. Gopalakrishnan",0
1583949,SOMSE: A semantic map based meta-search engine for the purpose of web information customization.,"To combat information overload, systems that are often referred to as information customization systems are needed. Such systems act on the user's behalf and can rely on existing information services like search engines that do the resource-intensive part of the work. These systems will be sufficiently lightweight to run on an average PC and serve as personal assistants. Since such an assistant has relatively modest resource requirements it can reside on an individual user's machine. If the assistant resides on the user's machine, there is no need to turn down intelligence. The system can have substantial local intelligence. In this paper, we propose an information customization system that combines meta-search and unsupervised learning. A meta-search engine simultaneously searches multiple search engines and returns a single list of results. The results retrieved by this engine can be highly relevant, since it is usually grabbing the first items from the relevancy-ranked list of hits returned by the individual search engines. The Kohonen Feature Map is then used to construct a self-organizing semantic map such that documents of similar contents are placed close to one another.",2011,1583949,Mohamed Salah Hamdi,0
1583955,A cooperative immunological approach for detecting network anomaly.,"Technology and biological systems have now bi-directional relation that each benefits from the other. Biological systems naturally enjoy many attractive features and inherent intelligence that fit in solving many research problems. The natural immune system as one of those biological systems is considered a good source of inspiration to artificial defense systems. It has its own intelligent mechanisms to detect the foreign bodies and fight them and without it, an individual cannot live, even just for several days. The new types of network attacks evolved and became more complex, severe and hard to detect. This resulted in increasing need for network defense systems, and especially those with unordinary approaches or with ability to face the dynamic nature of new and continuously changing network threats. In this work we investigate different AIS theories and show how to combine different ideas to solve problems of network security domain. An Intrusion Detection System (IDS) that apply those ideas was built and tested in a real-time environment to test the pros and cons of Artificial Immune System (AIS) and clarify its applicability. Also some investigation on the vaccination biological process is introduced. A special module was built to perform this process and check its usage and how it could be formulated in artificial life.",2011,1583955,"Tarek S. Sobh,Wael M. Mostafa",0
1583960,Dynamic reconstruction of nonlinear v-i characteristic in electric arc furnaces using adaptive neuro-fuzzy rule-based networks.,"This paper presents an application of adaptive neuro-fuzzy networks which dynamically reconstructs the model of nonlinear v-i characteristic in electric arc furnaces. Electric arc furnaces represent complex, multi-variable processes with time-variant parameters, and their effective modeling is a challenging task. This paper shows that adaptive neuro-fuzzy networks lend themselves well to nonlinear black-box modeling of v-i behavior of electric arc furnaces. A successful implementation is described, and its performance is illustrated in comparison to measurements from an operational furnace.",2011,1583960,"A. R. Sadeghian,J. D. Lavers",0
1583961,A type-2 neuro-fuzzy system based on clustering and gradient techniques applied to system identification and channel equalization.,"The integration of fuzzy systems and neural networks has recently become a popular approach in engineering fields for modelling and control of uncertain systems. This paper presents the development of novel type-2 neuro-fuzzy system for identification of time-varying systems and equalization of time-varying channels using clustering and gradient algorithms. It combines the advantages of type-2 fuzzy systems and neural networks. The type-2 fuzzy system allows handling the uncertainties associated with information or data in the knowledge base of the process. The structure of the proposed type-2 TSK fuzzy neural system (FNS) is given and its parameter update rule is derived, based on fuzzy clustering and gradient learning algorithm. The proposed structure is used for identification and noise equalization of time-varying systems. The effectiveness of the proposed system is evaluated by comparing the results obtained by the use of models seen in the literature.",2011,1583961,"Rahib Hidayat Abiyev,Okyay Kaynak,Tayseer Alshanableh,Fakhreddin Mamedov",0
1583963,Using a hybrid MCDM model to evaluate firm environmental knowledge management in uncertainty.,"Environmental practice in knowledge management capability (EKMC) is a complex uncertainty concept that is difficult to determine based on a firm's real situation because measuring EKMC requires a set of qualitative and quantitative measurements. A framework is proposed and uses a novel hybrid multi-criteria decision-making (MCDM) model to address the dependence relationships of criteria with the aid of the analytical network process (ANP) and Decision-Making Trial and Evaluation Laboratory (DEMATEL) in uncertainty. Fuzzy set theory is used to interpret the linguistic information in accordance with the subjective evaluation; ANP is used to analyze the dependence aspects, while DEMATEL is used to determine the intertwined relations among the criteria. The evaluation results obtained through the proposed approach are objective and unbiased for two reasons. First, the results are generated by a group of experts in the presence of motile attributes, and second, the fuzzy linguistic approach reduces the distortion and loss of information. Managers can then judge the need to improve and determine which criteria provide the most effective direction towards improvement.",2011,1583963,Ming-Lang Tseng,0
1583964,Classification of material type and its surface properties using digital signal processing techniques and neural networks.,"A novel method for the classification of material type and its surface roughness by means of a lightweight plunger probe and optical mouse is presented in this paper. An experimental prototype was developed which involves bouncing or hopping of the plunger based impact probe freely on the plain surface of an object under test. The time and features of bouncing signal are related to the material type and its surface properties, and each material has a unique set of such properties. During the bouncing of the probe, a time varying signal is generated from optical mouse that is recorded in a data file on PC. Some dominant unique features are then extracted using digital signal processing tools to optimize neural network based classifier used in the existing system. The classifier is developed on the basis of application of supervised structures of neural networks. For this, an optimum Multilayer Perceptron Neural Network (MLP NN) model is designed to maximize accuracy under the constraints of minimum network dimension. Conjugate-gradient learning algorithm, which provides faster rate convergence, has been found suitable for the training of the MLP NN. The optimal parameters of MLP NN model based on various performance measures that also includes the receiver operating characteristics curve and classification accuracy on the testing data sets even after attempting different data partitions are determined. The classification accuracy of MLP NN is found reasonable consistently in respect of rigorous testing using different data partitions.",2011,1583964,"Nadir Nizar Ali Charniya,Sanjay V. Dudul",0
1583966,"JADE implemented mobile multi-agent based, distributed information platform for pervasive health care monitoring.","In this paper we aim to implement a highly distributed information infrastructure-MADIP by using Intelligent Agent paradigm, which is able to notify the responsible care-provider of abnormality automatically, offer distance medical advice, and perform continuous health monitoring for those who need it. To confront the issues of interoperability, scalability, and openness in heterogeneous e-health environments, a FIPA2000 standard compliant agent development platform-JADE (Java Agent DEvelopment Framework) was adopted for the design and implementation of the proposed intelligent multi-agent based MADIP system.",2011,1583966,"Chuan-Jun Su,Chia-Ying Wu",0
1583967,Differential evolution for solving the mobile location management.,"In this work we present two new approaches to solve the location management problem, respectively, based on the location areas and the reporting cells strategies. The location management problem corresponds to the management of the network configuration with the objective of minimizing the costs involved. We use the differential evolution algorithm to find the best configuration for the location areas and the reporting cells strategies, which principally considers the location update and paging costs. With this work we want to define the best values to the differential evolution configuration, using test networks and also realistic networks, as well as compare our results with the ones obtained by other authors. These two new approaches applied to this problem have given us very good results, when compared with those obtained by other authors.",2011,1583967,"Sónia M. Almeida-Luz,Miguel A. Vega-Rodríguez,Juan Antonio Gómez Pulido,Juan Manuel Sánchez-Pérez",0
1583968,Genetic learning of fuzzy integrals accumulating human-reported environmental stress.,"In this paper, we develop models based on fuzzy integrals (both of the Choquet and Sugeno type) for accumulating annoyance by noise, odor or light caused by particular sources or activities. As underlying fuzzy measures, we have opted for k-maxitive measures (in particular 1-maxitive or 2-maxitive) as the best known crisp model points in this direction. The fuzzy measures are learnt from survey data and optimized using genetic algorithms. Attention is paid to several types of inconsistencies that typically arise in data sets collected through social surveys. Also, special care is taken to make sure that the Sugeno integral and the genetic algorithm that optimizes the associated fuzzy measure operates solely on the ordinal scale of linguistic labels.",2011,1583968,"Andy Verkeyn,Dick Botteldooren,Bernard De Baets",0
1584001,Digital ants as the best cicerones for museum visitors.,"In this paper we address the problem of personalization and automatic generation of museum visits with mobile devices guidance. We propose here a ubiquitous software and hardware infrastructure that supports, in a collaborative way, the generation of museum handheld-based visits. Particularly, we define the problem to be solved as an instance of the Orienteering Problem. We present an implementation of a distributed algorithm in which every computing element of the ubiquitous infrastructure contributes to the generation of sub-optimal solutions that meet the user requirements. Finally, we discuss some experimental results that validate the proposed ideas and the results obtained by a questionnaire answered by real visitors. This questionnaire was filled in during the largest deployment of this technology in Spain in which 100 Personal Digital Assistants (PDAs) were made available to visitors as part of the exhibition ''Sorolla: Vision of Spain''.",2011,1584001,"Javier Jaén Martínez,José A. Mocholí,Alejandro Catalá,Elena Navarro",0
1584002,Radial basis function network based on time variant multi-objective particle swarm optimization for medical diseases diagnosis.,"This paper proposes an adaptive evolutionary radial basis function (RBF) network algorithm to evolve accuracy and connections (centers and weights) of RBF networks simultaneously. The problem of hybrid learning of RBF network is discussed with the multi-objective optimization methods to improve classification accuracy for medical disease diagnosis. In this paper, we introduce a time variant multi-objective particle swarm optimization (TVMOPSO) of radial basis function (RBF) network for diagnosing the medical diseases. This study applied RBF network training to determine whether RBF networks can be developed using TVMOPSO, and the performance is validated based on accuracy and complexity. Our approach is tested on three standard data sets from UCI machine learning repository. The results show that our approach is a viable alternative and provides an effective means to solve multi-objective RBF network for medical disease diagnosis. It is better than RBF network based on MOPSO and NSGA-II, and also competitive with other methods in the literature.",2011,1584002,"Sultan Noman Qasem,Siti Mariyam Hj. Shamsuddin",0
1584003,Improved fuzzy sliding mode control for a class of MIMO nonlinear uncertain and perturbed systems.,"In this paper, a stable adaptive fuzzy sliding mode based tracking control is developed for a class of nonlinear MIMO systems that are represented by input output models involving system uncertainties and external disturbances. The main contribution of the proposed method is that the structure of the controller system is partially unknown and does not require the bounds of uncertainties and disturbance to be known. First, a fuzzy logic system is designed to estimate the unknown function. Secondly, in order to eliminate the chattering phenomenon brought by the conventional variable structure control, the signum function is replaced by an adaptive Proportional Derivative (PD) term in the proposed approach. All parameter adaptive laws and robustifying control terms are derived based on Lyapunov stability analysis, so that convergence to zero of tracking errors and boudedness of all signals in the closed-loop system can be guaranteed. Finally, a mass-spring-damper system is simulated to demonstrate the validity and the effectiveness of the proposed controller.",2011,1584003,"S. Aloui,Olivier Pagès,Ahmed El Hajjaji,A. Chaari,Y. Koubaa",0
1584004,Complex generalized-mean neuron model and its applications.,"The key element of neurocomputing research in complex domain is the development of artificial neuron model with improved computational power and generalization ability. The non-linear activities in neuronal interactions are observed in biological neurons. This paper presents architecture of a neuron with a non-linear aggregation function for complex-valued signals. The proposed aggregation function is conceptually based on generalized mean of signals impinging on a neuron. This function is general enough and is capable of realizing various conventional aggregation functions as its special case. The generalized-mean neuron has a simpler structure and variation in the value of generalization parameter embraces higher order structure of a neuron. Hence, it can be used without the hassles of possible combinatorial explosion, as in higher order neurons. The superiority of proposed neuron based network over real and complex multilayer perceptron is demonstrated through variety of experiments.",2011,1584004,"Bipin K. Tripathi,B. Chandra,Menakshi Singh,Prem Kumar Kalra",0
1584006,Genetic algorithm based NARX model identification for evaluation of insulin sensitivity.,"The evaluation of insulin sensitivity plays an important role in the clinical investigation of glucose related diseases. Mathematical models based on non-invasive tests provide an estimate of insulin sensitivity by solving a nonlinear optimization problem. However traditional optimization methods suffer from convergence problem and the final estimate is heavily dependent on the initial parameterization. This paper proposes a model based on the hybrid approach of nonlinear autoregressive with exogenous input (NARX) modeling and genetic algorithm (GA) for deriving an index of insulin sensitivity. The model does not need an initial parameterization and the convergence is always guaranteed. The index derived from the proposed model is found to correlate well with the widely used minimal model based insulin sensitivity, with a significantly higher accuracy of fit.",2011,1584006,"S. Ghosh,S. Maka",0
1584007,LPAT: An interactive simulation tool for assessing the lightning performance of Hellenic high voltage transmission lines.,"The paper presents the Lightning Performance Assessment Tool (LPAT), an interactive simulation tool which assesses the lightning performance of high voltage transmission lines. The LPAT design was based on a recently published methodology that has been developed in order to assess the lightning performance of the Hellenic high voltage transmission lines. This methodology has been adapted to enable tool's users to enter a large dataset required for the computations eliminating the possibility of mistakes. Moreover, LPAT, written in a high-level programming language and using the appropriate user interface models and programming architecture, enables electrical engineers to adapt the level of granularity in which they prefer to examine the lightning performance of a transmission line. This approach is proved to be ideal for electrical systems such as the Hellenic transmission system whose individual characteristics depend upon the meteorological and geographical characteristics of the Hellenic territory. This tool is also flexible to any modification and change in order to support different transmission systems. The presented simulation tool is valuable for designers of electric power systems intending in a more effective lightning protection.",2011,1584007,"Panagiotis Karampelas,Lambros Ekonomou,S. Panetsos,G. E. Chatzarakis",0
1584008,A hybrid ANFIS model based on AR and volatility for TAIEX forecasting.,"Time series models have been applied to forecast stock index movements and make reasonably accurate predictions. There are, however, two major drawbacks of conventional time series models: (1) most conventional time series models use only one variable to forecast; and (2) the rules that are mined from artificial neural networks (ANNs) are not easily understandable. To solve these problems and enhance the forecasting performance of fuzzy time series models, this paper proposes a hybrid adaptive network-based fuzzy inference system (ANFIS) model that is based on AR and volatility to forecast stock price problems of the Taiwan stock exchange capitalization weighted stock index (TAIEX). To evaluate forecasting performance, the proposed model is compared with Chen's model and Yu's model. Our results indicate that the proposed model is superior to other methods with regard to root mean squared error (RMSE).",2011,1584008,"Jing-Rong Chang,Liang-Ying Wei,Ching-Hsue Cheng",0
1584009,Fuzzy clustering of human motor motion.,"Acquisition of the behavioural skills of a human operator and recreating them in an intelligent autonomous system has been a critical but rather challenging step in the development of complex intelligent autonomous systems. Development of a systematic and generic method for realising this process by acquiring human postural and motor movements is explored. This is achieved by breaking down the human motion into a number of segments called motion or skill primitives. The proposed methodology is developed based on studying the movement of the human hand. The motion is measured by a dual-axis accelerometer and a gyroscope mounted on the hand. The gyroscope locates the position and configuration of the hand, whereas the accelerometer measures the kinematics parameters of the movement. The covariance and the mean of the data produced by the sensors are used as features in the clustering process. A fuzzy clustering method is developed and applied to identify different movements of the human hand. The proposed clustering approach identifies the sequence of the motion primitives embedded in the data produced from the human wrist movement. A review of the previous work in the area is carried out and the developed methodology is described. An overview of the experimental setup and procedures to validate the approach is given. The results of the validation are analysed critically and some conclusions are drawn.",2011,1584009,Fazel Naghdy,0
1584010,Automatic lateral control for unmanned vehicles via genetic algorithms.,"It is known that the techniques under the topic of Soft Computing have a strong capability of learning and cognition, as well as a good tolerance to uncertainty and imprecision. Due to these properties they can be applied successfully to Intelligent Vehicle Systems; ITS is a broad range of technologies and techniques that hold answers to many transportation problems. The unmanned control of the steering wheel of a vehicle is one of the most important challenges facing researchers in this area. This paper presents a method to adjust automatically a fuzzy controller to manage the steering wheel of a mass-produced vehicle; to reach it, information about the car state while a human driver is handling the car is taken and used to adjust, via iterative genetic algorithms an appropriated fuzzy controller. To evaluate the obtained controllers, it will be considered the performance obtained in the track following task, as well as the smoothness of the driving carried out.",2011,1584010,"Enrique Onieva,José Eugenio Naranjo,Vicente Milanés,Javier Alonso,Ricardo García,Joshué Pérez",0
1584011,Location discovery in Wireless Sensor Networks using metaheuristics.,"Wireless Sensor Networks (WSN) monitor the physical world using small wireless devices known as sensor nodes, with high precision and in real time, without the intervention of a human operator. Location information plays a critical role in many of the applications where WSN are used. Though a simple and effective solution could be to equip every node with self-locating hardware such as a GPS, the resulting cost renders such a solution unefficient. A widely used self-locating mechanism consists in equipping a small subset of the nodes with some GPS-like hardware, while the rest of the nodes employ reference estimations (received signal strength, time-of-arrival, etc.) in order to determine their locations. The task of determining the node locations using node-to-node distances combined with a set of known node locations is referred to as location discovery (LD). The main difficulty found in LD is the presence of distance estimation errors, which result in node positioning errors. We describe in this work an error model for the estimations, and propose a two-stage search procedure that combines minimization of an error norm function with maximization of a maximum likelihood function to solve the problem. We perform an empirical study of the performance of several variants of the guiding functions, and several metaheuristics used to solve real LD problem instances. Finally, we test our proposed technique against the single phase techniques in order to evaluate its performance.",2011,1584011,"Guillermo Molina,Enrique Alba",0
1584012,A neural network-based sliding-mode control for rotating stall and surge in axial compressors.,"A decoupled sliding-mode neural network variable-bound control system (DSMNNVB) is proposed to control rotating stall and surge in jet engine compression systems in presence of disturbance and uncertainty. The control objective is to drive the system state to the original equilibrium point and it proves that the control system is asymptotically stable. In this controller, an adaptive neural network (NN) control scheme is employed for unknown dynamic of nonlinear plant without using a model of the plant. Moreover, no prior knowledge of the plant is assumed. The proposed DSMNNVB controller ensures Lyapunov stability of the nonlinear dynamic of the system.",2011,1584012,"J. Javadi Moghaddam,Mehrdad H. Farahani,Nima Amanifard",0
1584013,A multi-objective artificial immune algorithm for parameter optimization in support vector machine.,"Support vector machine (SVM) is a classification method based on the structured risk minimization principle. Penalize, C; and kernel, @s parameters of SVM must be carefully selected in establishing an efficient SVM model. These parameters are selected by trial and error or man's experience. Artificial immune system (AIS) can be defined as a soft computing method inspired by theoretical immune system in order to solve science and engineering problems. A multi-objective artificial immune algorithm has been used to optimize the kernel and penalize parameters of SVM in this paper. In training stage of SVM, multiple solutions are found by using multi-objective artificial immune algorithm and then these parameters are evaluated in test stage. The proposed algorithm is applied to fault diagnosis of induction motors and anomaly detection problems and successful results are obtained.",2011,1584013,"Ilhan Aydin,Mehmet Karaköse,Erhan Akin",0
1584014,A novel hybrid feature selection method for microarray data analysis.,"Recently, many methods have been proposed for microarray data analysis. One of the challenges for microarray applications is to select a proper number of the most relevant genes for data analysis. In this paper, we propose a novel hybrid method for feature selection in microarray data analysis. This method first uses a genetic algorithm with dynamic parameter setting (GADP) to generate a number of subsets of genes and to rank the genes according to their occurrence frequencies in the gene subsets. Then, this method uses the @g^2-test for homogeneity to select a proper number of the top-ranked genes for data analysis. We use the support vector machine (SVM) to verify the efficiency of the selected genes. Six different microarray datasets are used to compare the performance of the GADP method with the existing methods. The experimental results show that the GADP method is better than the existing methods in terms of the number of selected genes and the prediction accuracy.",2011,1584014,"Chien-Pang Lee,Yungho Leu",0
1584015,Fuzzy-wavelet based prediction of Earth rotation parameters.,"Prediction of Earth rotation parameters (ERPs) is of importance especially for near real-time applications including navigation, remote sensing, and hazard monitoring. Therefore, prediction of ERPs at least over a few days in the future is necessary. Fuzzy-inference systems (FIS) are increasingly popular and have advantage over classical FFT that lacks stochastic stability due to non-stationarity, multiscaling, and persistent autocorrelations. Wavelet filtering can be used to handle such phenomenon. A FIS rule-base created from ERP time series, where the volatilities (returns) of the preprocessed series are used, and high frequency signals removed, is summarized. The performance of this system, trained using the fuzzy-wavelet method, is compared with that of a conventional FIS, trained on raw time series. The results show that the predictions by the fuzzy-wavelet method are superior to the FIS-only model for short-term predictions (up to 10 days in future). The improvement of prediction accuracy is found to be about 30% in terms of RMS error.",2011,1584015,"O. Akyilmaz,Hansjörg Kutterer,C. K. Shum,T. Ayan",0
1584016,A feedback based CRI approach to fuzzy reasoning.,"Fuzzy reasoning methods are extensively used in intelligent systems and fuzzy control. In our previous work, an explicit feedback mechanism is embedded into optimal fuzzy reasoning methods, and the resulting fuzzy reasoning methods are more robust than the optimal fuzzy reasoning methods. An interesting question is that, without the introduction of optimization goals, can the robustness of fuzzy reasoning methods be improved by embedding feedback mechanisms? This paper is intended to answer the question. By embedding feedback mechanisms into CRI approach, a new feedback based CRI (FBCRI) approach is proposed and three implementation methods with different strategies are obtained. Simulation results show that the feedback mechanisms are really useful for the improvement of the robustness of CRI methods. At last, to test the applicability of the proposed approach, it is applied to the solution of a real-time path planning problem for UAVs. The effectiveness and efficiency of an FBCRI based real-time path planning algorithm are verified by representative test examples, which show that embedding feedback information into the fuzzy reasoning process actually improve the quality of flight paths.",2011,1584016,"Zheng Zheng,Shanjie Wu,Wei Liu,Kai-Yuan Cai",0
1584017,Fuzzy goal-driven intelligent control for satellite environmental qualification.,"This paper reports on the application of Fuzzy Reference Gain-Scheduling Control (FRGS) to control a thermal-vacuum unit that emulates space environmental conditions for satellite and space device qualification. FRGS is a variation of fuzzy control that changes the controller gain surface in accordance to distinct operational conditions established by the reference (goal). This system allows to incorporate the experience of human operators by emulating human decision-making and reasoning, with the advantage of adaptation when parameters are adjusted on-line. The controller gain surface is adapted by modifying the shapes of the membership functions according to piecewise constant reference values. While the goal-driven adjustment is accomplished in a feedforward manner, feedback control technique is used to guarantee dynamical response. The experimental response obtained in the application to the real system has shown the effectiveness and flexibility of this control approach for such a nonlinear system.",2011,1584017,"Ernesto Araujo,Karl Kienitz,Sandra Sandri",0
1584019,Incipient fault detection in induction machine stator-winding using a fuzzy-Bayesian change point detection approach.,"In this paper the incipient fault detection problem in induction machine stator-winding is considered. The problem is solved using a new technique of change point detection in time series, based on a two-step formulation. The first step consists of a fuzzy clustering to transform the initial data, with arbitrary distribution, into a new one that can be approximated by a beta distribution. The fuzzy cluster centers are determined by using a Kohonen neural network. The second step consists in using the Metropolis-Hastings algorithm for performing the change point detection in the transformed time series generated by the first step with that known distribution. The incipient faults are detected as long as they characterize change points in such transformed time series. The main contribution of the proposed approach is the enhanced resilience of the new failure detection procedure against false alarms, combined with a good sensitivity that allows the detection of rather small fault signals. Simulation and practical results are presented to illustrate the proposed methodology.",2011,1584019,"Marcos F. S. V. D'Angelo,Reinaldo M. Palhares,Ricardo H. C. Takahashi,Rosangela Helena Loschi,Lane M. R. Baccarini,Walmir M. Caminhas",0
1584020,Classification of communications signals using an advanced technique.,"Because of rapid growing of radio communication technology of late years, importance of automatic classification of digital signal type is rising increasingly. This paper presents an advanced technique that identifies a variety of digital signal types. This method is a hybrid heuristic formed by a radial basis function neural networks (as a classifier) and particle swarm optimization technique. A suitable combination of higher order statistics up to eighth are proposed as the prominent characteristics of the considered signals. In conjunction with neural network we have used a cross-validation technique to improve the generalization ability. Experimental results indicate that the proposed technique has high percentage of correct classification to discriminate different types of digital signal even at low SNRs.",2011,1584020,"Ataollah Ebrahimzadeh,S. E. Mousavi",0
1584021,Embedded support vector regression on Cerebellar Model Articulation Controller with Gaussian noise.,"In this study, an approach utilizing support vector regression (SVR) as the learning scheme of a Cerebellar Model Articulation Controller (CMAC) to handle noisy data is proposed. This approach is referred to as SVR-CMAC. Firstly, the memory-associated vector is transformed via the SVR model. Then, the output is computed from the SVR model as a given input of a CMAC. That is, the memory size of the proposed SVR-CMAC depends on the number of support vectors. It is difference from the conventional CMAC and the kernel CMAC that mainly depends on the number of input variables. Secondly, in order to measure the distance between two memory-associated vectors (i.e. unipolar binary input data), the modified Hamming distance is used in the proposed SVR-CMAC. That is, the modified Hamming distance measure is incorporated into the kernel function in the SVR model. Furthermore, the existed SVR software is easily modified to implement the SVR approach with these new Gaussian kernel functions. Besides, some easy approaches to determine the hyperparameters of the proposed SVR-CMAC are also proposed. Consequently, the proposed SVR-CMAC solves once a linearly constrained quadratic programming problem to obtain the final results. However, the final results of the conventional CMAC and the kernel CMAC need to update the weights with iteration. Finally, from the simulation results, the performance of the proposed SVR-CMAC is better than the conventional CMAC and the kernel CMAC for noisy data.",2011,1584021,"Chen-Chia Chuang,Chia-Chu Hsu,Chin-Wang Tao",0
1584022,Two-stage fuzzy stochastic programming with Value-at-Risk criteria.,"A new class of fuzzy stochastic optimization models-two-stage fuzzy stochastic programming with Value-at-Risk (FSP-VaR) criteria is built in this paper. Some properties of the two-stage FSP-VaR, such as value of perfect information (VPI), value of fuzzy random solution (VFRS), and bounds of the fuzzy random solution, are discussed. An Approximation Algorithm is proposed to compute the VaR by combining discretization method of fuzzy variable, random simulation technique and bisection method. The convergence of the approximation algorithm is proved. To solve the two-stage FSP-VaR, a hybrid mutation-neighborhood-based particle swarm optimization (MN-PSO) which comprises the Approximation Algorithm is proposed to search for the approximate optimal solution. Furthermore, a neural network-based acceleration method is discussed. A numerical experiment illustrates the effectiveness of the proposed hybrid MN-PSO algorithm. The comparison shows that the hybrid MN-PSO exhibits better performance than the one when using other approaches such as hybrid PSO and GA.",2011,1584022,"Shuming Wang,Junzo Watada",0
1584023,Optimal design of arch dams subjected to earthquake loading by a combination of simultaneous perturbation stochastic approximation and particle swarm algorithms.,"An efficient optimization procedure is introduced to find the optimal shapes of arch dams considering fluid-structure interaction subject to earthquake loading. The optimization is performed by a combination of simultaneous perturbation stochastic approximation (SPSA) and particle swarm optimization (PSO) algorithms. This serial integration of the two single methods is termed as SPSA-PSO. The operation of SPSA-PSO includes three phases. In the first phase, a preliminary optimization is accomplished using the SPSA. In the second phase, an optimal initial swarm is produced using the first phase results. In the last phase, the PSO is employed to find the optimum design using the optimal initial swarm. The numerical results demonstrate the high performance of the proposed strategy for optimal design of arch dams. The solutions obtained by the SPSA-PSO are compared with those of SPSA and PSO. It is revealed that the SPSA-PSO converges to a superior solution compared to the SPSA and PSO having a lower computation cost.",2011,1584023,"S. M. Seyedpoor,Javad Salajegheh,Eysa Salajegheh,Saeed Gholizadeh",0
1584024,Designing a model of FANP in brand image decision-making.,"Both theoretical and practical efforts in band images often neglect the characteristics having interactions and mutual influence among attributes or criteria, even in the stages of different brand life cycles. This study aims to create a hierarchical framework for brand image management. The analytical network process and fuzzy sets theory have been applied to both mindshare in brand images and inherent interaction/interdependencies among diverse information resources. A real empirical application is demonstrated in the department store. Both the theoretical and practical background of this paper have shown the fuzzy analytical network process can capture expert's knowledge existing in the form of incomplete and vague information for the mutual influence on attribute and criteria of brand image management.",2011,1584024,"Ling-Zhong Lin,Tsuen-Ho Hsu",0
1584025,Classification of audio signals using AANN and GMM.,"Today, digital audio applications are part of our everyday lives. Audio classification can provide powerful tools for content management. If an audio clip automatically can be classified it can be stored in an organised database, which can improve the management of audio dramatically. In this paper, we propose effective algorithms to automatically classify audio clips into one of six classes: music, news, sports, advertisement, cartoon and movie. For these categories a number of acoustic features that include linear predictive coefficients, linear predictive cepstral coefficients and mel-frequency cepstral coefficients are extracted to characterize the audio content. The autoassociative neural network model (AANN) is used to capture the distribution of the acoustic feature vectors. The AANN model captures the distribution of the acoustic features of a class, and the backpropagation learning algorithm is used to adjust the weights of the network to minimize the mean square error for each feature vector. The proposed method also compares the performance of AANN with a Gaussian mixture model (GMM) wherein the feature vectors from each class were used to train the GMM models for those classes. During testing, the likelihood of a test sample belonging to each model is computed and the sample is assigned to the class whose model produces the highest likelihood.",2011,1584025,"P. Dhanalakshmi,S. Palanivel,Vennila Ramalingam",0
1584026,Short-term combined economic emission scheduling of hydrothermal systems with cascaded reservoirs using particle swarm optimization technique.,"This paper develops an efficient and reliable particle swarm optimization (PSO) based algorithm for solving combined economic emission scheduling of hydrothermal systems with cascaded reservoirs. A multi-chain cascaded hydrothermal system with non-linear relationship between water discharge rate, power generation and net head is considered in this paper. The water transport delay between connected reservoirs is also considered. The problem is formulated considering both cost and emission as competing objectives. Combined economic emission scheduling (CEES) is a bi-objective problem. A price penalty factor approach is utilized here to convert this bi-objective CEES problem into a single objective one. The effect of valve-point loading is also taken into account in the present problem formulation. The feasibility of the proposed method is demonstrated on a sample test system consisting of four cascaded hydro units and three thermal units. The results of the proposed technique based on PSO are compared with other evolutionary programming method. It is found that the results obtained by the proposed technique are superior in terms of fuel cost, emission output etc. It is also observed that the computation time is considerably reduced by the proposed technique based on PSO.",2011,1584026,"K. K. Mandal,N. Chakraborty",0
1584027,Differential Evolution Classifier in Noisy Settings and with Interacting Variables.,"In this paper, we have studied the performance of a differential evolution (DE) classifier in classifying data in noisy settings. We have also studied the performance in handling extra variables which simply consists of gaussian noise. Furthermore, we have carried out the classification by adding on all two component interaction terms as extra variables into the data. Also, in this situation it is crucial to have a classifier which is tolerant to noisy variables. Namely, even though there can be interaction effects in the data that can influence classification results positively, it is usually not known a priori which particular interaction components are contributing to the classification results. Therefore, we need to add all possible combinations despite the likelihood of then creating also some noisy variables which do not influence the classification accuracy, or which actually reduce the accuracy. In experimentation, we used four widely applied test data sets; the new-thyroid, heart-statlog, Hungarian heart and lenses data sets. The results indicated the DE classifier to be robust from the noise tolerance point of view in all studied cases and situations. The results suggest that the DE classifier is useful especially in the cases where interaction effects may have a significant influence to the classification accuracy.",2011,1584027,"Pasi Luukka,Jouni Lampinen",0
1584028,Identification of the linear parts of nonlinear systems for fuzzy modeling.,"In direct approach to fuzzy modeling, structure identification is one of the most critical tasks. In modeling the nonlinear system, this fact is more crucial. In this paper, a new hybrid method is proposed to cluster the data located in the linear parts on the nonlinear systems. The proposed method can partition the input-output data in two groups: data located in the linear parts and data in the extrema. It is shown that the first group of data is suitable to be clustered by Fuzzy C-Regression Model (FCRM) clustering algorithm and the second group by Fuzzy C-Means (FCM). Then, based on the above findings, a new hybrid clustering algorithm is proposed. Finally, the proposed approach is tested and validated by several numerical examples of nonlinear functions.",2011,1584028,"Mahmood Rezaei Sadrabadi,Mohammad Hossein Fazel Zarandi",0
1584029,Velocity Modulated Bacterial Foraging Optimization Technique (VMBFO).,"In this paper, a Velocity Modulated Bacterial Foraging Optimization Technique is proposed. The Bacterial Foraging algorithm is hybridized with Particle Swarm Optimization technique to reduce the convergence time while maintaining high accuracy. The hybridized optimization technique is tested on various benchmark functions like Sphere, Rosenbrock, Generalized Rastrigin, Griewank and Ackley. In addition to these, the proposed algorithm is also tested to calculate the resonant frequency of Rectangular Microstrip Antenna. The results are compared with the existing algorithms and that of experimental results and are found to be in good agreement.",2011,1584029,"Sastry V. R. S. Gollapudi,Shyam S. Pattnaik,O. P. Bajpai,Swapna Devi,K. M. Bakwad",0
1584030,New methods for system planning.,"In this paper, three artificial intelligence techniques, genetic algorithm (GA), simulated annealing (SA), and tabu search (TS) are proposed to search for solutions to transmission network planning. The performance of each of the techniques is studied and the results compared with these from conventional methods. The problem of transmission network planning is formulated as a genetic algorithm, simulated annealing and tabu search with the cost of new lines and security constraints at each bus-bar included. The novel results from a set of tests carried out on the prototype show that the application of genetic algorithm; simulated annealing and tabu search techniques are feasible in transmission network planning. Finally, observations (1), (2), (3), (4), and (5) are presented by analysis of variance.",2011,1584030,Ahmad Sadegheih,0
1584031,Two-stage EDA-based approach for all optical WDM mesh network survivability under SRLG constraints.,"In this paper, a two-stage evolutionary algorithm is proposed to solve an NP-complete telecommunication problem-all optical wavelength-division multiplexing (WDM) mesh network survivability under shared-risk-link-group (SRLG) constraints. First of all, a novel greedy heuristic with two control parameters is developed to construct feasible solutions of the telecommunication problem. An estimation of distribution algorithm (EDA) with guided mutation is applied to search for optimum settings of the two control parameters in respective two stages. Given the found best control parameters, an optimal solution of the considered problem can be constructed by the greedy heuristic. Experimental results show that the proposed approach compares favorably against the best-known evolutionary-based algorithm in 26 out of 30 test instances in terms of solution quality within given time limit.",2011,1584031,Jianyong Sun,0
1584032,Numerical solution of PDEs via integrated radial basis function networks with adaptive training algorithm.,"This paper develops a mesh-free numerical method for solving PDEs, based on integrated radial basis function networks (IRBFNs) with adaptive residual subsampling training scheme. The multiquadratic function is chosen as the transfer function of the neurons. The nonlinear algebraic equation systems for weights training are solved by Levenberg-Marquardt algorithm. The performance of the proposed method is demonstrated in numerical examples by approximating several functions and solving nonlinear PDEs. The result of numerical experiments shows that the IRBFNs with the adaptive procedure requires less neurons to attain the desired accuracy than conventional radial basis function networks.",2011,1584032,"Hong Chen,Li Kong,Wen-Jun Leng",0
1584034,Robust guaranteed cost control of uncertain fuzzy systems under time-varying sampling.,"In practice, the system is often modeled as a continuous-time fuzzy system, while the control input is applied only at discrete instants. This system is called a sampled-data control system. In this paper, robust guaranteed cost control for uncertain sampled-data fuzzy systems is discussed. A guaranteed cost control where a quadratic cost function is bounded by a certain scalar, not only stabilizes a system but also considers a control performance. A typical sampled-data control is the zero-order input, which can be represented as a piecewise-continuous delay. Here we take a delay system approach to the sampled-data guaranteed cost control problem. The closed-loop system with a sampled-data state feedback controller becomes a system with time-varying delay. First, guaranteed cost control performance conditions for the closed-loop system are given in terms of linear matrix inequalities (LMIs). Such conditions are derived by using Leibniz-Newton formula and free weighting matrix method for fuzzy systems under the assumption that sampling time is not greater than some prescribed scalar. Then, a design method of robust guaranteed cost state feedback controller for uncertain sampled-data fuzzy systems is proposed. Examples are given to illustrate our robust sampled-data guaranteed cost control design.",2011,1584034,Jun Yoneyama,0
1584035,Neuro-genetic approach to optimize parameter design of dynamic multiresponse experiments.,"Engineers have widely applied the Taguchi method, a traditional approach for robust experimental design, to a variety of quality engineering problems for enhancing system robustness. However, the Taguchi method is unable to deal with dynamic multiresponse owing to increasing complexity of the product or design process. Although several alternative approaches have been presented to resolve this problem, they cannot effectively treat situations in which the control factors have continuous values. This study incorporates desirability functions into a hybrid neural network/genetic algorithm approach to optimize the parameter design of dynamic multiresponse with continuous values of parameters. The objective is to find the optimal combination of control factors to simultaneously maximize robustness of each response. The proposed approach is based on three stages which (1) use neural networks for constructing a response function model of a dynamic multiresponse system, (2) use exponential desirability functions for evaluating overall performance of a specific factor combination, and (3) use a genetic algorithm to optimize parameter design. Effectiveness of the proposed approach is illustrated with a simulated example. Analysis results reveal that the approach has higher performance than the traditional experimental design.",2011,1584035,"Hsu-Hwa Chang,Yan-Kwang Chen",0
1584036,Simultaneous concept-based evolutionary multi-objective optimization.,"In contrast to traditional multi-objective problems the concept-based version of such problems involves sets of particular solutions, which represent predefined conceptual solutions. This paper addresses the concept-based multi-objective problem by proposing two novel multi objective evolutionary algorithms. It also compares two major search approaches.The suggested algorithms deal with resource sharing among concepts, and within each concept, while simultaneously evolving concepts towards a Pareto front by way of their representing sets. The introduced algorithms, which use a simultaneous search approach, are compared with a sequential one. For this purpose concept-based performance indicators are suggested and used. The comparison study includes both the computational time and the quality of the concept-based front representation. Finally, the effect on the computational time of both the concept fitness evaluation time and concept optimality, for both the sequential and simultaneous approaches, is highlighted.",2011,1584036,"Gideon Avigad,Amiram Moshaiov",0
1584038,The effects of two new crossover operators on genetic algorithm performance.,"In this study, two new crossover operators in genetic algorithm are proposed; sequential and random mixed crossover. In the first stage, existing and developed crossover operators were applied to two benchmark problems, namely the reinforced concrete beam problem and the space truss problem. In the second stage, the developed crossover operators were applied to a deep beam problem and, a concrete mix design problem. The fittest values obtained using developed crossover operators were higher than those were obtained with existing crossover operator after 30,000 generations. Moreover, in developed crossover operators, the random mixed crossover yields higher fitness values than the existing crossover operators.",2011,1584038,Mustafa Kaya,0
1584039,"Synergetic use of different evaluation, parameterization and search tools within a multilevel optimization platform.","This paper presents the synergetic use of different evaluation tools, parameterization schemes and search methods on the levels of a multilevel optimization platform to efficiently solve single- and multi-objective computationally demanding optimization problems. The platform is formed by a number of levels which concurrently search for optimal solutions, by regularly exchanging promising individual solutions. Each level is associated with a problem-specific evaluation tool with its own accuracy and computational cost, a parameterization scheme which determines the design variables and their mapping to generate individual solutions and a search algorithm which is either a metamodel-assisted evolutionary algorithm or a gradient-based method. The use of the multilevel platform with only one of the aforementioned features changing from level to level was presented in a previous paper by the authors. The present paper shows that the combined use of hierarchical evaluation, hierarchical parameterization and hierarchical search decreases further the computational cost by increasing the efficiency of the optimization method. This is demonstrated on function minimization and aerodynamic shape optimization problems; though only two levels are used herein, this is not a restriction and the optimization platform may accommodate any number of them.",2011,1584039,"Ioannis C. Kampolis,Kyriakos C. Giannakoglou",0
1584040,Bond graph based Bayesian network for fault diagnosis.,"Model-based fault diagnosis using artificial intelligence techniques often deals with uncertain knowledge and incomplete information. Probability reasoning is a method to deal with uncertain or incomplete information, and Bayesian network is a tool that brings it into the real world application. A novel approach for constructing the Bayesian network structure on the basis of a bond graph model is proposed. Specification of prior and conditional probability distributions (CPDs) for the Bayesian network can be completed by expert knowledge and learning from historical data. The resulting Bayesian network is then applied for diagnosing faulty components from physical systems. The performance of the proposed fault diagnosis scheme based on bond graph derived Bayesian network is demonstrated through simulation studies.",2011,1584040,"C. H. Lo,Y. K. Wong,Ahmad B. Rad",0
1584041,On reducing computational overhead in multi-objective genetic Takagi-Sugeno fuzzy systems.,"The use of multi-objective evolutionary algorithms (MOEAs) to generate a set of fuzzy rule-based systems (FRBSs) with different trade-offs between complexity and accuracy has gained more and more interest in the scientific community. The evolutionary process requires, however, a large number of FRBS generations and evaluations. When we deal with high dimensional datasets, these tasks can be very time-consuming, especially when we generate Takagi-Sugeno FRBSs, thus making a satisfactory exploration of the search space very awkward. In this paper, we first analyze the time complexity for both the generation and the evaluation of Takagi-Sugeno FRBSs. Then we introduce a simple but effective technique for speeding up the identification of the rule consequent parameters, one of the most time-consuming phases in Takagi-Sugeno FRBS generation. Finally, we highlight how the application of this technique produces as a side-effect a decoupling of the rules. This decoupling allows us to avoid re-computing consequent parameters of rules which are not directly modified during the evolutionary process, thus saving a considerable amount of time. In the experimental part we first test the correctness of the predicted asymptotical time complexity. Then we show the benefits in terms of computing time saving and improved search space exploration through an example of multi-objective genetic learning of Takagi-Sugeno FRBSs in the regression domain.",2011,1584041,"Marco Cococcioni,Beatrice Lazzerini,Francesco Marcelloni",0
1584042,Stability and robustness of fuzzy adaptive control of nonlinear systems.,"In this paper, we propose a new approach that guarantees the stability and robustness of an adaptive control law of a nonlinear system. The control diagram proposed contains a Takagi-Sugeno-Kang fuzzy controller (TSK-FC) and a training block allowing the online adaptation of the FC parameters. The adaptation algorithm used is based on the gradient with minimization of the quadratic error between the system output and that desired by using the direct method of Lyapunov. However, our approach considers the gradient step of each adaptive FC parameter to be bound. This approach was applied to the control of an inverted pendulum. The results obtained confirm well the validity of such an adaptation especially the guarantee of the pendulum stability and the robustness of its control with respect to the disturbances introduced on the FC parameters and the pendulum itself.",2011,1584042,"Raouf Ketata,Yassine Rezgui,Nabil Derbel",0
1584043,Application of particle swarm optimization to association rule mining.,"In the area of association rule mining, most previous research had focused on improving computational efficiency. However, determination of the threshold values of support and confidence, which seriously affect the quality of association rule mining, is still under investigation. Thus, this study intends to propose a novel algorithm for association rule mining in order to improve computational efficiency as well as to automatically determine suitable threshold values. The particle swarm optimization algorithm first searches for the optimum fitness value of each particle and then finds corresponding support and confidence as minimal threshold values after the data are transformed into binary values. The proposed method is verified by applying the FoodMart2000 database of Microsoft SQL Server 2000 and compared with a genetic algorithm. The results indicate that the particle swarm optimization algorithm really can suggest suitable threshold values and obtain quality rules. In addition, a real-world stock market database is employed to mine association rules to measure investment behavior and stock category purchasing. The computational results are also very promising.",2011,1584043,"R. J. Kuo,C. M. Chao,Y. T. Chiu",0
1584044,A hybrid model combining case-based reasoning and fuzzy decision tree for medical data classification.,"In this research, a hybrid model is developed by integrating a case-based data clustering method and a fuzzy decision tree for medical data classification. Two datasets from UCI Machine Learning Repository, i.e., liver disorders dataset and Breast Cancer Wisconsin (Diagnosis), are employed for benchmark test. Initially a case-based clustering method is applied to preprocess the dataset thus a more homogeneous data within each cluster will be attainted. A fuzzy decision tree is then applied to the data in each cluster and genetic algorithms (GAs) are further applied to construct a decision-making system based on the selected features and diseases identified. Finally, a set of fuzzy decision rules is generated for each cluster. As a result, the FDT model can accurately react to the test data by the inductions derived from the case-based fuzzy decision tree. The average forecasting accuracy for breast cancer of CBFDT model is 98.4% and for liver disorders is 81.6%. The accuracy of the hybrid model is the highest among those models compared. The hybrid model can produce accurate but also comprehensible decision rules that could potentially help medical doctors to extract effective conclusions in medical diagnosis.",2011,1584044,"Chin-Yuan Fan,Pei-Chann Chang,Jyun-Jie Lin,Jui-Chien C. Hsieh",0
1584045,Performance evaluation of feed-forward neural network with soft computing techniques for hand written English alphabets.,"This paper describes the performance evaluation for the feed forward neural network with three different soft computing techniques to recognition of hand written English alphabets. Evolutionary algorithms for the hybrid neural network are showing the numerous potential in the field of pattern recognition. We have taken five trials and two networks of each of the algorithm: back propagation, Evolutionary algorithm, and Hybrid Evolutionary algorithm respectively. These algorithms have been taken the definite lead on the conventional approaches of neural network for pattern recognition. It has been analyzed that the feed forward neural network by two Evolutionary algorithms makes better generalization accuracy in character recognition problems. The problem of not convergence the weight in conventional backpropagation has also eliminated by using the soft computing techniques. It has been observed that, there are more than one converge weight matrix in character recognition for every training set. The results of the experiments show that the hybrid evolutionary algorithm can solve challenging problem most reliably and efficiently. These algorithms have also been compared on the basis of time and space complexity for the training set.",2011,1584045,"Saurabh Shrivastava,Manu Pratap Singh",0
1584046,Structural damage detection using fuzzy cognitive maps and Hebbian learning.,A new algorithmic approach for structural damage detection based on the fuzzy cognitive map (FCM) is developed in this paper. Structural damage is modeled using the continuum mechanics approach as a loss of stiffness at the damaged location. A finite element model of a cantilever beam is used to calculate the change in the first six beam frequencies because of structural damage. The measurement deviations due to damage are fuzzified and then mapped to a set of faults using FCM. The input concepts for the FCM are the frequency deviations and the output of the FCM is at five possible damage locations along the beam. The FCM works quite well for structural damage detection for ideal and noisy data. Further improvement in performance is obtained when an unsupervised neural network approach based on Hebbian learning is used to evolve the FCM. Numerical results clearly show that the use of FCM and Hebbian learning results in accurate damage detection and represents a powerful tool for structural health monitoring.,2011,1584046,"P. Beena,Ranjan Ganguli",0
1584048,Information recovery from measured data by linear artificial neural networks - An example from rainfall-runoff modeling.,"The results of a study using linear artificial neural networks (ANNs) to determine the physical parameters in event-based rainfall-runoff modeling are presented in this paper. The input structure of the ANN was determined based on an analysis of the discretized form of the kinematic wave equations, and the physical parameters were obtained through a back calculation of the weights and biases of the ANN. Two cases were considered; using ANNs trained on datasets derived from: (1) effective rain of entire events and (2) total rain of wet portion of events only. For Case (1), the parameters @Dt and @a(=S/n) were determined with excellent accuracy. For Case (2), in addition to @Dt and @a, the constant loss rate, @F, was also determined with excellent accuracy. Further, the use of total rain of entire events (a common practice in ANN applications in rainfall-runoff modeling) was also investigated, and it was found that the results from this analysis are less realistic compared to those of Case (2).",2011,1584048,"Lloyd H. C. Chua,Tommy S. W. Wong,X. H. Wang",0
1584049,A hybrid genetic algorithm for the minimum energy broadcast problem in wireless ad hoc networks.,"Given a wireless ad hoc network with a specified source node that has to broadcast messages to all other nodes in the network, the minimum energy broadcast (MEB) problem seeks a broadcast scheme for this network with minimum energy consumption. The MEB problem is NP-Hard. This paper describes a hybrid approach to the MEB problem combining a genetic algorithm with a local search heuristic. We have compared our hybrid approach against the best heuristic approaches known for this problem. Our approach outperformed all these approaches and emerged as the best.",2011,1584049,"Alok Singh,Wilson Naik Bhukya",0
1584050,Performance evaluation of efficient multi-objective evolutionary algorithms for design space exploration of embedded computer systems.,"Multi-objective evolutionary algorithms (MOEAs) have received increasing interest in industry because they have proved to be powerful optimizers. Despite the great success achieved, however, MOEAs have also encountered many challenges in real-world applications. One of the main difficulties in applying MOEAs is the large number of fitness evaluations (objective calculations) that are often needed before an acceptable solution can be found. There are, in fact, several industrial situations in which fitness evaluations are computationally expensive and the time available is very short. In these applications efficient strategies to approximate the fitness function have to be adopted, looking for a trade-off between optimization performance and efficiency. This is the case in designing a complex embedded system, where it is necessary to define an optimal architecture in relation to certain performance indexes while respecting strict time-to-market constraints. This activity, known as design space exploration (DSE), is still a great challenge for the EDA (electronic design automation) community. One of the most important bottlenecks in the overall design flow of an embedded system is due to simulation. Simulation occurs at every phase of the design flow and is used to evaluate a system which is a candidate for implementation. In this paper we focus on system level design, proposing an extensive comparison of the state-of-the-art of MOEA approaches with an approach based on fuzzy approximation to speed up the evaluation of a candidate system configuration. The comparison is performed in a real case study: optimization of the performance and power dissipation of embedded architectures based on a Very Long Instruction Word (VLIW) microprocessor in a mobile multimedia application domain. The results of the comparison demonstrate that the fuzzy approach outperforms in terms of both performance and efficiency the state of the art in MOEA strategies applied to DSE of a parameterized embedded system.",2011,1584050,"Giuseppe Ascia,Vincenzo Catania,Alessandro G. Di Nuovo,Maurizio Palesi,Davide Patti",0
1584051,A new methodology for Decisions in Medical Informatics using fuzzy cognitive maps based on fuzzy rule-extraction techniques.,"In this research work, a novel framework for the construction of augmented Fuzzy Cognitive Maps based on Fuzzy Rule-Extraction methods for decisions in medical informatics is investigated. Specifically, the issue of designing augmented Fuzzy Cognitive Maps combining knowledge from experts and knowledge from data in the form of fuzzy rules generated from rule-based knowledge discovery methods is explored. Fuzzy cognitive maps are knowledge-based techniques which combine elements of fuzzy logic and neural networks and work as artificial cognitive networks. The knowledge extraction methods used in this study extract the available knowledge from data in the form of fuzzy rules and insert them into the FCM, contributing to the development of a dynamic decision support system. The fuzzy rules, which derived by these extraction algorithms (such as fuzzy decision trees, association rule-based methods and neuro-fuzzy methods) are implemented to restructure the FCM model, producing new weights into the FCM model, that initially structured by experts. Concluding, our scope is to present a new methodology through a framework for decision making tasks using the soft computing technique of FCMs based on knowledge extraction methods. A well known medical decision making problem pertaining to the problem of radiotherapy treatment planning selection is presented to illustrate the application of the proposed framework and its functioning.",2011,1584051,Elpiniki I. Papageorgiou,0
1584052,Reinforcement learning of competitive and cooperative skills in soccer agents.,"The main aim of this paper is to provide a comprehensive numerical analysis on the efficiency of various reinforcement learning (RL) techniques in an agent-based soccer game. The SoccerBots is employed as a simulation testbed to analyze the effectiveness of RL techniques under various scenarios. A hybrid agent teaming framework for investigating agent team architecture, learning abilities, and other specific behaviours is presented. Novel RL algorithms to verify the competitive and cooperative learning abilities of goal-oriented agents for decision-making are developed. In particular, the tile coding (TC) technique, a function approximation approach, is used to prevent the state space from growing exponentially, hence avoiding the curse of dimensionality. The underlying mechanism of eligibility traces is evaluated in terms of on-policy and off-policy procedures, as well as accumulating traces and replacing traces. The results obtained are analyzed, and implications of the results towards agent teaming and learning are discussed.",2011,1584052,"Jinsong Leng,Chee Peng Lim",0
1584053,Two-layer particle swarm optimization for unconstrained optimization problems.,"In this article, a two-layer particle swarm optimization (TLPSO) is proposed to increase the diversity of the particles so that the drawback of trapping in a local optimum is avoided. In order to design the TLPSO, a structure with two layers (top layer and bottom layer) is proposed so that M swarms of particles and one swarm of particles are generated in the bottom layer and the top layer, respectively. Each global best position in each swarm of the bottom layer is set to be the position of the particle in the swarm of the top layer. Therefore, the global best position in the swarm of the top layer influences indirectly the particles of each swarm in the bottom layer so that the diversity of the particles increases to avoid trapping into a local optimum. Besides, a mutation operation is added into the particles of each swarm in the bottom layer so that the particles leap the local optimum to find the global optimum. Finally, some optimization problems of different types of high dimensional functions are used to illustrate the efficiency of the proposed method.",2011,1584053,Chia-Chong Chen,0
1584054,An ant colony algorithm for solving fixed destination multi-depot multiple traveling salesmen problems.,"The fixed destination multi-depot multiple traveling salesmen problem (MmTSP) is a problem, in which more than one salesmen depart from several starting cities and having returned to the starting city, form tours so that each city is visited with exactly one salesman, and the tour lengths stay within certain limits. This problem is of a great complexity and few investigations have been done on it previously. In this paper an ant system is designed to solve the problem and the results are compared to the answers obtained by solving the same problems by Lingo 8.0 which uses exact methods.",2011,1584054,"Soheil Ghafurian,Nikbakhsh Javadian",0
1584055,Heuristic wavelet shrinkage for denoising.,"Noise reduction without any prior knowledge of noise or signals is addressed in this study. Compared with conventional filters, wavelet shrinkage can respect this requirement to reduce noise from received signal in wavelet coefficients. However, wavelet threshold depends on an estimate of noise deviation and a weight relating signal's length cannot be applied in every case. This paper uses particle swarm optimization (PSO) to explore a suitable threshold in a complete solution space, named PSOShrink. A general-purpose objective function which is derived from blind signal separation (BSS) theory is further proposed. In simulation, four benchmarks signals and three degrading degrees are testing; meanwhile, three existing algorithm with state-of-the-art are performed for comparison. PSOShrink can not only recovers source signals from a heavy blurred signal but also remains details of a source signal from a light blurred signal; moreover, it performs outstanding denoising in every simulation case.",2011,1584055,"Chan-Cheng Liu,Tsung-Ying Sun,Shang-Jeng Tsai,Yu-Hsiang Yu,Sheng-Ta Hsieh",0
1584056,Stock trading system based on the multi-objective particle swarm optimization of technical indicators on end-of-day market data.,"Stock traders consider several factors or objectives in making decisions. Moreover, they differ in the importance they attach to each of these objectives. This requires a tool that can provide an optimal tradeoff among different objectives, a problem aptly solved by a multi-objective optimization (MOO) system. This paper aims to investigate the application of multi-objective optimization to end-of-day historical stock trading. We present a stock trading system that uses multi-objective particle swarm optimization (MOPSO) of financial technical indicators. Using end-of-day market data, the system optimizes the weights of several technical indicators over two objective functions, namely, percent profit and Sharpe ratio. The performance of the system was compared to the performance of the technical indicators, the performance of the market, and the performance of another stock trading system which was optimized with the NSGA-II algorithm, a genetic algorithm-based MOO method. The results show that the system performed well on both training and out-of-sample data. In terms of percent profit, the system outperformed most, if not all, of the indicators under study, and, in some instances, it even outperformed the market itself. In terms of Sharpe ratio, the system consistently performed significantly better than all the technical indicators. The proposed MOPSO system also performed far better than the system optimized by NSGA-II. The proposed system provided a diversity of solutions for the two objective functions and is found to be robust and fast. These results show the potential of the system as a tool for making stock trading decisions.",2011,1584056,"Antonio C. Briza,Prospero C. Naval Jr.",0
1584057,Quantifying influences in the qualitative probabilistic network with interval probability parameters.,"A qualitative probabilistic network (QPN) is the qualitative abstraction of a Bayesian network by encoding variables and the qualitative influences between them in a directed acyclic graph. How to quantify the strengths of these influences is critical to resolve trade-offs and avoid ambiguities with conflicting signs during inference, which is hotly debated and studied in recent years. In order to provide for measuring the strengths of qualitative influences and resolving trade-offs, we take interval probability parameters as indicators of influence strengths in this paper. First, we define the conditional interval probabilities and multiplication rules that support causality representation and inference. Then we give the definition of qualitative influences associated with strengths represented by interval probabilities. Further, we propose the corresponding method for inference with the interval-probability-enhanced QPN. By the calculation of interval probabilities, the symmetry and transitivity properties are addressed. By giving a superposition method for qualitative influences associated with strengths, the composition property is interpreted. Building upon these 3 properties, the trade-offs can be well resolved.",2011,1584057,"Kun Yue,Weiyi Liu,MingLiang Yue",0
1584058,A clustering-based differential evolution for global optimization.,"Hybridization with other different algorithms is an interesting direction for the improvement of differential evolution (DE). In this paper, a hybrid DE based on the one-step k-means clustering, called clustering-based DE (CDE), is presented for the unconstrained global optimization problems. The one-step k-means clustering acts as several multi-parent crossover operators to utilize the information of the population efficiently, and hence it can enhance the performance of DE. To validate the performance of our approach, 30 benchmark functions of a wide range of dimensions and diversity complexities are employed. Experimental results indicate that our approach is effective and efficient. Compared with other state-of-the-art DE approaches, our approach performs better, or at least comparably, in terms of the quality of the final solutions and the reduction of the number of fitness function evaluations (NFFEs).",2011,1584058,"Zhihua Cai,Wenyin Gong,Charles X. Ling,Harry Zhang",0
1584059,Predicting weather events using fuzzy rule based system.,"Discovering and understanding the dynamic phenomena of weather to accurately predict different weather events has been an integral component of scientific investigations worldwide. The weather data, being inherently fuzzy in nature, requires highly complex processing based on human observations, satellite photography, or radar followed by computer simulations. This is further combined with an understanding of the principles of global and local weather dynamics. This paper attempts to solve weather event prediction for Lahore by implementing a fuzzy rule based system. The difficult problem of weather event prediction has been dealt in this paper through two separate experimental settings. In the first experimental setting a smaller dataset consisting of 365 instances with 4 inputs and 8 weather events has been used to develop a fuzzy inference system. In the second experimental setting the developed fuzzy system has been enhanced for a larger dataset consisting of over 2500 data points, having 17 inputs, and 10 weather events. For the later experiments the results of the fuzzy system have been compared with two other models i.e., decision tree (DT) based model and partial least square based regression (PLSR) model. It has been observed in the present study that the performance of the fuzzy system is sensitive to bootstrapping sampling technique that has been used for generating training and test samples for developing the fuzzy, DT and PLSR models. Further the models under consideration have been less sensitive to principal component analysis based dimensionality reduction method.",2011,1584059,"Malik Shahzad Kaleem Awan,Mian M. Awais",0
1584060,Soft computing of the Borda count by fuzzy linguistic quantifiers.,"Borda count is a well-known social choice method frequently used for group decision making problems, however, it does not consider the optimistic/pessimistic view of the director, which has a great effect on group decisions. In the present study, the traditional Borda method is extended by using the ordered weighted averaging (OWA) operator to consider the risk-attitudinal characteristics. The new approach, entitled Borda-OWA, solves the group decision making problem in a more intelligent procedure. Borda-OWA is then applied on two real case studies of forest management and projects ranking. The outcomes show that the Borda-OWA operator produces more soft and sensitive results by using the director's risk attitudes, guided by the fuzzy linguistic quantifiers. These results are examined by sensitivity analysis, which shows the significance of selecting the suitable quantifier. It is also shown that the original Borda is a special case of the Borda-OWA approach, indicating only the neutral view for the director. Because of the uncertainty in selecting the quantifier, we have defined a new measure, which considers both the expected value and the variance of the combined goodness measure for each alternative. By using this measure, the Borda-OWA model will give more robust decision to the stakeholders whose optimism degrees are different than that of the group director.",2011,1584060,Mahdi Zarghami,0
1584061,Flocking behaviour in simple ecosystems as a result of artificial evolution.,"In this paper the FLOCK system is presented, which was designed to test the ability of simple genetic algorithm to evolve flocking behaviour in ecosystems consisting of plants, herbivores and predators. Open-ended evolution was applied in the experiments, first. Many different behaviours were observed, which were very similar to those in nature, for instance: the escape of herbivore from predators, making herbivores route towards plants or a pursuit of predators after herbivores. Another interesting behaviour was grouping of predators around plants, where the probability of meeting herbivores is greater than in other places. But open-ended evolution and complex vision system of animal were not sufficient to observe flocking behaviour. The advanced behaviour such as creation of flocks appeared as a result of steered evolution.",2011,1584061,"Halina Kwasnicka,Urszula Markowska-Kaczmar,Marcin Mikosik",0
1584062,Integrated Learning Particle Swarm Optimizer for global optimization.,"This study proposes a novel Integrated Learning Particle Swarm Optimizer (ILPSO), for optimizing complex multimodal functions. The algorithm modifies the learning strategy of basic PSO to enhance the convergence and quality of solution. The ILPSO approach finds the diverged particles and accelerates them towards optimal solution. This novel study also introduces the particle's updating strategy based on hyperspherical coordinates system. This is especially helpful in handling evenly distributed multiple minima. The proposed technique is integrated with comprehensive learning strategy to explore the solution effectively. The performance comparison is carried out against different high quality PSO variants on the set of standard benchmark functions with and without coordinate rotation and with asymmetric initialization. Proposed ILPSO algorithm is efficient in terms of convergence rate, solution accuracy, standard deviation, and computation time compared with other PSO variants. Friedman non-parametric statistical test followed by Dunn post analysis results indicate that the proposed ILPSO algorithm is an effective technique to optimize complex multimodal functions of higher dimension.",2011,1584062,"Samrat L. Sabat,Layak Ali,Siba K. Udgata",0
1584063,In-depth analysis and simulation study of an innovative fuzzy approach for ranking alternatives in multiple attribute decision making problems based on TOPSIS.,"In this paper, an innovative fuzzy approach for ranking alternatives in multiple attribute decision making problems based on TOPSIS is presented in-depth and studied through simulation comparison with the original method. The TOPSIS method provides the principle of compromise that the chosen alternative should have the shortest distance from the ideal solution and, simultaneously, the farthest distance from the negative ideal solution. However, the TOPSIS method does not always produce results in harmony with this principle due to an oversimplified definition of its aggregation function which does not grasp the contradictory nature of the principle's formulation. Our approach addresses this issue through the introduction of a fuzzy set representation of the closeness to the ideal and to the negative ideal solution for the definition of the aggregation function which is modeled as the membership function of the intersection of two fuzzy sets. This model enables a parameterization of the method according to the risk attitude of the decision maker. Thus, a class of methods is formulated whose different instances correspond to different risk attitudes of the decision makers. In order to define some clear advises for decision makers facilitating a proper parameterization of the method, a comparative analysis of the proposed class of methods with the original TOPSIS method is performed according to well defined simulation techniques. The results of the simulation experiment show on the one hand that there is no direct correspondence between the proposed class of methods and TOPSIS, and on the other hand that it is adequate to distinguish three instances that correspond respectively to risk-averse, risk-neutral and risk-seeking decision makers. Finally, a numerical example pertaining to the problem of service provider selection is presented to illustrate the application of the proposed class of methods and its functioning.",2011,1584063,"Ioannis Chamodrakas,I. Leftheriotis,Drakoulis Martakos",0
1584064,Solving the constrained coverage problem.,"Coverage problem which is one of the challenging problems in facility location studies, is NP-hard. In this paper, we focus on a constrained version of coverage problem in which a set of demand points and some constrained regions are given and the goal is to find a minimum number of sensors which covers all demand points. A heuristic approach is presented to solve this problem by using the Voronoi diagram and p-center problem's solution. The proposed algorithm is relatively time-saving and is compared with alternative solutions. The results are discussed, and concluding remarks and future work are given.",2011,1584064,"Mansoor Davoodi,Ali Mohades",0
1584065,Parameter tuning of PBIL and CHC evolutionary algorithms applied to solve the Root Identification Problem.,"Evolutionary algorithms are among the most successful approaches for solving a number of problems where systematic searches in huge domains must be performed. One problem of practical interest that falls into this category is known as The Root Identification Problem in Geometric Constraint Solving, where one solution to the geometric problem must be selected among a number of possible solutions bounded by an exponential number. In previous works we have shown that applying genetic algorithms, a category of evolutionary algorithms, to solve the Root Identification Problem is both feasible and effective. In this work, we report on an empirical statistical study conducted to establish the influence of the driving parameters in the PBIL and CHC evolutionary algorithms when they are used to solve the Root Identification Problem. We identify a set of values that optimize algorithms performance. The driving parameters considered for the PBIL algorithm are population size, mutation probability, mutation shift and learning rate. For the CHC algorithm we studied population size, divergence rate, differential threshold and the set of best individuals. In both cases we applied unifactorial and multifactorial analysis, post hoc tests and best parameter level selection. Experimental results show that CHC outperforms PBIL when applied to solve the Root Identification Problem.",2011,1584065,"Robert Joan-Arinyo,M. Victoria Luzón,Enrique Yeguas",0
1584066,Some optimal models for facility location-allocation problem with random fuzzy demands.,"Facility location-allocation (FLA) problem is widely used in practical life. Many papers have introduced the problem with the random or fuzzy demands of customers, but few give the case that the customers' demands are random fuzzy. In this paper, the expected cost minimization model, (@a,@b)-cost minimization model and chance maximization model are presented with random fuzzy demands. In order to solve these random fuzzy models, the simplex algorithm, random fuzzy simulations and genetic algorithm are integrated to produce a hybrid intelligent algorithm. Finally, some numerical examples are provided for the sake of illustration.",2011,1584066,"Meilin Wen,Rui Kang",0
1584067,Modeling and simulation of chaotic phenomena in electrical power systems.,"Modeling and simulation of nonlinear systems under chaotic behavior is presented. Nonlinear systems and their relation to chaos as a result of nonlinear interaction of different elements in the system are presented. Application of chaotic theory for power systems is discussed through simulation results. Simulation of some mathematical equations, e.g. Vander Pol's equation, Lorenz's equation, Duffing's equation and double scroll equations are presented. Theoretical aspects of dynamical systems, the existence of chaos in power system and their dependency on system parameters and initial conditions using computer simulations are discussed. From the results one can easily understand the strange attractor and transient stages to voltage collapse, angle instability or voltage collapse and angle divergence simultaneously. Important simulation results of chaos for a model three bus system are presented and discussed.",2011,1584067,"Deepak Kumar Lal,K. S. Swarup",0
1584068,Quality-oriented optimization of wave soldering process by using self-organizing maps.,"In this paper, the optimal process parameters of a wave soldering process were defined. The optimization was performed in respect to soldering quality by minimizing a cost function describing the total repairing cost of a wave-soldered printed circuit board (PCB). The data analysis stages were as follows. First, the process data were coded into inputs for a self-organizing map (SOM). Next, a function for the repairing cost was constructed and used to find the optimal map neurons. At the last phase, the optimal parameters were approximated on the basis of the reference vectors of the optimal neurons. The results showed clearly potential in the optimization of the wave soldering process, especially in the visualization of the optimal process conditions. Therefore, it would be useful to exploit the method more widely in the electronics industry.",2011,1584068,"Mika Liukkonen,Elina Havia,Hannu Leinonen,Yrjö Hiltunen",0
1584069,On generating interpretable and precise fuzzy systems based on Pareto multi-objective cooperative co-evolutionary algorithm.,"A novel approach to construct a set of interpretable and precise fuzzy systems based on the Pareto multi-objective cooperative co-evolutionary algorithm (PMOCCA) is proposed in this paper. Firstly, feature selection is used to reduce the dimensionality of the data in order to improve the performance and reduce computational burden. Secondly, the fuzzy clustering algorithm is employed to identify the initial fuzzy system. Thirdly, the PMOCCA is carried out to evolve the initial fuzzy system to optimize the number of rules, the antecedents of the rules and the parameters of the antecedents simultaneously. Furthermore, the interpretability-driven simplification techniques are used iteratively to reduce the fuzzy systems, thus the interpretability of the fuzzy systems is improved. Finally, the proposed approach is applied to benchmark problems, and the results show its validity.",2011,1584069,"Yong Zhang,Xiaobei Wu,Zongyi Xing,Weili Hu",0
1584070,Ant clustering PHD filter for multiple-target tracking.,"A novel ant clustering filtering algorithm, under the guidance of first-order statistic moment of posterior multiple-target state (probability hypothesis density), is investigated and applied to estimate the time-varying number of targets and their individual states in a cluttered environment. The ant clustering filtering algorithm includes two clustering steps: the first step is called rough ant clustering, which involves the stochastic selection of each ant and its state local adjustment according to the current likelihood function and posterior intensity, respectively; while the second is called fine ant clustering, which employs these ants to extract the multiple-target state. Numerical simulations verify the tracking multiple-target capability of our proposed algorithm through performance comparison with the Sequential Monte Carlo (SMC) method.",2011,1584070,"Benlian Xu,Huigang Xu,Jihong Zhu",0
1584071,A polar coordinate particle swarm optimiser.,"The Particle Swarm Optimisation (PSO) algorithm consists of a population (or swarm) of particles that are ''flown'' through an n-dimensional space in search of a global best solution to an optimisation problem. PSO operates in Cartesian space, producing Cartesian solution vectors. By making use of an appropriate mapping function the algorithm can be modified to search in polar space. This mapping function is used to convert the position vectors (now defined in polar space) to Cartesian space such that the fitness value of each particle can be calculated accordingly. This paper introduces the polar PSO algorithm that is able to search in polar space. This new algorithm is compared to its Cartesian counterpart on a number of benchmark functions. Experimental results show that the polar PSO outperforms the Cartesian PSO in low dimensions when both algorithms are applied to the search for eigenvectors of different nxn square matrices. Performance of the polar PSO on general unconstrained functions is not as good as the Cartesian PSO, which emphasizes the main conclusion of this paper, namely that the PSO is not an efficient search algorithm for general unconstrained optimisation problems defined in polar space.",2011,1584071,"Wiehann Matthysen,Andries Petrus Engelbrecht",0
1584072,A novel clustering approach: Artificial Bee Colony (ABC) algorithm.,"Artificial Bee Colony (ABC) algorithm which is one of the most recently introduced optimization algorithms, simulates the intelligent foraging behavior of a honey bee swarm. Clustering analysis, used in many disciplines and applications, is an important tool and a descriptive task seeking to identify homogeneous groups of objects based on the values of their attributes. In this work, ABC is used for data clustering on benchmark problems and the performance of ABC algorithm is compared with Particle Swarm Optimization (PSO) algorithm and other nine classification techniques from the literature. Thirteen of typical test data sets from the UCI Machine Learning Repository are used to demonstrate the results of the techniques. The simulation results indicate that ABC algorithm can efficiently be used for multivariate data clustering.",2011,1584072,"Dervis Karaboga,Celal Ozturk",0
1584073,Timing of resources exploration in the behavior of firm - Empirical simulation by intelligent hybrid model.,"We have insight into the importance of resource exploration derived from the quest for sustaining competitive advantage as well as the growth of the firm, which are well-explicated in the resources point of view. However, we really do not know when the firm will seriously commit to this kind of activities. Therefore, this study proposes an intelligent hybrid model using quantum minimization (QM) to tune a composite model adaptive support vector regression (ASVR) and nonlinear generalized autoregressive conditional heteroscedasticity (NGARCH) such that it constitutes the relationship among five indicators, the growth rate of long-term investment, the firm size, the return on total asset, the return on common equity, and the return on sales. In particular, this proposed approach outperforms several typical methods such as auto-regressive moving-average regression (ARMAX), back-propagation neural network (BPNN), or adaptive neuron-fuzzy inference system (ANFIS) for this timing problem in term of comparing their achievement and the goodness of fit. Consequently, the preceding methods involved in this problem truly explain the timing of resources exploration in the behavior of firm. Meanwhile, the performance summary among methods is compared quantitatively.",2011,1584073,"Bao Rong Chang,Hsiu Fen Tsai",0
1584074,A new chaos-based fast image encryption algorithm.,"In recent years, various image encryption algorithms based on the permutation-diffusion architecture have been proposed where, however, permutation and diffusion are considered as two separate stages, both requiring image-scanning to obtain pixel values. If these two stages are combined, the duplicated scanning effort can be reduced and the encryption can be accelerated. In this paper, a fast image encryption algorithm with combined permutation and diffusion is proposed. First, the image is partitioned into blocks of pixels. Then, spatiotemporal chaos is employed to shuffle the blocks and, at the same time, to change the pixel values. Meanwhile, an efficient method for generating pseudorandom numbers from spatiotemporal chaos is suggested, which further increases the encryption speed. Theoretical analyses and computer simulations both confirm that the new algorithm has high security and is very fast for practical image encryption.",2011,1584074,"Yong Wang,Kwok-Wo Wong,Xiaofeng Liao,Guanrong Chen",0
1584075,A Tabu Search algorithm for dynamic routing in ATM cell-switching networks.,This paper deals with the dynamic routing problem in ATM cell-switching networks. We present a mathematical programming model based on cell loss and a Tabu Search algorithm with short-term memory that is reinforced with a long-term memory procedure. The estimation of the quality of the solutions is fast due to the specific encoding of the feasible solutions. The Tabu Search algorithm reaches good quality solutions outperforming other approaches such as Genetic Algorithms and the Minimum Switching Path heuristic attending both to the cell loss and the CPU time consumption. The best results were found for the more complex networks with a high number of switches and links.,2011,1584075,"Pablo Cortés,Jesús Muñuzuri,Luis Onieva,J. Fernández",0
1584076,Simulation optimization using particle swarm optimization algorithm with application to assembly line design.,"Assembly line design is an important part of production system. Some processes need to undergo changes in order to increase in efficiency. Computer simulation has been applied on process design for many decades. Traditionally, simulation had to run all possible alternatives of assembly line and was not considered as an optimization technique. Thus, this study employs particle swarm optimization (PSO) algorithm which is with mutation based on similarity for simulation optimization in order to optimize the managerial parameters in production system. Through experimentation designs and statistics tests, the simulation results show that the proposed method is better than other algorithms, like genetic algorithm and conventional PSO algorithm for solving assembly line design problem.",2011,1584076,"R. J. Kuo,C. Y. Yang",0
1584077,Chaotic maps based on binary particle swarm optimization for feature selection.,"Feature selection is a useful pre-processing technique for solving classification problems. The challenge of solving the feature selection problem lies in applying evolutionary algorithms capable of handling the huge number of features typically involved. Generally, given classification data may contain useless, redundant or misleading features. To increase classification accuracy, the primary objective is to remove irrelevant features in the feature space and to correctly identify relevant features. Binary particle swarm optimization (BPSO) has been applied successfully to solving feature selection problems. In this paper, two kinds of chaotic maps-so-called logistic maps and tent maps-are embedded in BPSO. The purpose of chaotic maps is to determine the inertia weight of the BPSO. We propose chaotic binary particle swarm optimization (CBPSO) to implement the feature selection, in which the K-nearest neighbor (K-NN) method with leave-one-out cross-validation (LOOCV) serves as a classifier for evaluating classification accuracies. The proposed feature selection method shows promising results with respect to the number of feature subsets. The classification accuracy is superior to other methods from the literature.",2011,1584077,"Li-Yeh Chuang,Cheng-Hong Yang,Jung-Chike Li",0
1584078,Hardware based genetic evolution of self-adaptive arbitrary response FIR filters.,"This work presents a hardware implementation of an FIR filter that is self-adaptive; that responds to arbitrary frequency response landscapes; that has built-in coefficient error tolerance capabilities; and that has a minimal adaptation latency. This hardware design is based on a heuristic genetic algorithm. Experimental results show that the proposed design is more efficient than non-evolutionary designs even for arbitrary response filters. As a byproduct, the paper also presents a novel flow for the complete hardware design of what is termed as an Evolutionary System on Chip (ESoC). With the inclusion of an evolutionary process, the ESoC is a new paradigm in modern System on Chip (SoC) designs. The ESoC methodology could be a very useful structured FPGA/ASIC implementation alternative in many practical applications of FIR filters.",2011,1584078,"Shoaib Mohammed,S. K. Noor Mahammad,V. Kamakoti",0
1584079,Clustered-Hybrid Multilayer Perceptron network for pattern recognition application.,"This paper introduces a modified version of the Hybrid Multilayer Perceptron (HMLP) network to improve the performance of the conventional HMLP network. We adopted the Clustering Algorithm from the Radial Basis Function (RBF) network architecture and incorporated it into the conventional HMLP network architecture. The modified model is called Clustered-Hybrid Multilayer Perceptron (Clustered-HMLP) network. The proposed Clustered-HMLP network architecture is trained using modified training algorithm called Clustered-Modified Recursive Prediction Error (Clustered-MRPE). The capability of the Clustered-HMLP network with Clustered-MRPE training algorithm is demonstrated using seven benchmark datasets from the University of California at Irvine (UCI) machine learning repository (i.e. Iris, Ionosphere, Pima Indian Diabetes, Wine, Lung Cancer, Hayes-Roth and Glass) and compared with the performance of other twelve classifiers reported in literature. Further, the new network is implemented to model a Transformer Fault Diagnosis System and Aggregate Shape Identification System. The results indicate that the proposed Clustered-HMLP network outperforms other eleven classifiers and provides a significant improvement to the conventional HMLP network for pattern recognition application.",2011,1584079,"Nor Ashidi Mat Isa,Wan Mohd Fahmi Wan Mamat",0
1584080,A genetic algorithm for optimization problems with fuzzy relation constraints using max-product composition.,"We consider nonlinear optimization problems constrained by a system of fuzzy relation equations. The solution set of the fuzzy relation equations being nonconvex, in general, conventional nonlinear programming methods are not practical. Here, we propose a genetic algorithm with max-product composition to obtain a near optimal solution for convex or nonconvex solution set. Test problems are constructed to evaluate the performance of the proposed algorithm showing alternative solutions obtained by our proposed model.",2011,1584080,"Reza Hassanzadeh,Esmaile Khorram,Iraj Mahdavi,Nezam Mahdavi-Amiri",0
1584081,A meta-heuristic framework for forecasting household electricity consumption.,"It may be difficult to model household electricity consumption with conventional methods such as regression due to seasonal and monthly changes. This paper illustrates a flexible integrated meta-heuristic framework based on Artificial Neural Network (ANN) Multi Layer Perceptron (MLP), conventional regression and design of experiment (DOE) for forecasting household electricity consumption. Previous studies base their verification by the difference in error estimation, whereas this study uses various error estimation methods and design of experiment (DOE). Moreover, DOE is based on analysis of variance (ANOVA) and Duncan Multiple Range Test (DMRT). Furthermore, actual data is compared with ANN MLP and conventional regression model through ANOVA. If the null hypothesis is accepted, DMRT is used to select either ANN MLP or conventional regression. However, if the null hypothesis is accepted then the proposed framework selects either the MLP or regression model based on the average of Minimum Absolute Percentage Error (MAPE), Mean Square Error (MSE) and Mean Absolute Error (MAE). The significance of this study is the integration of ANN MLP, conventional regression and DOE for flexible modeling and improved processing, development and testing of household electricity consumption. Some of the previous studies assume that ANN MLP provide better estimation and others estimate electricity consumptions based on the conventional regression approach. However, this study presents a flexible integrated framework to locate the best model based on the actual data. Moreover, it would provide more reliable and precise forecasting for policy makers. To show the applicability and superiority of the integrated approach, annual household electricity consumption in Iran from 1974 to 2003 was collected for processing, training and testing purpose.",2011,1584081,"A. Azadeh,Z. S. Faiz",0
1584082,A computer vision approach for weeds identification through Support Vector Machines.,"This paper outlines an automatic computer vision system for the identification of avena sterilis which is a special weed seed growing in cereal crops. The final goal is to reduce the quantity of herbicide to be sprayed as an important and necessary step for precision agriculture. So, only areas where the presence of weeds is important should be sprayed. The main problems for the identification of this kind of weed are its similar spectral signature with respect the crops and also its irregular distribution in the field. It has been designed a new strategy involving two processes: image segmentation and decision making. The image segmentation combines basic suitable image processing techniques in order to extract cells from the image as the low level units. Each cell is described by two area-based attributes measuring the relations among the crops and weeds. The decision making is based on the Support Vector Machines and determines if a cell must be sprayed. The main findings of this paper are reflected in the combination of the segmentation and the Support Vector Machines decision processes. Another important contribution of this approach is the minimum requirements of the system in terms of memory and computation power if compared with other previous works. The performance of the method is illustrated by comparative analysis against some existing strategies.",2011,1584082,"Alberto Tellaeche,Gonzalo Pajares,Xavier P. Burgos-Artizzu,Angela Ribeiro",0
1584083,Multilevel image segmentation with adaptive image context based thresholding.,"Neural network based image segmentation techniques primarily focus on the selection of appropriate thresholding points in the image feature space. Research initiatives in this direction aim at addressing this problem of effective threshold selection for activation functions. Multilevel activation functions resort to fixed and uniform thresholding mechanisms. These functions assume homogeneity of the image information content. In this paper, we propose a collection of adaptive thresholding approaches to multilevel activation functions. The proposed thresholding mechanisms incorporate the image context information in the thresholding process. Applications of these mechanisms are demonstrated on the segmentation of real life multilevel intensity images using a self-supervised multilayer self-organizing neural network (MLSONN) and a supervised pyramidal neural network (PyraNet). We also present a bi-directional self-organizing neural network (BDSONN) architecture suitable for multilevel image segmentation. The architecture uses an embedded adaptive thresholding mechanism to a characteristic multilevel activation function. The segmentation efficiencies of the thresholding mechanisms evaluated using four unsupervised measures of merit, are reported for the three neural network architectures considered.",2011,1584083,"Siddhartha Bhattacharyya,Ujjwal Maulik,Paramartha Dutta",0
1584084,Preferences in multi-objective evolutionary optimisation of electric motor speed control with hardware in the loop.,"This paper presents the design of a robust Pareto-optimal controller with the designer preference articulation for a Permanent Magnet Synchronous Motor (PMSM). An evolutionary multi-objective optimisation (EMOO) algorithm is used to tune the proportional integral (PI) speed regulator in the Direct Torque Control drive system. Approximation of the Pareto front with hardware in the loop (HiL) is chosen as an alternative to the time-consuming software simulation studies. Thanks to this approach problems of un-modelled plant dynamics are alleviated and additional manual tuning on-line is not required. The weak point of the HiL approach is caused by disruptive presence of noise which affects the performance of EMOO. This influence is strongly problem-dependent; therefore no generalized results have yet been presented in the literature. In this paper the robustness features of the proposed design approach are verified using the state-of-the-art multi-objective evolutionary Non-dominated Sorting Genetic Algorithm (NSGA-II; Deb, 2001 [1]). The on-line optimised motor drive speed controller is shown to be effective, possessing good dynamic characteristics, demonstrating applicability of the a priori preference articulation technique to the controller design. The final Pareto-optimal solution is selected according to the designer's preference articulation before the Pareto front is approximated by EMOO.",2011,1584084,Piotr Wozniak,0
1584085,Incremental modeling with rough and fine tuning method.,"In this paper, we propose a new learning approach for designing an incremental model that has a cascade learning structure combined with a rough and fine tuning method for the learning scheme. Recently, various fuzzy logic-based modeling methods, with fuzzy if-then type rules, have been proposed in an attempt to obtain good approximations and generalization performances. In contrast to these various modeling methods, the new proposed incremental modeling scheme presented here is combined with a rough and fine tuning scheme, to learn and construct the best architecture for the model. A compensation idea is introduced in the fine tuning stage to solve the over-fitting problem caused from testing data. For this purpose, a construct of an extreme learning machine (ELM) is used as a global model, and this is compensated through a conditional fuzzy C-means (CFCM)-based fuzzy inference system (FIS) with a Takagi-Sugeno-Kang (TSK)-type method, which captures the remaining localized nonlinearities of the model. The experimental results, obtained by the proposed model have proved to show better performances in comparison with previous works.",2011,1584085,"Sung-Suk Kim,Keun Chang Kwak",0
1584086,Multiobjective optimization for manpower assignment in consulting engineering firms.,"In this article, a new multiobjective optimization model, MUST, is proposed to facilitate the staff-to-job assignment in consulting engineering firms. In addition to the typical objective of maximizing profits, other human resource related objectives are also incorporated to balance workloads, avoid excessive overtime, and eliminate demoralizing idleness while giving preference to projects with specified priorities. The present optimization problem is of significant complexity (nonlinear, non-smooth, and combinatorial) and has been proved NP- and #P-complete. To handle all the difficulties, MUST incorporates a particle swarm optimization algorithm to approximate the tradeoff surface consisting of non-dominated solutions. The application of MUST is demonstrated through a numerical case of assigning six engineering teams to fifteen incoming projects. It has been shown that non-dominated solutions generated by MUST help decision makers choose the compromised assignment plan which is otherwise hard and time-consuming to obtain. The comparisons with SPEA2 and LINGO verify the effectiveness and efficiency of MUST.",2011,1584086,"I-Tung Yang,Jui-Sheng Chou",0
1584087,A multiobjective approach to MR brain image segmentation.,"This article proposes a novel multiobjective real coded genetic fuzzy clustering scheme for segmentation of multispectral magnetic resonance image (MRI) of the human brain. The proposed technique is able to automatically evolve the number of clusters along with the clustering result. The multiobjective variable string length clustering technique encodes the cluster centers in its chromosomes and simultaneously optimizes the global fuzzy compactness and fuzzy separation among the clusters. In the final generation, it produces a set of non-dominated solutions, from which the best solution in terms of a recently proposed validity index I is chosen to be the final clustering solution. The corresponding chromosome length provides the number of clusters. The proposed method is applied on many simulated T1-weighted, T2-weighted and proton density-weighted normal and MS lesion MRI brain images. Superiority of the proposed method over K-means, Fuzzy C-means, Expectation Maximization, hierarchical clustering, Single Objective Genetic clustering and other recent multiobjective clustering algorithms has been demonstrated quantitatively. The automatic segmentation obtained by the proposed clustering technique is also compared with the available ground truth information.",2011,1584087,"Anirban Mukhopadhyay,Ujjwal Maulik",0
1584088,Railway freight transportation planning with mixed uncertainty of randomness and fuzziness.,"The railway freight transportation planning problem under the mixed uncertain environment of fuzziness and randomness is investigated in this paper, in which the optimal paths, the amount of commodities passing through each path and the frequency of services need to be determined. Based on the chance measure and critical values of the random fuzzy variable, three chance-constrained programming models are constructed for the problem with respect to different criteria. Some equivalents of objectives and constraints are also discussed in order to investigate mathematical properties of the models. To solve the models, a potential path searching algorithm, simulation algorithms and a genetic algorithm are integrated as a hybrid algorithm to solve an optimal solution. Finally, some numerical examples are performed to show the applications of the models and the algorithm.",2011,1584088,"Lixing Yang,Ziyou Gao,Keping Li",0
1584089,Integrating dominance properties with genetic algorithms for parallel machine scheduling problems with setup times.,"This paper deals with an unrelated parallel machine scheduling problem with the objective of minimizing the makespan. There are machine-dependent and job sequence-dependent setup times and all jobs are available at time zero. This is a NP-hard problem and a set of dominance properties are developed including inter-machine (i.e., adjacent and non-adjacent interchange) and intra-machine switching properties as necessary conditions of job sequencing orders in an optimal schedule. As a result, by applying these dominance properties for a given sequence, a near-optimal solution can be derived. In addition, a new meta-heuristic is introduced by integrating the dominance properties with genetic algorithm to further improve the solution quality for larger problems. The performance of this meta-heuristic is evaluated by using benchmark problems from the literature. The intensive experimental results show that GADP can find all optimal solutions for the small problems and outperformed the solutions obtained by the existing heuristics for larger problems.",2011,1584089,"Pei-Chann Chang,Shih-Hsin Chen",0
1584090,Hybrid GA-BF based intelligent PID controller tuning for AVR system.,"This paper deals with hybrid system (GA-BF) based on the conventional GA (Genetic Algorithm) and BF (Bacterial Foraging) which is the social foraging behavior of bacteria. A variety of test function is introduced and simulated to illustrate the characteristics and performance by mutation, crossover, variation of step size, variation of chemotactic step, and variation of lifetime of bacteria in the proposed hybrid system GA-BF. The simulated results represent that the proposed method is highly satisfactory. This approach provides us with novel hybrid model based on foraging behavior and also with a possible new connection between evolutionary forces in social foraging and distributed nongradient optimization algorithm design for global optimization over noisy surfaces.",2011,1584090,Dong Hwa Kim,0
1584091,A note on the learning automata based algorithms for adaptive parameter selection in PSO.,"PSO, like many stochastic search methods, is very sensitive to efficient parameter setting such that modifying a single parameter may cause a considerable change in the result. In this paper, we study the ability of learning automata for adaptive PSO parameter selection. We introduced two classes of learning automata based algorithms for adaptive selection of value for inertia weight and acceleration coefficients. In the first class, particles of a swarm use the same parameter values adjusted by learning automata. In the second class, each particle has its own characteristics and sets its parameter values individually. In addition, for both classed of proposed algorithms, two approaches for changing value of the parameters has been applied. In first approach, named adventurous, value of a parameter is selected from a finite set while in the second approach, named conservative, value of a parameter either changes by a fixed amount or remains unchanged. Experimental results show that proposed learning automata based algorithms compared to other schemes such as SPSO, PSOIW, PSO-TVAC, PSOLP, DAPSO, GPSO, and DCPSO have the same or even higher ability to find better solutions. In addition, proposed algorithms converge to stopping criteria for some of the highly multi modal functions significantly faster.",2011,1584091,"Ali B. Hashemi,Mohammad Reza Meybodi",0
1584092,The mass appraisal of the real estate by computational intelligence.,"Mass appraisal is the systematic appraisal of groups of properties as of a given date using standardized procedures and statistical testing. Mass appraisal is commonly used to compute real estate tax. There are three traditional real estate valuation methods: the sales comparison approach, income approach, and the cost approach. Mass appraisal models are commonly based on the sales comparison approach. The ordinary least squares (OLS) linear regression is the classical method used to build models in this approach. The method is compared with computational intelligence approaches - support vector machine (SVM) regression, multilayer perceptron (MLP), and a committee of predictors in this paper. All the three predictors are used to build a weighted data-depended committee. A self-organizing map (SOM) generating clusters of value zones is used to obtain the data-dependent aggregation weights. The experimental investigations performed using data cordially provided by the Register center of Lithuania have shown very promising results. The performance of the computational intelligence-based techniques was considerably higher than that obtained using the official real estate models of the Register center. The performance of the committee using the weights based on zones obtained from the SOM was also higher than of that exploiting the real estate value zones provided by the Register center.",2011,1584092,"Vilius Kontrimas,Antanas Verikas",0
1584093,Classifier design for static security assessment using particle swarm optimization.,"Security is recognized as an important problem in planning, design and operation stages of electric power systems. Power system security assessment deals with the system's ability to continue to provide service in the event of an unforeseen contingency. This paper proposes a particle swarm optimization (PSO) based classification for static security evaluation in power systems. A straightforward and quick procedure is used to select a small number of variables as features from a large set of variables which are normally available in power systems. A simple first order security function is designed using the selected features for classification. The training of weights in the classifier function (security function) is carried out by PSO technique. The PSO algorithm has minimized the error rate in classification. The procedure to determine the security function (classifier) is discussed. The performance of the algorithm is tested on IEEE 14 Bus, IEEE 57 Bus and IEEE 118 Bus systems. Simulation results show that the PSO classifier gives a fairly high classification accuracy and less misclassification rate.",2011,1584093,"S. Kalyani,K. S. Swarup",0
1584094,A fuzzy logic model for predicting the development effort of short scale programs based upon two independent variables.,"Fuzzy models have been recently used for estimating the development effort of software projects and this practice could start with short scale programs. In this paper, new and changed (N&C) as well as reused code were gathered from small programs developed by 74 programmers using practices of the Personal Software Process; these data were used as input for a fuzzy model for estimating the development effort. Accuracy of this fuzzy model was compared with the accuracy of a statistical regression model. Two samples of 163 and 68 programs were used for verifying and validating respectively the models; the comparison criterion was the Mean Magnitude of Error Relative to the estimate (MMER). In verification and validation stages, fuzzy model kept a MMER lower or equal than that regression model and an accuracies comparison of the models based on ANOVA, did not show a statistically significant difference amongst their means. This result suggests that fuzzy logic could be used for predicting the effort of small programs based upon these two kinds of lines of code.",2011,1584094,Cuauhtémoc López Martín,0
1584095,Software project portfolio optimization with advanced multiobjective evolutionary algorithms.,"Large software companies have to plan their project portfolio to maximize potential portfolio return and strategic alignment, while balancing various preferences, and considering limited resources. Project portfolio managers need methods and tools to find a good solution for complex project portfolios and multiobjective target criteria efficiently. However, software project portfolios are challenging to describe for optimization in a practical way that allows efficient optimization. In this paper we propose an approach to describe software project portfolios with a set of multiobjective criteria for portfolio managers using the COCOMO II model and introduce a multiobjective evolutionary approach, mPOEMS, to find the Pareto-optimal front efficiently. We evaluate the new approach with portfolios choosing from a set of 50 projects that follow the validated COCOMO II model criteria and compare the performance of the mPOEMS approach with state-of-the-art multiobjective optimization evolutionary approaches. Major results are as follows: the portfolio management approach was found usable and useful; the mPOEMS approach outperformed the other approaches.",2011,1584095,"Thomas Kremmel,Jirí Kubalík,Stefan Biffl",0
1584096,Employing multiple-kernel support vector machines for counterfeit banknote recognition.,"Finding an efficient method to detect counterfeit banknotes is an imperative task in business transactions. In this paper, we propose a system based on multiple-kernel support vector machines for counterfeit banknote recognition. A support vector machine (SVM) to minimize false rates is developed. Each banknote is divided into partitions and the luminance histograms of the partitions are taken as the input of the system. Each partition is associated with its own kernels. Linearly weighted combination is adopted to combine multiple kernels into a combined matrix. Optimal weights with kernel matrices in the combination are obtained through semi-definite programming (SDP) learning. Two strategies are adopted to reduce the amount of time and space required by the SDP method. One strategy assumes the non-negativity of the kernel weights, and the other one is to set the sum of the weights to be unity. Experiments with Taiwanese banknotes show that the proposed approach outperforms single-kernel SVMs, standard SVMs with SDP, and multiple-SVM classifiers.",2011,1584096,"Chi-Yuan Yeh,Wen-Pin Su,Shie-Jue Lee",0
1584097,Multiple Instance Learning with Multiple Objective Genetic Programming for Web Mining.,"This paper introduces a multi-objective grammar based genetic programming algorithm, MOG3P-MI, to solve a Web Mining problem from the perspective of multiple instance learning. This algorithm is evaluated and compared to other algorithms that were previously used to solve this problem. Computational experiments show that the MOG3P-MI algorithm obtains the best results, adds comprehensibility and clarity to the knowledge discovery process and overcomes the main drawbacks of previous techniques obtaining solutions which maintain a balance between conflicting measurements like sensitivity and specificity.",2011,1584097,"Amelia Zafra,Eva Lucrecia Gibaja Galindo,Sebastián Ventura",0
1584098,Differential Evolution performances for the solution of mixed-integer constrained process engineering problems.,"An important number of publications deal with the computational efficiency of a novel Evolutionary Algorithm called Differential Evolution (DE). However, there is still a noticeable lack of studies on DE's performance on engineering problems, which combine large-size instances, constraint-handling and mixed-integer variables issues. This paper proposes the solution by DE of process engineering problems and compares its computational performance with an exact optimization method (Branch-and-Bound) and with a Genetic Algorithm. Two analytical formulations are used to model the batch plant design problem and a set of examples gathering the three above-mentioned issues are also provided. The computational results obtained highlight the clear superiority of DE since its best found solutions always lie very close to the Branch-and-Bound optima. Moreover, for an equal number of objective function evaluations, the results repeatability was found to be much better for the DE method than for the Genetic Algorithm.",2011,1584098,"Antonin Ponsich,Carlos Artemio Coello Coello",0
1584099,An incremental adaptive neural network model for online noisy data regression and its application to compartment fire studies.,"This paper presents a probabilistic-entropy-based neural network (PENN) model for tackling online data regression problems. The network learns online with an incremental growth network structure and performs regression in a noisy environment. The training samples presented to the model are clustered into hyperellipsoidal Gaussian kernels in the joint space of the input and output domains by using the principles of Bayesian classification and minimization of entropy. The joint probability distribution is established by applying the Parzen density estimator to the kernels. The prediction is carried out by evaluating the expected conditional mean of the output space with the given input vector. The PENN model is demonstrated to be able to remove symmetrically distributed noise embedded in the training samples. The performance of the model was evaluated by three benchmarking problems with noisy data (i.e., Ozone, Friedman#1, and Santa Fe Series E). The results show that the PENN model is able to outperform, statistically, other artificial neural network models. The PENN model is also applied to solve a fire safety engineering problem. It has been adopted to predict the height of the thermal interface which is one of the indicators of fire safety level of the fire compartment. The data samples are collected from a real experiment and are noisy in nature. The results show the superior performance of the PENN model working in a noisy environment, and the results are found to be acceptable according to industrial requirements.",2011,1584099,Eric Wai Ming Lee,0
1584100,Hybrid robust support vector machines for regression with outliers.,"In this study, a hybrid robust support vector machine for regression is proposed to deal with training data sets with outliers. The proposed approach consists of two stages of strategies. The first stage is for data preprocessing and a support vector machine for regression is used to filter out outliers in the training data set. Since the outliers in the training data set are removed, the concept of robust statistic is not needed for reducing the outliers' effects in the later stage. Then, the training data set except for outliers, called as the reduced training data set, is directly used in training the non-robust least squares support vector machines for regression (LS-SVMR) or the non-robust support vector regression networks (SVRNs) in the second stage. Consequently, the learning mechanism of the proposed approach is much easier than that of the robust support vector regression networks (RSVRNs) approach and of the weighted LS-SVMR approach. Based on the simulation results, the performance of the proposed approach with non-robust LS-SVMR is superior to the weighted LS-SVMR approach when the outliers exist. Moreover, the performance of the proposed approach with non-robust SVRNs is also superior to the RSVRNs approach.",2011,1584100,"Chen-Chia Chuang,Zne-Jung Lee",0
1584101,Designing a hierarchical neural network based on fuzzy clustering for fault diagnosis of the Tennessee-Eastman process.,"This paper proposes a hierarchical artificial neural network (HANN) for isolating the faults of the Tennessee-Eastman process (TEP). The TEP process is the simulation of a chemical plant created by the Eastman Chemical Company to provide a realistic industrial process for evaluating process control and monitoring methods The first step in designing the HANN is to divide the fault patterns space into a few sub-spaces through using fuzzy C-means clustering algorithm. For each sub-space of fault patterns a special neural network has been trained in order to diagnose the faults of that sub-space. A supervisor network has been developed to decide which one of the special neural networks should be triggered. In this regard, each neural network in the proposed HANN has been given a specific duty, so the proposed procedure can be called Duty-Oriented HANN (DOHANN). The neuromorphic structure of the networks is based on multilayer perceptron (MLP) networks. The simulation of Tennessee-Eastman (TE) process has been used to generate the required training and test data. The performance of the developed method has been evaluated and compared to that of a conventional single neural network (SNN) as well as the technique of dynamic principal component analysis (DPCA). The simulation results indicate that the DOHANN diagnoses the TEP faults considerably better than SNN and DPCA methods. Training of each MLP network for the DOHANN model has required less computer time in comparison to SNN model. This is because of structurally simpler MLPs used by the developed DOHANN method.",2011,1584101,Reza Eslamloueyan,0
1584102,Quality of Service constrained routing optimization using Evolutionary Computation.,"In this work, a novel optimization framework is proposed that allows the improvement of Quality of Service levels in TCP/IP based networks, by configuring the routing weights of link-state protocols such as OSPF. Since this is a NP-hard problem, some algorithms from Evolutionary Computation were considered, working over a mathematical model that allows the definition of flexible cost functions that can take into account several measures of the network behaviour, such as network congestion and end-to-end delays. A number of experiments were performed, over a large set of network topologies, where Evolutionary Algorithms (EAs), Differential Evolution, local search methods and common heuristics were compared. EAs make the most promising alternative leading to solutions with an effective network performance, even under unfavourable scenarios. A number of state of the art multi-objective optimization algorithms were also tested, but the proposed EAs still hold as the most consistent method for network optimization.",2011,1584102,"Miguel Rocha,Pedro Sousa,Paulo Cortez,Miguel Rio",0
1584103,A new asynchronous parallel global optimization method based on simulated annealing and differential evolution.,"This paper presents a new asynchronous parallel global optimization method and its application to the automated device sizing in analog integrated circuit (IC) design. The method is based on the simulated annealing algorithm (SA), but incorporates features from differential evolution (DE) to improve the sampling efficiency and avoid the problems involved with the cooling schedule selection. A simple local search procedure is also incorporated to improve the fine tuning capabilities of the method. To reduce the optimization time, the method is designed as an asynchronous master-slave parallel system that allows simultaneous evaluation of several trial solutions. Comparison with simple SA and DE on a set of well-known analytical test functions confirms the method's efficiency. The parallel efficiency of the method is also verified by optimizing the functions with 1, 2, 4, and 8 processors. The proposed approach is also applied to several real world cases of device sizing in analog IC design. The optimization results indicate that the method is capable of finding near optimal circuits. The parallel efficiency of the method is confirmed with optimization runs on 1, 2, 4, and 8 processors.",2011,1584103,"Jernej Olensek,Tadej Tuma,Janez Puhan,Árpád Bürmen",0
1584104,Using GMDH-based networks for improved spam detection and email feature analysis.,"Unsolicited or spam email has recently become a major threat that can negatively impact the usability of electronic mail. Spam substantially wastes time and money for business users and network administrators, consumes network bandwidth and storage space, and slows down email servers. In addition, it provides a medium for distributing harmful code and/or offensive content. In this paper, we explore the application of the GMDH (Group Method of Data Handling) based inductive learning approach in detecting spam messages by automatically identifying content features that effectively distinguish spam from legitimate emails. We study the performance for various network model complexities using spambase, a publicly available benchmark dataset. Results reveal that classification accuracies of 91.7% can be achieved using only 10 out of the available 57 attributes, selected through abductive learning as the most effective feature subset (i.e. 82.5% data reduction). We also show how to improve classification performance using abductive network ensembles (committees) trained on different subsets of the training data. Comparison with other techniques such as neural networks and naive Bayesian classifiers shows that the GMDH-based learning approach can provide better spam detection accuracy with false-positive rates as low as 4.3% and yet requires shorter training time.",2011,1584104,"El-Sayed M. El-Alfy,Radwan E. Abdel-Aal",0
1584105,A fast pruning redundant rule method using Galois connection.,"Besides preprocessing, post-analysis also plays an important role in knowledge discovery. It can effectively assist users to grasp the obtained knowledge. However, many of data mining algorithms merely take performance into consideration and put the post-analysis of results aside. They generate a modest number of rules for the purpose of improving accuracy. Unfortunately, most induced rules are redundant or insignificant. Their presence not only confuses end-users in post-analysis, but also degrades efficiency in future decision task. Thus, it is necessary to eliminate redundant or irrelevant rules as more as possible. In this paper, we present an efficient post-processing method to prune redundant rules by virtue of the property of Galois connection, which inherently constrains rules with respect to objects. Its advantage is that information will not be lost greatly during pruning step. The experimental evaluation shows that the proposed method is competent for discarding a large number of superfluous rules effectively and a high compression factor will be achieved. What's more, the computational cost of our method is surprisingly lower than the Apriori method.",2011,1584105,"Huawen Liu,Lei Liu,Huijie Zhang",0
1584106,NEWER: A system for NEuro-fuzzy WEb Recommendation.,"In the era of the Web, there is urgent need for developing systems able to personalize the online experience of Web users on the basis of their needs. Web recommendation is a promising technology that attempts to predict the interests of Web users, by providing them with information and/or services that they need without explicitly asking for them. In this paper we propose NEWER, a usage-based Web recommendation system that exploits the potential of Computational Intelligence techniques to dynamically suggest interesting pages to users according to their preferences. NEWER employs a neuro-fuzzy approach in order to determine categories of users sharing similar interests and to discover a recommendation model as a set of fuzzy rules expressing the associations between user categories and relevances of pages. The discovered model is used by a online recommendation module to determine the list of links judged relevant for users. The results obtained on both synthetic and real-world data show that NEWER is effective for recommendation, leading to a quality of the generated recommendations comparable and often significantly better than those of other approaches employed for the comparison.",2011,1584106,"Giovanna Castellano,Anna Maria Fanelli,Maria Alessandra Torsello",0
1584107,Systematic image processing for diagnosing brain tumors: A Type-II fuzzy expert system approach.,"This paper presents a systematic Type-II fuzzy expert system for diagnosing the human brain tumors (Astrocytoma tumors) using T""1-weighted Magnetic Resonance Images with contrast. The proposed Type-II fuzzy image processing method has four distinct modules: Pre-processing, Segmentation, Feature Extraction, and Approximate Reasoning. We develop a fuzzy rule base by aggregating the existing filtering methods for Pre-processing step. For Segmentation step, we extend the Possibilistic C-Mean (PCM) method by using the Type-II fuzzy concepts, Mahalanobis distance, and Kwon validity index. Feature Extraction is done by Thresholding method. Finally, we develop a Type-II Approximate Reasoning method to recognize the tumor grade in brain MRI. The proposed Type-II expert system has been tested and validated to show its accuracy in the real world. The results show that the proposed system is superior in recognizing the brain tumor and its grade than Type-I fuzzy expert systems.",2011,1584107,"Mohammad Hossein Fazel Zarandi,Marzie Zarinbal,M. Izadi",0
1584108,Fuzzy MCDM approach for selecting the best environment-watershed plan.,"In the real word, the decision-making problems are very vague and uncertain in a number of ways. Most of the criteria have interdependent and interactive features, so they cannot be evaluated by conventional measure method. Such as the feasibility, thus, to approximate the human subjective evaluation process, it would be more suitable to apply a fuzzy method in the environment-watershed plan topic. This paper describes the design of a fuzzy decision support system in multi-criteria analysis approach for selecting the best plan alternatives or strategies in environment watershed. The fuzzy analytic hierarchy process (FAHP) method is used to determine the preference weightings of criteria for decision makers by subjective perception (natural language). A questionnaire was used to find out from three related groups comprising 15 experts, including 5 from the university of expert scholars (include Water Resources Engineering and Conservation, Landscape and Recreation, Urban Planning, Environment Engineering, Architectural Engineering, etc.), 5 from the government departments, and 5 from industry. Subjectivity and vagueness analysis is dealt with the criteria and alternatives for selection process and simulation results by using fuzzy numbers with linguistic terms. It incorporated the decision-makers' attitude towards the preference; overall performance value of each alternative can be obtained based on the concept of fuzzy multiple-criteria decision-making (FMCDM). This research also gives an example of evaluation consisting of five alternatives, solicited from an environment-watershed plan work in Taiwan, is illustrated to demonstrate the effectiveness and usefulness of the proposed approach. The result is useful for destination planning and the sustainability of watershed tourism resources as well.",2011,1584108,"Vivien Y. C. Chen,Hui-Pang Lien,Chui-Hua Liu,James J. H. Liou,Gwo-Hshiung Tzeng,Lung-Shih Yang",0
1584109,Efficient Distributed Genetic Algorithm for Rule extraction.,"This paper presents an Efficient Distributed Genetic Algorithm for classification Rule extraction in data mining (EDGAR), which promotes a new method of data distribution in computer networks. This is done by spatial partitioning of the population into several semi-isolated nodes, each evolving in parallel and possibly exploring different regions of the search space. The presented algorithm shows some advantages when compared with other distributed algorithms proposed in the specific literature. In this way, some results are presented showing significant learning rate speedup without compromising the accuracy.",2011,1584109,"Miguel Rodríguez,Diego M. Escalante,Antonio Peregrín",0
1584110,Classical and fuzzy-genetic autopilot design for unmanned aerial vehicles.,"In this paper, an efficient strategy is proposed to design the altitude hold mode autopilot for a UAV which is non-minimum phase, and its model includes both parametric uncertainties and unmodeled nonlinear dynamics. This work has been motivated by the challenge of developing and implementing an autopilot that is robust with respect to these uncertainties. By combination of classic controller as the principal section of the autopilot and the fuzzy logic controller to increase the robustness in a single loop scheme, it is tried to exploit both methods advantages. The multi-objective genetic algorithm is used to mechanize the optimal determination of fuzzy logic controller parameters based on an efficient cost function that comprises undershoot, overshoot, rise time, settling time, steady state error and stability. Simulation results show that the proposed strategy performances are desirable in terms of the time response characteristics for both phugoid mode and short period mode, the robustness, and the adaptation of itself with respect to the large commands.",2011,1584110,"A. R. Babaei,M. Mortazavi,M. H. Moradi",0
1584111,A framework for the automatic synthesis of hybrid fuzzy/numerical controllers.,"In this paper, a framework for the automatic synthesis of hybrid fuzzy/numerical controllers is proposed. The methodology is based on model checking and on a very precise analysis of a system. This allows one to synthesize optimal numerical controllers and then use them to consistently improve fuzzy controllers. Moreover, we present a new approach that integrates the numerical and the fuzzy components and automatically outputs a hybrid controller. Such a hybrid controller exploits the optimality of numerical controllers and the robustness of fuzzy ones, and it is very compact and fast to read thanks to the use of OBDDs. We apply our methodology to two benchmark problems, the dc motor and the inverted pendulum. The results show that the hybrid controller can handle linear as well as nonlinear systems outperforming both the numerical and the fuzzy controllers.",2011,1584111,Daniele Magazzeni,0
1584112,Solving bus terminal location problems using evolutionary algorithms.,"Bus terminal assignment with the objective of maximizing public transportation service is known as bus terminal location problem (BTLP). We formulate the BTLP, a problem of concern in transportation industry, as a p-uncapacitated facility location problem (p-UFLP) with distance constraint. The p-UFLP being NP-hard (Krarup and Pruzan, 1990), we propose evolutionary algorithms for its solution. According to the No Free Lunch theorem and the good efficiency of the distinctive preserve recombination (DPX) operator, we design a new recombination operator for solving a BTLP by new evolutionary and memetic algorithms namely, genetic local search algorithms (GLS). We also define the potential objective function (POF) for the nodes and design a new mutation operator based on POF. To make the memetic algorithm faster, we estimate the variation of the objective function based on POF in the local search as part of an operator in memetic algorithms. Finally, we explore numerically the performance of nine proposed algorithms on over a thousand randomly generated problems and select the best two algorithms for further testing. The comparative studies show that our new hybrid algorithm composing the evolutionary algorithm with the GLS outperforms the multistart simulated annealing algorithm.",2011,1584112,"Reza Ghanbari,Nezam Mahdavi-Amiri",0
1584113,Neural network based on adaptive resonance theory with continuous training for multi-configuration transient stability analysis of electric power systems.,"This work presents a methodology to analyze electric power systems transient stability for first swing using a neural network based on adaptive resonance theory (ART) architecture, called Euclidean ARTMAP neural network. The ART architectures present plasticity and stability characteristics, which are very important for the training and to execute the analysis in a fast way. The Euclidean ARTMAP version provides more accurate and faster solutions, when compared to the fuzzy ARTMAP configuration. Three steps are necessary for the network working, training, analysis and continuous training. The training step requires much effort (processing) while the analysis is effectuated almost without computational effort. The proposed network allows approaching several topologies of the electric system at the same time; therefore it is an alternative for real time transient stability of electric power systems. To illustrate the proposed neural network an application is presented for a multi-machine electric power systems composed of 10 synchronous machines, 45 buses and 73 transmission lines.",2011,1584113,"Sandra C. Marchiori,Maria do Carmo G. da Silveira,Anna Diva P. Lotufo,Carlos R. Minussi,Mara Lúcia Martins Lopes",0
1584114,"On performance of case-based reasoning in Chinese business failure prediction from sensitivity, specificity, positive and negative values.","Case-based reasoning (CBR) is a machine learning technique of high performance in classification problems, and it is also a chief method in predicting business failure. Recently, several techniques have been introduced into the life-cycle of CBR for business failure prediction (BFP). The drawback of former researches on CBR-based BFP is that they only use total predictive accuracy when assessing CBR. In this research, we provide evidence on performance of CBR in Chinese BFP from various views of sensitivity, specificity, positive and negative values. Data are collected from Shanghai Stock Exchange and Shenzhen Stock Exchange in China. And we present how data are preprocessed from the view of data mining. The classical CBR model on the base of Euclidean metric, the grey CBR model on the base of grey coefficient metric, and the pseudo CBR model on the base of pseudo outranking relations are employed to make a comparative study on CBR's predictive performance in BFP. Meanwhile, support vector machine (SVM) is employed to be a baseline model for comparison. The results indicate that pseudo CBR produces better performance in Chinese BFP than classical CBR and grey CBR significantly on the whole, and it outperforms SVM marginally by total predictive accuracy and sensitivity, while it is not significantly worse than SVM by specificity.",2011,1584114,"Hui Li,Jie Sun",0
1584115,Large population size IGA with individuals' fitness not assigned by user.,"User fatigue problem in traditional interactive genetic algorithms restricts the population size. It is necessary to maintain large population size in order to apply these algorithms to optimize complicated problems. We present a large population size interactive genetic algorithm with an individual's fitness not assigned by the user in this paper. The algorithm divides a population into several clusters, and the maximum number of clusters is changeable with the evolution and the distribution of the population. A user only evaluates one representative individual in each cluster, and others' fitness are estimated based on these representative ones. In addition, to assign a representative individual's fitness, we record time when the user evaluates it satisfactory or unsatisfactory according to his/her sensibility, and its fitness is automatically calculated based on the time. Finally, we apply the proposed algorithm in a fashion evolutionary design system, and compare it with other two IGAs each of which has one aspect, including the population size and the evaluation method, the same as the proposed algorithm. The experimental results validate its efficiency.",2011,1584115,"Dunwei Gong,Jie Yuan",0
1584116,A generic optimising feature extraction method using multiobjective genetic programming.,"In this paper, we present a generic, optimising feature extraction method using multiobjective genetic programming. We re-examine the feature extraction problem and show that effective feature extraction can significantly enhance the performance of pattern recognition systems with simple classifiers. A framework is presented to evolve optimised feature extractors that transform an input pattern space into a decision space in which maximal class separability is obtained. We have applied this method to real world datasets from the UCI Machine Learning and StatLog databases to verify our approach and compare our proposed method with other reported results. We conclude that our algorithm is able to produce classifiers of superior (or equivalent) performance to the conventional classifiers examined, suggesting removal of the need to exhaustively evaluate a large family of conventional classifiers on any new problem.",2011,1584116,"Yang Zhang,Peter Rockett",0
1584117,Model-free control based on reinforcement learning for a wastewater treatment problem.,"This article presents a proposal, based on the model-free learning control (MFLC) approach, for the control of the advanced oxidation process in wastewater plants. This is prompted by the fact that many organic pollutants in industrial wastewaters are resistant to conventional biological treatments, and the fact that advanced oxidation processes, controlled with learning controllers measuring the oxidation-reduction potential (ORP), give a cost-effective solution. The proposed automation strategy denoted MFLC-MSA is based on the integration of reinforcement learning with multiple step actions. This enables the most adequate control strategy to be learned directly from the process response to selected control inputs. Thus, the proposed methodology is satisfactory for oxidation processes of wastewater treatment plants, where the development of an adequate model for control design is usually too costly. The algorithm proposed has been tested in a lab pilot plant, where phenolic wastewater is oxidized to carboxylic acids and carbon dioxide. The obtained experimental results show that the proposed MFLC-MSA strategy can achieve good performance to guarantee on-specification discharge at maximum degradation rate using readily available measurements such as pH and ORP, inferential measurements of oxidation kinetics and peroxide consumption, respectively.",2011,1584117,"S. Syafiie,F. Tadeo,E. Martinez,Teresa Alvarez",0
1584118,Enhancing the classification accuracy by scatter-search-based ensemble approach.,"Data-mining algorithms have been used in many classification problems. Among them, the decision tree (DT), back-propagation network (BPN), and support vector machine (SVM) are popular and can be applied to various areas. Nevertheless, different problems may require different parameter values when applying DT, BPN or SVM. If parameter values are not set well, results may turn out to be unsatisfactory. Further, a dataset may contain many features; however, not all features are beneficial for classifications. Therefore, a scatter search (SS) approach is proposed to obtain the better parameters and select the beneficial subset of features to attain better classification results. The above classification algorithms have their respective advantages and disadvantages, and suitability is influenced by the characteristics of the problem. If the algorithms can function together in a so-called ensemble, it is expected that better results can be obtained. Therefore, this study adapts ensemble to further enhance the classification accuracy rate. In order to evaluate the performance of the proposed approach, datasets in UCI (University of California, Irvine) were applied as the test problem set. The corresponding results were compared to several well-known, published approaches. The comparative study shows that the proposed approach improved the classification accuracy rate in most datasets. Thus, the proposed approach can be useful to both practitioners and researchers.",2011,1584118,"Shih-Chieh Chen,Shih-Wei Lin,Shuo-Yan Chou",0
1584120,A differential evolution based neural network approach to nonlinear system identification.,"This paper addresses the effectiveness of soft computing approaches such as evolutionary computation (EC) and neural network (NN) to system identification of nonlinear systems. In this work, two evolutionary computing approaches namely differential evolution (DE) and opposition based differential evolution (ODE) combined with Levenberg Marquardt algorithm have been considered for training the feed-forward neural network applied for nonlinear system identification. Results obtained envisage that the proposed combined opposition based differential evolution neural network (ODE-NN) approach to identification of nonlinear system exhibits better model identification accuracy compared to differential evolution neural network (DE-NN) approach. The above method is finally tested on a one degree of freedom (1DOF) highly nonlinear twin rotor multi-input-multi-output system (TRMS) to verify the identification performance.",2011,1584120,"Bidyadhar Subudhi,Debashisha Jena",0
1584121,Design of interval networks based on neural network and Choquet integral.,"Design and learning of networks best suited for a particular application is a never-ending process. But this process is restricted due to problems like stability, plasticity, computation and memory consumption. In this paper, we try to overcome these problems by proposing two interval networks (INs), based on a simple feed-forward neural network (NN) and Choquet integral (CI). They have simple structures that reduce the problems of computation and memory consumption. The use of Lyapunov stability (LS) in combination with fuzzy difference (FD) based learning algorithm evolve the converging and diverging process which in turn assures the stability. FD gives a range of variation of parameters having the lower and the upper bounds within which the system is stable thus defining the plasticity. Effectiveness and applicability of the NN and CI based network models are investigated on several benchmark problems dealing with both identification and control.",2011,1584121,"Madhusudan Singh,Smriti Srivastava,Madasu Hanmandlu,J. R. P. Gupta",0
1584122,Formulating and solving a class of optimization problems for high-performance gray world automatic white balance.,"This paper provides new insights into methods performing automatic white balance for a digitally captured image. It is shown that automatic white balance may be formulated as an optimization problem with explicit definition of objective function, decision variables, and constraints. Three alternative methods of formulating the optimization problem are proposed. It is also shown that fuzzy inference rules, commonly utilized in existing literatures to evaluate to what degree an image satisfying the gray world assumption, may be incorporated into the objective function of the optimization problem. A two-stage adjustment law with constrained search direction is then proposed to update the decision variables. A gradient descent algorithm is employed to numerically solve the problem, which guarantees the convergence and that optimal white balance effort is achieved for most images. Experimental results and a comparative study justify that the proposed methods are preferable to existing methods with regard to the execution time, the algorithmic complexity, and the performance.",2011,1584122,"Cheng-Lun Chen,Shao-Hua Lin",0
1584124,Cohesion: A concept and framework for confident association discovery with potential application in microarray mining.,"The minimal frequency constraint in classical association mining algorithms turns out to be a challenging bottleneck in discovery of large number of infrequent associations that can be potential in knowledge content. A lower choice for threshold frequency not only incurs huge cost of pattern explosion but also cuts reliability of discovered knowledge. The goal of the present paper is to devise a new framework addressing two necessities. The first is discovery of confident associations unconstrained to classical minimal frequency. The second is to ensure quality of the discovered rules. We propose a new property among items, terming it cohesion, and develop cohesion-based scalable algorithms for confident association discovery. In order to assess quality of rules in terms of knowledge content, we propose two new measures, accuracy and predictability based on documented associations. Experiments with market-basket data as well as microarray data establish superiority of cohesion-based technique both in terms of amount and quality of discovered knowledge.",2011,1584124,Ramkishore Bhattacharyya,0
1584125,Solving timetabling problems using a cultural algorithm.,"This paper addresses the solution of timetabling problems using cultural algorithms. The core idea is to extract problem domain information during the evolutionary search, and then combine it with some previously proposed operators, in order to improve performance. The proposed approach is validated using a benchmark of 20 instances, and its results are compared with respect to three other approaches: two evolutionary algorithms and simulated annealing, all of which have been previously adopted to solve timetabling problems.",2011,1584125,"Carlos Soza,Ricardo Landa Becerra,María-Cristina Riff,Carlos A. Coello Coello",0
1584126,A predictive and probabilistic load-balancing algorithm for cluster-based web servers.,"The exponential demands for high performance web servers led to use of cluster-based web servers. This increasing trend continues as dynamic contents are changing traditional web environments. Increasing utilization of cluster web servers through effective and fair load balancing is a crucial task specifically when it comes to advent of dynamic contents and database-driven applications on the internet. The proposed load-balancing algorithm classifies requests into different classes. The algorithm dynamically selects a request from a class and assigns the request to a server. For both the scheduling and dispatching, new probabilistic algorithms are proposed. To avoid using unreliable measured utilization in the face of fluctuating loads the proposed load-balancing algorithm benefits from a queuing model to predict the utilization of each server. We also used a control loop feedback to adjust the predicted values periodically based on soft computing techniques. The implementation results, using standard benchmarks confirms the effectiveness of proposed load-balancing algorithm. The algorithm significantly improves both the throughput and mean response time in contrast to two existing load-balancing algorithms.",2011,1584126,"Saeed Sharifian,Seyed A. Motamedi,Mohammad K. Akbari",0
1584127,Enhancement of Self-adaptive real-coded genetic algorithm using Taguchi method for Economic dispatch problem.,"In this paper, a new optimization algorithm, namely Taguchi self-adaptive real-coded genetic algorithm (TSARGA) is proposed and implemented to solve economic dispatch (ED) problem with valve-point loading. The TSARGA combines the self-adaptive real-coded genetic algorithm with Taguchi method which can exploit the potential offspring. The self-adaptation is achieved by means of simulated binary crossover (SBX). Moreover, powerful exploration capability is achieved through tournament selection by creating tournaments between two solutions. The better solution is chosen and placed in the mating pool leading to better convergence and reduced computational burden. The systematic reasoning ability of the Taguchi method is incorporated after SBX operations to select the potential genes to achieve polynomial mutation, and consequently, enhance the robustness of the solution. The proposed TSARGA is effectively applied to solve the ED problem with valve-point loading with 6, 13 and 40-generator systems. The proposed method yields solutions towards global optimum and it compares far better with other methods in terms of solution quality, handling constraints and computation time.",2011,1584127,"P. Subbaraj,R. Rengaraj,S. Salivahanan",0
1584128,Decoupled optimal design for power electronic circuits with adaptive migration in coevolutionary environment.,This paper presents a genetic based decoupled optimal design method for power electronics circuit design using an adaptive collaboration approach in a cooperative coevolutionary environment. The circuit parameters of the power conversion stage and the feedback network of a buck regulator are optimized through two parallel coadaptive genetic based optimization processes. The best candidate of the tunable parameters in one evolutionary process for the design of the power conversion stage is merged to the other evolutionary process for the design of the feedback network as untunable factors through a collaboration controller in which the collaboration strategy is adaptively controlled by a first-order projection of the maximum and minimum bounds of the fitness value of the genes representing the circuit design parameters in each generation. The proposed design methodology is suitable for parallel computation resulting in considerable improvement in searching efficiency. Simulated results of the design of a buck regulator with the proposed approach were verified with experimental results from the actual hardware implementation. It showed that the design with the proposed scheme was compatible with the design specification.,2011,1584128,"Angus K. M. Wu,Jin Zhang,Henry Shu-Hung Chung",0
1584129,Memetic Pareto Evolutionary Artificial Neural Networks to determine growth/no-growth in predictive microbiology.,"The main objective of this work is to automatically design neural network models with sigmoid basis units for binary classification tasks. The classifiers that are obtained achieve a double objective: a high classification level in the dataset and a high classification level for each class. We present MPENSGA2, a Memetic Pareto Evolutionary approach based on the NSGA2 multiobjective evolutionary algorithm which has been adapted to design Artificial Neural Network models, where the NSGA2 algorithm is augmented with a local search that uses the improved Resilient Backpropagation with backtracking-IRprop+ algorithm. To analyze the robustness of this methodology, it was applied to four complex classification problems in predictive microbiology to describe the growth/no-growth interface of food-borne microorganisms such as Listeria monocytogenes, Escherichia coli R31, Staphylococcus aureus and Shigella flexneri. The results obtained in Correct Classification Rate (CCR), Sensitivity (S) as the minimum of sensitivities for each class, Area Under the receiver operating characteristic Curve (AUC), and Root Mean Squared Error (RMSE), show that the generalization ability and the classification rate in each class can be more efficiently improved within a multiobjective framework than within a single-objective framework.",2011,1584129,"J. C. Fernández,César Hervás,Francisco J. Martínez-Estudillo,Pedro Antonio Gutiérrez",0
1584130,Design of a pipeline leakage detection using expert system: A novel approach.,"Pipeline leakage is a demand from governmental and environmental associations that companies need to comply with. Due the high accuracy on detecting leakage, it is necessary to set procedures that will achieve the leading performance. This paper describes a methodology to set instrumentations systems to accomplish with the legal requirement keeping high reliability during normal and fail operations conditions. To achieving the described state this paper proposes a set of models acting as Expert systems: each one observing and diagnosing pipeline leakage in real-time. The proposed system also validates the operations according the business rules applied to it. A set of techniques is applied in order to be possible the system executes its function: fuzzy logic, neural network, genetic algorithm and statistic analysis. The application of the methodology proposed is in operation supervising pipeline in a Brazilian petroleum installation.",2011,1584130,"C. A. Laurentys,C. H. M. Bomfim,B. R. Menezes,Walmir M. Caminhas",0
1584131,Wavelet domain association rules for efficient texture classification.,"The wavelet domain association rules method is proposed for efficient texture characterization. The concept of association rules to capture the frequently occurring local intensity variation in textures. The frequency of occurrence of these local patterns within a region is used as texture features. Since texture is basically a multi-scale phenomenon, multi-resolution approaches such as wavelets, are expected to perform efficiently for texture analysis. Thus, this study proposes a new algorithm which uses the wavelet domain association rules for texture classification. Essentially, this work is an extension version of an early work of the Rushing et al. [10,11], where the generation of intensity domain association rules generation was proposed for efficient texture characterization. The wavelet domain and the intensity domain (gray scale) association rules were generated for performance comparison purposes. As a result, Rushing et al. [10,11] demonstrated that intensity domain association rules performs much more accurate results than those of the methods which were compared in the Rushing et al. work. Moreover, the performed experimental studies showed the effectiveness of the wavelet domain association rules than the intensity domain association rules for texture classification problem. The overall success rate is about 97%.",2011,1584131,"Murat Karabatak,M. Cevdet Ince,Abdulkadir Sengür",0
1584132,Modeling of road-traffic noise with the use of genetic algorithm.,"Two models for predicting in-city road-traffic noise pollution of Mashhad have been obtained. Traffic volume, composition, and speed have been chosen as model's parameters. Vehicles were classified into light cars and medium and heavy trucks. Reference emission level of each group was determined experimentally based on perpendicular propagation from central lane of traffic road. Simultaneous measurements of noise level and vehicle flow and composition were done. Two mathematical models have been proposed by the use of genetic algorithms which can be used for calculating L""e""q. These models have been validated against noise data. Subsequently, measured traffic noise has been compared with calculated ones, using developed models, and a relatively good agreement has been obtained among them. The models are found accurate within +/-1% and can be used for flat road noise prediction.",2011,1584132,"S. Rahmani,S. M. Mousavi,M. J. Kamali",0
1584133,EMPSO-based optimization for inter-temporal multi-product revenue management under salvage consideration.,"The retail market is governed by customer behavior, demand pattern and inventory replenishment policies. It is also observed that any decision would prove to be full of errors, and objective of enhancing the market share could not be achieved, without inclusion of these factors and policies. While an extensive set of literature exists on single and multi-product dynamic pricing, the issue of liquidation of leftover inventory has so far received scant attention from the researchers of Operations Management community. The current work primarily tries to bridge this research gap by addressing dual objectives of revenue maximization and reduction of salvaging losses. In this paper an inter-temporal dynamic pricing model for multiple products is developed under a market setup with price-sensitive demand. Ideas proposed by [1] and [2] have been taken into account for constructing a revenue structure. The formulated objective function is found to be tractable for deriving prices and procurement quantities of large product portfolios. A multi-objective problem has been devised to handle the optimization of normal and clearance revenue by satisfying several pragmatic constraints. Subsequently, an effective algorithm deriving its traits from Particle Swarm Optimization has been proposed to address this problem. An illustrative example from retail apparel industry has been simulated and solved by the afore-mentioned approach. To validate the model statistical analysis has been carried out and the managerial insights portrayed to reveal the practical complexities involved.",2011,1584133,"Ankit Kumar Gandhi,Sri Krishna Kumar,Mayank Kumar Pandey,M. K. Tiwari",0
1584134,Expert system to predict forging load and axial stress.,"Finite element (FE) analysis of forging process generally takes a long time to carry out. Sometimes, it might be required to predict the results of FE analysis as accurately as possible in a less processing time. A pre-analysis prediction of the results could also be helpful in some cases. Soft computing-based expert system has been developed in the present work, to predict forging load and axial stress developed. Forging load and axial stress have been calculated for an axi-symmetric part by varying the values of maximum contact friction stress and normal contact stiffness factor in ANSYS FE package. Data generated in the process have been utilized for developing a fuzzy logic-based expert system, on the basis of the authors' knowledge, which has been optimized subsequently using a genetic algorithm (GA). Two other expert systems (ESs) have been developed automatically using the GA, without taking any aid from the manually-designed fuzzy logic system. It has been found that the expert systems are able to make predictions of forging load and axial stress as accurately as the FE package can do. Results of the ESs have also been seen to be comparable with the experimental results reported in the available literature. Instantaneous prediction capability of the developed expert systems proves their suitability for on-line implementations.",2011,1584134,"Tapas Gangopadhyay,Dilip Kumar Pratihar,Indrajit Basak",0
1584135,Risk evaluation through decision-support architectures in threat assessment and countering terrorism.,"Owing to their inherent nature, terrorist activities could be highly diversified. The risk assessment becomes a crucial component as it helps us weigh pros and cons versus possible actions or some planning pursuits. The recognition of threats and their relevance/seriousness is an integral part of the overall process of classification, recognition, and assessing eventual actions undertaken in presence of acts of chem.-bio terrorism. In this study, we introduce an overall scheme of risk assessment realized on a basis of classification results produced for some experimental data capturing the history of previous threat cases. The structural relationships in these experimental data are first revealed with the help of information granulation - fuzzy clustering. We introduce two criteria using which information granules are evaluated, that is (a) representation capabilities which are concerned with the quality of representation of numeric data by abstract constructs such as information granules, and (b) interpretation aspects which are essential in the process of risk evaluation. In case of representation facet of information granules, we demonstrate how a reconstruction criterion quantifies their quality. Three ways in which interpretability is enhanced are studied. First, we show how to construct the information granules with extended cores (where the uncertainty associated with risk evaluation could be reduced) and shadowed sets, which provide a three-valued logic perspective of information granules given in the form of fuzzy sets. Subsequently, we show a way of interpreting fuzzy sets via an optimized set of its @a-cuts.",2011,1584135,"Witold Pedrycz,S. C. Chen,Stuart H. Rubin,G. Lee",0
1587615,A note on the perimeter of fat objects.,"In this Note, we show that the size of the perimeter of (@a,@b)-covered objects is a linear function of the diameter. Specifically, for an (@a,@b)-covered object O, per(O)=",2011,1587615,"Prosenjit Bose,Otfried Cheong,Vida Dujmovic",0
1587616,Efficient on-line algorithms for Euler diagram region computation.,"Euler diagrams are an accessible and effective visualisation of data involving simple set-theoretic relationships. Sets are represented by closed curves in the plane and often have wellformedness conditions placed on them in order to enhance comprehensibility. The theoretical underpinning for tool support has usually focussed on the problem of generating an Euler diagram from an abstract model. However, the problem of efficient computation of the abstract model from the concrete diagram has not been addressed before, despite this computation being a necessity for computer interpretations of user drawn diagrams. This may be used, together with automated manipulations of the abstract model, for purposes such as semantic information presentation or diagrammatic theorem proving. Furthermore, in interactive settings, the user may update diagrams ''on-line'' by adding and removing curves, for example, in which case a system requirement is the update of the abstract model (without the necessity of recomputation of the entire abstract model). We define the notion of marked Euler diagrams, together with a method for associating marked points on the diagram with regions in the plane. Utilising these, we provide on-line algorithms which quickly compute the abstract model of a weakly reducible wellformed Euler diagram (constructible as a sequence of additions or removals of curves, keeping a wellformed diagram at each step), and quickly updates both the set of curves in the plane as well as the abstract model according to the on-line operations. Efficiency is demonstrated by comparison with a common, naive algorithm. Furthermore, the methodology enables a straightforward implementation which has subsequently been realised as an application for the user classification domain.",2011,1587616,"Gennaro Cordasco,Rosario De Chiara,Andrew Fish",0
1588350,A decision support model for warranty servicing of repairable items.,"This paper addresses the question of servicing warranties for repairable items. Each time a unit under warranty fails, the manufacturer is obliged to restore it to operating condition either by repairing it or by replacing it with a new unit. The decision to replace or repair depends on a variety of factors such as cost of immediate repair, age of the unit, the extent of usage, the condition of the unit, and the remaining warranty period. We consider products with phase type lifetime distributions, where the phase describes the condition of the unit, and develop a decision support system (DSS) for the repair/replace decisions in servicing the warranty using the criterion of expected cost of servicing the remaining warranty. A comprehensive example is presented to illustrate the DSS. Effectiveness of the proposed DSS is verified by simulation.",2011,1588350,B. Madhu Rao,0
1588370,Computational experience with a core-based reduction procedure for the 2-knapsack problem.,"In this note we consider the 2-knapsack problem where each item has both a weight and a size. We show that a simple yet powerful reduction procedure can significantly enhance the capabilities of a commercial ILP solver, leading to solve very large instances.",2011,1588370,"Federico Della Croce,Andrea Grosso",0
1588371,Exact algorithms for a generalization of the order acceptance and scheduling problem in a single-machine environment.,"This paper studies a generalization of the order acceptance and scheduling problem in a single-machine environment where a pool consisting of firm planned orders as well as potential orders is available from which an over-demanded company can select. The capacity available for processing the accepted orders is limited and each order is characterized by a known processing time, delivery date, revenue and a weight representing a penalty per unit-time delay beyond the delivery date. We prove that the existence of a constant-factor approximation algorithm for this problem is unlikely. We propose two linear formulations that are solved using an IP solver and we devise two exact branch-and-bound procedures able to solve instances with up to 50 jobs within reasonable CPU times. We compare the efficiency and quality of the results obtained using the different solution approaches.",2011,1588371,"Fabrice Talla Nobibon,Roel Leus",0
1588372,On the application of graph colouring techniques in round-robin sports scheduling.,"The purpose of this paper is twofold. First, it explores the issue of producing valid, compact round-robin sports schedules by considering the problem as one of graph colouring. Using this model, which can also be extended to incorporate additional constraints, the difficulty of such problems is then gauged by considering the performance of a number of different graph colouring algorithms. Second, neighbourhood operators are then proposed that can be derived from the underlying graph colouring model and, in an example application, we show how these operators can be used in conjunction with multi-objective optimisation techniques to produce high-quality solutions to a real-world sports league scheduling problem encountered at the Welsh Rugby Union in Cardiff, Wales.",2011,1588372,"R. Lewis,J. Thompson",0
1588373,An improved multi-objective evolutionary algorithm for the vehicle routing problem with time windows.,"The vehicle routing problem with time windows is a complex combinatorial problem with many real-world applications in transportation and distribution logistics. Its main objective is to find the lowest distance set of routes to deliver goods, using a fleet of identical vehicles with restricted capacity, to customers with service time windows. However, there are other objectives, and having a range of solutions representing the trade-offs between objectives is crucial for many applications. Although previous research has used evolutionary methods for solving this problem, it has rarely concentrated on the optimization of more than one objective, and hardly ever explicitly considered the diversity of solutions. This paper proposes and analyzes a novel multi-objective evolutionary algorithm, which incorporates methods for measuring the similarity of solutions, to solve the multi-objective problem. The algorithm is applied to a standard benchmark problem set, showing that when the similarity measure is used appropriately, the diversity and quality of solutions is higher than when it is not used, and the algorithm achieves highly competitive results compared with previously published studies and those from a popular evolutionary multi-objective optimizer.",2011,1588373,"Abel Garcia-Najera,John A. Bullinaria",0
1588374,MIP models for connected facility location: A theoretical and computational study.,"This article comprises the first theoretical and computational study on mixed integer programming (MIP) models for the connected facility location problem (ConFL). ConFL combines facility location and Steiner trees: given a set of customers, a set of potential facility locations and some inter-connection nodes, ConFL searches for the minimum-cost way of assigning each customer to exactly one open facility, and connecting the open facilities via a Steiner tree. The costs needed for building the Steiner tree, facility opening costs and the assignment costs need to be minimized. We model ConFL using seven compact and three mixed integer programming formulations of exponential size. We also show how to transform ConFL into the Steiner arborescence problem. A full hierarchy between the models is provided. For two exponential size models we develop a branch-and-cut algorithm. An extensive computational study is based on two benchmark sets of randomly generated instances with up to 1300 nodes and 115,000 edges. We empirically compare the presented models with respect to the quality of obtained bounds and the corresponding running time. We report optimal values for all but 16 instances for which the obtained gaps are below 0.6%.",2011,1588374,"Stefan Gollowitzer,Ivana Ljubic",0
1588375,A constraint programming-based solution approach for medical resident scheduling problems.,"Persistent calls come from within the graduate medical education community and from external sources for regulating the resident duty hours in order to meet the obligations about the quality of resident education, the well-being of residents themselves, and the quality of patient care services. The report of the Accreditation Council for Graduate Medical Education (ACGME) proposes common program requirements for resident hours. In this paper, we first develop a mixed-integer programming model for scheduling residents' duty hours considering the on-call night, day-off, rest period, and total work-hour ACGME regulations as well as the demand coverage requirements of the residency program. Subsequently, we propose a column generation model that consists of a master problem and an auxiliary problem. The master problem finds a configuration of individual schedules that minimizes the sum of deviations from the desired service levels for the day and night periods. The formulation of this problem is possible by representing the feasible schedules using column variables, whereas the auxiliary problem finds the whole set of feasible schedules using constraint programming. The proposed approach has been tested on a series of problems using real data obtained from a hospital. The results indicate that high-quality schedules can be obtained within a few seconds.",2011,1588375,"Seyda Topaloglu,Irem Ozkarahan",0
1588377,Time-constrained project scheduling with adjacent resources.,"We develop a decomposition method for the Time-Constrained Project Scheduling Problem (TCPSP) with adjacent resources. For adjacent resources the resource units are ordered and the units assigned to a job have to be adjacent. On top of that, adjacent resources are not required by single jobs, but by job groups. As soon as a job of such a group starts, the adjacent resource units are occupied, and they are not released before all jobs of that group are completed. The developed decomposition method separates the adjacent resource assignment from the rest of the scheduling problem. Test results demonstrate the applicability of the decomposition method. The presented decomposition forms a first promising approach for the TCPSP with adjacent resources and may form a good basis to develop more elaborated methods.",2011,1588377,"Johann Hurink,A. L. Kok,Jacob Jan Paulus,J. M. J. Schutten",0
1588378,A branch-and-cut algorithm for the partitioning-hub location-routing problem.,"We introduce the Partitioning-Hub-Location-Routing Problem (PHLRP), a hub location problem involving graph partitioning and routing features. The PHLRP consists of partitioning a given network into sub-networks, locating at least one hub in each sub-network and routing the traffic within the network at minimum cost. This problem finds applications in deployment of an Internet Routing Protocol called Intermediate System-Intermediate System (ISIS), and strategic planning of LTL ground freight distribution systems. We present an Integer Programming (IP) model for solving exactly the PHLRP and explore possible valid inequalities to strengthen it. Computational experiments prove the effectiveness of our model which is able to tackle instances of PHLRP containing up to 20 vertices.",2011,1588378,"Daniele Catanzaro,Éric Gourdin,Martine Labbé,F. Aykut Özsoy",0
1588379,Hybridizing principles of TOPSIS with case-based reasoning for business failure prediction.,"Case-based reasoning (CBR) solves many real-world problems under the assumption that similar observations have similar outputs. As an implementation of this assumption and inspired by the technique for order performance by the similarity to ideal solution (TOPSIS), this paper proposes a new type of multiple criteria CBR method for binary business failure prediction (BFP) with similarities to positive and negative ideal cases (SPNIC). Assuming that the binary prediction of business failure generates two results, i.e., failure and non-failure, we set the principle of this CBR forecasting method which is termed as SPNIC-based CBR as follows: new observations should have the same output as the positive or negative ideal case to which they are more similar. From the perspective of CBR, the SPNIC-based CBR forecasting method consists of R^4 processes: retrieving positive and negative ideal cases, reusing solutions of ideal cases to forecast, retain cases, and reconstruct the case base. As a demonstration, we applied this method to forecast business failure in China with three data representations of a formerly collected dataset from normal economic environment and a representation of a recently collected dataset from financial crisis environment. The results indicate that this new CBR forecasting method can produce significantly better short-term discriminate capability than comparative methods, except for support vector machine, in normal economic environment; On the contrary, it cannot produce acceptable performance in financial crisis environment. Further topics about this method are discussed.",2011,1588379,"Hui Li,Hojjat Adeli,Jie Sun,Jian-Guang Han",0
1588380,Prize collecting Steiner trees with node degree dependent costs.,"We discuss a variant of the prize-collecting Steiner tree problem with node degree dependent costs using a telecommunications setting to motivate these costs. We present and test models which are tailored for this variant of the problem. Results taken from instances with up to 100 nodes are used to evaluate the quality of the proposed models for solving the problem, as well as, in terms of the correspondent linear programming relaxation.",2011,1588380,"Luís Gouveia,Pedro Moura,Amaro de Sousa",0
1588381,Team formation based on group technology: A hybrid grouping genetic algorithm approach.,"This paper presents a new model for team formation based on group technology (TFPGT). Specifically, the model is applied as a generalization of the well-known Machine-Part Cell Formation problem, which has become a classical problem in manufacturing in the last few years. In this case, the model presented is especially well-suited for problems of team formation arising in R&D-oriented or teaching institutions. A parallel hybrid grouping genetic algorithm (HGGA) is also proposed in the paper to solve the TFPGT. The performance of the algorithm is shown in several synthetic TFPGT instances, and in a real problem: the formation of teaching groups at the Department of Signal Theory and Communications of the Universidad de Alcala in Spain.",2011,1588381,"Luis E. Agustín-Blas,Sancho Salcedo-Sanz,Emilio G. Ortíz-García,Antonio Portilla-Figueras,Ángel M. Pérez-Bellido,Silvia Jiménez-Fernández",0
1588382,A new mathematical programming approach to multi-group classification problems.,"In this paper we introduce a goal programming formulation for the multi-group classification problem. Although a great number of mathematical programming models for two-group classification problems have been proposed in the literature, there are few mathematical programming models for multi-group classification problems. Newly proposed multi-group mathematical programming model is compared with other conventional multi-group methods by using different real data sets taken from the literature and simulation data. A comparative analysis on the real data sets and simulation data shows that our goal programming formulation may suggest efficient alternative to traditional statistical methods and mathematical programming formulations for the multi-group classification problem.",2011,1588382,"Hasan Bal,H. Hasan Örkcü",0
1588383,Models for a traveling purchaser problem with additional side-constraints.,"The traveling purchaser problem (TPP) is the problem of determining a tour of a purchaser that needs to buy several items in different shops such that the total amount of travel and purchase costs is minimized. Motivated by an application in machine scheduling, we study a variant of the problem with additional constraints, namely, a limit on the maximum number of markets to be visited, a limit on the number of items bought per market and where only one copy per item needs to be bought. We present an integer linear programming (ILP) model which is adequate for obtaining optimal integer solutions for instances with up to 100 markets. We also present and test several variations of a Lagrangian relaxation combined with a subgradient optimization procedure. The relaxed problem can be solved by dynamic programming and can also be viewed as resulting from applying a state space relaxation technique to a dynamic programming formulation. The Lagrangian based method is combined with a heuristic that attempts to transform relaxed solutions into feasible solutions. Computational results for instances with up to 300 markets show that with the exception of a few cases, the reported differences between best upper bound and lower bound values on the optimal solutions are reasonably small.",2011,1588383,"Luis Gouveia,Ana Paias,Stefan Voß",0
1588384,Size-reduction heuristics for the unrelated parallel machines scheduling problem.,"In this paper we study the unrelated parallel machines problem where n independent jobs must be assigned to one out of m parallel machines and the processing time of each job differs from machine to machine. We deal with the objective of the minimisation of the maximum completion time of the jobs, usually referred to as makespan or C""m""a""x. This is a type of assignment problem that has been frequently studied in the scientific literature due to its many potential applications. We propose a set of metaheuristics based on a size-reduction of the original assignment problem that produce solutions of very good quality in a short amount of time. The underlying idea is to consider only a few of the best possible machine assignments for the jobs and not all of them. The results are simple, yet powerful methods. We test the proposed algorithms with a large benchmark of instances and compare them with current state-of-the-art methods. In most cases, the proposed size-reduction algorithms produce results that are statistically proven to be better by a significant margin.",2011,1588384,"Luis Fanjul-Peyro,Rubén Ruiz",0
1588385,New formulations for the hop-constrained minimum spanning tree problem via Sherali and Driscoll's tightened Miller-Tucker-Zemlin constraints.,"Given an undirected network with positive edge costs and a natural number p, the hop-constrained minimum spanning tree problem (HMST) is the problem of finding a spanning tree with minimum total cost such that each path starting from a specified root node has no more than p hops (edges). In this paper, the new models based on the Miller-Tucker-Zemlin (MTZ) subtour elimination constraints are developed and computational results together with comparisons against MTZ-based, flow-based, and hop-indexed formulations are reported. The first model is obtained by adapting the MTZ-based Asymmetric Traveling Salesman Problem formulation of Sherali and Driscoll [18] and the other two models are obtained by combining topology-enforcing and MTZ-related constraints offered by Akgun and Tansel (submitted for publication) [20] for HMST with the first model appropriately. Computational studies show that the best LP bounds of the MTZ-based models in the literature are improved by the proposed models. The best solution times of the MTZ-based models are not improved for optimally solved instances. However, the results for the harder, large-size instances imply that the proposed models are likely to produce better solution times. The proposed models do not dominate the flow-based and hop-indexed formulations with respect to LP bounds. However, good feasible solutions can be obtained in a reasonable amount of time for problems for which even the LP relaxations of the flow-based and hop-indexed formulations can be solved in about 2 days.",2011,1588385,Ibrahim Akgün,0
1588386,A new lower bound for the resource-constrained project scheduling problem with generalized precedence relations.,In this paper we propose a new lower bound for the resource-constrained project scheduling problem with generalized precedence relationships. The lower bound is based on a relaxation of the resource constraints among independent activities and on a solution of the relaxed problem suitably represented by means of an AON acyclic network. Computational results are presented and confirmed a better practical performance of the proposed method with respect to those present in the literature.,2011,1588386,"Lucio Bianco,Massimiliano Caramia",0
1588387,A multi-dimensional shooting algorithm for the two-facility location-allocation problem with dense demand.,"We develop an efficient allocation-based solution framework for a class of two-facility location-allocation problems with dense demand data. By formulating the problem as a multi-dimensional boundary value problem, we show that previous results for the discrete demand case can be extended to problems with highly dense demand data. Further, this approach can be generalized to non-convex allocation decisions. This formulation is illustrated for the Euclidean metric case by representing the affine bisector with two points. A specialized multi-dimensional shooting algorithm is presented and illustrated on an example. Comparisons with two alternative methods through a computational study confirm the efficiency of the proposed methodology.",2011,1588387,"Alper Murat,Vedat Verter,Gilbert Laporte",0
1588388,An efficient heuristic for adaptive production scheduling and control in one-of-a-kind production.,"Even though research in flow shop production scheduling has been carried out for many decades, there is still a gap between research and application-especially in manufacturing paradigms such as one-of-a-kind production (OKP) that intensely challenges real time adaptive production scheduling and control. Indeed, many of the most popular heuristics continue to use Johnson's algorithm (1954) as their core. This paper presents a state space (SS) heuristic, integrated with a closed-loop feedback control structure, to achieve adaptive production scheduling and control in OKP. Our SS heuristic, because of its simplicity and computational efficiency, has the potential to become a core heuristic. Through a series of case studies, including an industrial implementation in OKP, our SS-based production scheduling and control system demonstrates significant potential to improve production efficiency.",2011,1588388,"Wei Li,Barrie R. Nault,Deyi Xue,Yiliu Tu",0
1588389,A differential evolution algorithm with self-adapting strategy and control parameters.,"This paper presents a Differential Evolution algorithm with self-adaptive trial vector generation strategy and control parameters (SspDE) for global numerical optimization over continuous space. In the SspDE algorithm, each target individual has an associated strategy list (SL), a mutation scaling factor F list (FL), and a crossover rate CR list (CRL). During the evolution, a trial individual is generated by using a strategy, F, and CR taken from the lists associated with the target vector. If the obtained trial individual is better than the target vector, the used strategy, F, and CR will enter a winning strategy list (wSL), a winning F list (wFL), and a winning CR list (wCRL), respectively. After a given number of iterations, the FL, CRL or SL will be refilled at a high probability by selecting elements from wFL, wCRL and wSL or randomly generated values. In this way, both the trial vector generation strategy and its associated parameters can be gradually self-adapted to match different phases of evolution by learning from their previous successful experience. Extensive computational simulations and comparisons are carried out by employing a set of 19 benchmark problems from the literature. The computational results show that overall the SspDE algorithm performs better than the state-of-the-art differential evolution variants.",2011,1588389,"Quan-Ke Pan,Ponnuthurai N. Suganthan,Ling Wang,Liang Gao,Rammohan Mallipeddi",0
1588390,A tabu search algorithm for the heterogeneous fixed fleet vehicle routing problem.,"In the heterogeneous fixed fleet vehicle routing problem there are different types of vehicles and a given number of vehicles of each type. The resolution of this problem consists of assigning the customers to the existing vehicles and, in relation to each vehicle, defining the order of visiting each customer for the delivery or collection of goods. The objective is to minimize the total costs, satisfying customers' requirements and visiting each customer exactly once. In this paper a tabu search algorithm is proposed and tested on several benchmark problems. The computational experiments show that the proposed algorithm produces high quality solutions within an acceptable computation time. Four new best solutions are reported for a set of test problems used in the literature.",2011,1588390,José Brandão,0
1588391,A modified super-efficiency measure based on simultaneous input-output projection in data envelopment analysis.,"Super-efficiency data envelopment analysis (SE-DEA) models have been developed and applied in many situations. However, under the condition of variable returns to scale (VRS), infeasibility of the SE-DEA model may occur and restrict its application. A modified SE-DEA measure based on simultaneous input-output projection is proposed as a way to systematically characterize the super-efficiency in both inputs and outputs. The modified measure overcomes the infeasibility problem while providing ease of computation and interpretation. The practicability of the proposed measure in real applications and its comparison to other super-efficiency measures are illustrated empirically using an example.",2011,1588391,"Jin-Xiao Chen,Mingrong Deng,Sylvain Gingras",0
1588392,A two-machine flowshop problem with two agents.,"The multiple-agent scheduling problems have received increasing attention recently. However, most of the research focuses on deriving feasible/optimal solutions or examining the computational complexity of the intractable cases in a single machine. Often a number of operations have to be done on every job in many manufacturing and assembly facilities (Pinedo, 2002 [1]). In this paper, we consider a two-machine flowshop problem where the objective is to minimize the total completion time of the first agent with no tardy jobs for the second agent. We develop a branch-and-bound algorithm and simulated annealing heuristic algorithms to search for the optimal solution and near-optimal solutions for the problem, respectively.",2011,1588392,"Wen-Chiung Lee,Shiuan-Kang Chen,Cheng-Wei Chen,Chin-Chia Wu",0
1588393,"Hybrid-LP: Finding advanced starting points for simplex, and pivoting LP methods.","The simplex method has proven its efficiency in practice for linear programming (LP) problems of various types and sizes. However, its theoretical worst-case complexity in addition to its poor performance for very large-scale LP problems has driven researchers to develop alternative methods for LP problems. In this paper, we develop the hybrid-LP; a two-phase approach for solving LP problems. Rather than following a path of extreme points on the boundary of the feasible region as in the simplex method, the first phase of the hybrid-LP moves through the interior of the feasible region to obtain an improved and advanced initial basic feasible solution (BFS). Then, in the second phase simplex or other LP methods can be used to find the optimal solution. Since the introduction of polynomial-time methods for LP, a considerable amount of research has focused on interior-point methods for solving large-scale LP problems. Although fewer iterations are needed for interior-point methods to converge to a solution, the iterations are computationally intensive. Our approach is a hybrid method that uses a computationally efficient pivot to move in the interior of the feasible region in its first phase. This single iteration is able to bypassing several extreme points to an improved BFS, which can then be used as a starting point in any LP method in the second phase of the method. Our approach can also be modified to perform a number of interior pivots in the first phase based on the trade-off between the number of iterations and the running time. The hybrid-LP uses an efficient pivoting iteration which is computationally comparable to the standard simplex iteration. Another feature is adaptability in finding the advanced starting point by avoiding the boundaries of the feasible region. In addition, the hybrid-LP has the ability to start from a feasible point which may not be a BFS. Our computational experiments demonstrate that the hybrid-LP reduces both the number of iterations and the running time compared to the simplex method on a wide range of test problems.",2011,1588393,"Camelia Al-Najjar,Behnam Malakooti",0
1588394,A branch and bound enhanced genetic algorithm for scheduling a flowline manufacturing cell with sequence dependent family setup times.,"This paper addresses the permutation flowline manufacturing cell with sequence dependent family setup times problem with the objective to minimize the makespan criterion. We develop a cooperative approach including a genetic algorithm and a branch and bound procedure. The latter is probabilistically integrated in the genetic algorithm in order to enhance the current solution. Moreover, the application of the branch and bound algorithm is based upon the decomposition of the problem into subproblems. The performance of the proposed method is tested by numerical experiments on a large number of representative problems.",2011,1588394,"Radhouan Bouabda,Bassem Jarboui,Mansour Eddaly,Abdelwaheb Rebaï",0
1588396,A double genetic algorithm for the MRCPSP/max.,"This paper presents a heuristic solution procedure for a very general resource-constrained project scheduling problem. Here, multiple execution modes are available for the individual activities of the project. In addition, minimum as well as maximum time lags between different activities may be given. The objective is to determine a mode and a start time for each activity such that the temporal and resource constraints are met and the project duration is minimised. Project scheduling problems of this type occur e.g. in process industries. The heuristic is a two-phased genetic algorithm with different representation, fitness, crossover operator, etc., in each of them. One of the contributions of the paper is the optimisation in the first phase of a problem dual to the original, the searching for the best modes of the activities. Computational results show that the algorithm outperforms the state-of-the-art algorithms in medium and large instances.",2011,1588396,"Agustín Barrios,Francisco Ballestín,Vicente Valls",0
1588397,A Neurogenetic approach for the resource-constrained project scheduling problem.,"A variety of metaheuristic approaches have emerged in recent years for solving the resource-constrained project scheduling problem (RCPSP), a well-known NP-hard problem in scheduling. In this paper, we propose a Neurogenetic approach which is a hybrid of genetic algorithms (GA) and neural-network (NN) approaches. In this hybrid approach the search process relies on GA iterations for global search and on NN iterations for local search. The GA and NN search iterations are interleaved in a manner that allows NN to pick the best solution thus far from the GA pool and perform an intensification search in the solution's local neighborhood. Similarly, good solutions obtained by NN search are included in the GA population for further search using the GA iterations. Although both GA and NN approaches, independently give good solutions, we found that the hybrid approach gives better solutions than either approach independently for the same number of shared iterations. We demonstrate the effectiveness of this approach empirically on the standard benchmark problems of size J30, J60, J90 and J120 from PSPLIB.",2011,1588397,"Anurag Agarwal,Selcuk Colak,S. Selçuk Erengüç",0
1588398,Theoretical and practical fundamentals for multi-objective optimisation in resource-constrained project scheduling problems.,"Project scheduling is an inherently multi-objective problem, since managers want to finish projects as soon as possible with the minimum cost and the maximum quality. However, there are only a few papers dealing with multiobjective resource-constrained project scheduling problems (MORCPSPs). Moreover, there is no theoretical study in the literature that establishes the fundamentals for correct algorithmic developments. In this paper we try to close the gap by proving several results for MORCPSPs. With these results as a basis, both exact and heuristic procedures capable of obtaining a set of efficient solutions for several important MORCPSPs can be created. We develop algorithms for the case where all objective functions are of the same type, called regular. In this case, specific codifications, techniques, and procedures can be used. Extensive computational results help decide which algorithms or techniques are the most promising for the problem. With the aid of these algorithms we study the Pareto fronts in this case. Finally, we apply a metaheuristic algorithm to a particular example of the general case in order to analyse the differences in the Pareto fronts. The project instances and Pareto fronts obtained can be downloaded from a website to facilitate comparisons with future research efforts.",2011,1588398,"Francisco Ballestín,Rosa Blanco",0
1588399,Approximations of lead-time distributions in an assemble-to-order system under a base-stock policy.,"In this paper, we consider an assemble-to-order production system in which a product with multiple components is produced under a base-stock policy. The demand follows a Poisson arrival process, and the production time at each facility is exponentially distributed. Our main results are closed-form formulae for approximating the lead-time distributions in equilibrium. The approximation formulae for the lead-time distributions can be obtained with a linear combination of bounds, in which certain constants are obtained from auxiliary properties and simulation experiments. To determine the quality of the approximations, we obtained sample lead-time distributions from many simulations. Kolmogorov-Smirnov tests show that our formulae are very good fits of the sample distributions. The approximations are good over the entire range of traffic intensities and are even more accurate in light or heavy traffic.",2011,1588399,"Sung-Seok Ko,Jin Young Choi,Dong-Won Seo",0
1588400,A hybrid radial basis function and data envelopment analysis neural network for classification.,"We propose a hybrid radial basis function network-data envelopment analysis (RBFN-DEA) neural network for classification problems. The procedure uses the radial basis function to map low dimensional input data from input space @? to a high dimensional @?^+ feature space where DEA can be used to learn the classification function. Using simulated datasets for a non-linearly separable binary classification problem, we illustrate how the RBFN-DEA neural network can be used to solve it. We also show how asymmetric misclassification costs can be incorporated in the hybrid RBFN-DEA model. Our preliminary experiments comparing the RBFN-DEA with feed forward and probabilistic neural networks show that the RBFN-DEA fares very well.",2011,1588400,Parag C. Pendharkar,0
1588401,Price strategy implementation.,"Consider a situation in which a company sells several different items to a set of customers. However, the company is not satisfied with the current pricing strategy and wishes to implement new prices for its items. Implementing these new prices in one single step might not be desirable, for example, because of the change in contract prices for the customers. Therefore, the company changes the prices gradually, such that the prices charged to a subset of the customers, the target market, do not differ too much from one period to the next. We propose a polynomial time algorithm to implement the new prices in the minimum number of time periods needed, given that the prices charged to the customers in the target market increase by at most a factor 1+@d, for a given @d>0. Furthermore, we address the problem of maximizing the overall revenue during the price implementation over a given time horizon. For this problem, we describe a dynamic program for the case of integer price vectors, and a local search algorithm for arbitrary prices. Also, we present a mixed integer programming formulation for this problem and apply our algorithms in a practical study.",2011,1588401,"André Berger,Alexander Grigoriev,Joyce van Loon",0
1588402,On minimax-regret Huff location models.,"We address the following single-facility location problem: a firm is entering into a market by locating one facility in a region of the plane. The demand captured from each user by the facility will be proportional to the users buying power and inversely proportional to a function of the user-facility distance. Uncertainty exists on the buying power (weight) of the users. This is modeled by assuming that a set of scenarios exists, each scenario corresponding to a weight realization. The objective is to locate the facility following the Savage criterion, i.e., the minimax-regret location is sought. The problem is formulated as a global optimization problem with objective written as difference of two convex monotonic functions. The numerical results obtained show that a branch and bound using this new method for obtaining bounds clearly outperforms benchmark procedures.",2011,1588402,"Lenys Bello,Rafael Blanquero,Emilio Carrizosa",0
1588403,Extended guided tabu search and a new packing algorithm for the two-dimensional loading vehicle routing problem.,"In this paper, we develop an extended guided tabu search (EGTS) and a new heuristic packing algorithm for the two-dimensional loading vehicle routing problem (2L-CVRP). The 2L-CVRP is a combination of two well-known NP-hard problems, the capacitated vehicle routing problem, and the two-dimensional bin packing problem. It is very difficult to get a good performance solution in practice for these problems. We propose a meta-heuristic methodology EGTS which incorporates theories of tabu search and extended guided local search (EGLS). It has been proved that tabu search is a very good approach for the CVRP, and the guiding mechanism of the EGLS can help tabu search to escape effectively from local optimum. Furthermore, we have modified a collection of packing heuristics by adding a new packing heuristic to solve the loading constraints in 2L-CVRP, in order to improve the cost function significantly. The effectiveness of the proposed algorithm is tested, and proven by extensive computational experiments on benchmark instances.",2011,1588403,"Stephen C. H. Leung,Xiyue Zhou,Defu Zhang,Jiemin Zheng",0
1588404,On solving the assembly line worker assignment and balancing problem via beam search.,"Certain types of manufacturing processes can be modelled by assembly line balancing problems. In this work we deal with a specific assembly line balancing problem that is known as the assembly line worker assignment and balancing problem (ALWABP). This problem appears in settings where tasks must be assigned to workers, and workers to work stations. Task processing times are worker specific, and workers might even be incompatible with certain tasks. The ALWABP was introduced to model assembly lines typical for sheltered work centers for the Disabled. In this paper we introduce an algorithm based on beam search for solving the ALWABP with the objective of minimizing the cycle time when given a fixed number of work stations, respectively, workers. This problem version is denoted as ALWABP-2. The experimental results show that our algorithm is currently a state-of-the-art method for the ALWABP-2. In comparison to results from the literature, our algorithm obtains better or equal results in all cases. Moreover, the algorithm is very robust for what concerns the application to problem instances of different characteristics.",2011,1588404,"Christian Blum,Cristóbal Miralles",0
1588405,Heuristic algorithms for the general nonlinear separable knapsack problem.,"We consider the nonlinear knapsack problem with separable nonconvex functions. Depending on the assumption on the integrality of the variables, this problem can be modeled as a nonlinear programming or as a (mixed) integer nonlinear programming problem. In both cases, this class of problems is very difficult to solve, both from a theoretical and a practical viewpoint. We propose a fast heuristic algorithm, and a local search post-optimization procedure. A series of computational comparisons with a heuristic method for general nonconvex mixed integer nonlinear programming and with global optimization methods shows that the proposed algorithms provide high-quality solutions within very short computing times.",2011,1588405,"Claudia D'Ambrosio,Silvano Martello",0
1588406,Heuristic methods for the optimal statistic median problem.,"We consider the problem of finding the convex combination of vectors for which the median is maximum. The objective function of this problem is non-concave and non-differentiable, with many local optima that can trap any subgradient algorithm. So we analyzed and tested some heuristic procedures to find optimal or near-optimal solutions. First, we introduced a variant of Random Restart, in which starting solutions are the vertices of the simplex, and we proved that small size problems are solved by this procedure. Then, we analyzed two versions of Variable Neighborhood Search that are used to explore the whole space of the feasible solutions, and we show that the continuous version is more powerful than the discrete version.",2011,1588406,Stefano Benati,0
1588407,A meta-heuristic approach for improving the accuracy in some classification algorithms.,"Current classification algorithms usually do not try to achieve a balance between fitting and generalization when they infer models from training data. Furthermore, current algorithms ignore the fact that there may be different penalty costs for the false-positive, false-negative, and unclassifiable types. Thus, their performance may not be optimal or may even be coincidental. This paper proposes a meta-heuristic approach, called the Convexity Based Algorithm (CBA), to address these issues. The new approach aims at optimally balancing the data fitting and generalization behaviors of models when some traditional classification approaches are used. The CBA first defines the total misclassification cost (TC) as a weighted function of the three penalty costs and the corresponding error rates as mentioned above. Next it partitions the training data into regions. This is done according to some convexity properties derivable from the training data and the traditional classification method to be used in conjunction with the CBA. Next the CBA uses a genetic approach to determine the optimal levels of fitting and generalization. The TC is used as the fitness function in this genetic approach. Twelve real-life datasets from a wide spectrum of domains were used to better understand the effectiveness of the proposed approach. The computational results indicate that the CBA may potentially fill in a critical gap in the use of current or future classification algorithms.",2011,1588407,"Huy Nguyen Anh Pham,Evangelos Triantaphyllou",0
1588408,Analysis of facility protection strategies against an uncertain number of attacks: The stochastic R-interdiction median problem with fortification.,"We present the Stochastic R-Interdiction Median Problem with Fortification (S-RIMF). This model optimally allocates defensive resources among facilities to minimize the worst-case impact of an intentional disruption. Since the extent of terrorist attacks and malicious actions is uncertain, the problem deals with a random number of possible losses. A max-covering type formulation for the S-RIMF is developed. Since the problem size grows very rapidly with the problem inputs, we propose pre-processing techniques based on the computation of valid lower and upper bounds to expedite the solution of instances of realistic size. We also present heuristic approaches based on heuristic concentration-type rules. The heuristics are able to find an optimal solution for almost all the problem instances considered. Extensive computational testing shows that both the optimal algorithm and the heuristics are very successful at solving the problem. Finally, a discussion of the importance of recognizing the stochastic nature of the number of possible attacks is provided.",2011,1588408,"Federico Liberatore,Maria Paola Scaparra,Mark S. Daskin",0
1588409,An exact bit-parallel algorithm for the maximum clique problem.,"This paper presents a new exact maximum clique algorithm which improves the bounds obtained in state of the art approximate coloring by reordering the vertices at each step. Moreover, the algorithm can make full use of bit strings to sort vertices in constant time as well as to compute graph transitions and bounds efficiently, exploiting the ability of CPUs to process bitwise operations in blocks of size the ALU register word. As a result it significantly outperforms a current leading algorithm.",2011,1588409,"Pablo San Segundo,Diego Rodríguez-Losada,Agustín Jiménez",0
1588410,An efficient placement heuristic for three-dimensional rectangular packing.,"By embodying the spirit of ''gold corner, silver side and strawy void'' directly on the candidate packing place such that the searching space is reduced considerably, and by utilizing the characteristic of weakly heterogeneous problems that many items are in the same size, a fit degree algorithm (FDA) is proposed for solving a classical 3D rectangular packing problem, container loading problem. Experiments show that FDA works well on the complete set of 1500 instances proposed by Bischoff, Ratcliff and Davies. Especially for the 800 difficult strongly heterogeneous instances among them, FDA outperforms other algorithms with an average volume utilization of 91.91%, which to our knowledge is 0.45% higher than current best result just reported in 2010.",2011,1588410,"Kun He,Wenqi Huang",0
1588411,Bilevel model for production-distribution planning solved by using ant colony optimization.,"This paper addresses a hierarchical production-distribution planning problem. There are two different decision makers controlling the production and the distribution processes, respectively, that do not cooperate because of different optimization strategies. The distribution company, which is the leader of the hierarchical process, controls the allocation of retailers to each depot and the routes which serve them. In order to supply items to retailers, the distribution company orders from the manufacturing company the items which have to be available at the depots. The manufacturing company, which is the follower of the hierarchical process, reacts to these orders deciding which manufacturing plants will produce them. A bilevel program is proposed to model the problem and an ant colony optimization based approach is developed to solve the bilevel model. In order to construct a feasible solution, the procedure uses ants to compute the routes of a feasible solution of the associated multi-depot vehicle route problem. Then, under the given data on depot needs, the corresponding production problem of the manufacturing company is solved. Global pheromone trail updating is based on the leader objective function, which involves costs of sending items from depots to retailers and costs of acquiring items from manufacturing plants and unloading them into depots. A computational experiment is carried out to analyze the performance of the algorithm.",2011,1588411,"Herminia I. Calvete,Carmen Galé,María-José Oliveros",0
1588412,An efficient Lagrangean heuristic for rental vehicle scheduling.,"In this article we present a heuristic based on Lagrangean relaxation for scheduling rental vehicles. The scheduling problem can be formulated as a set of large assignment problems with linking constraints. We discuss the theory behind the heuristic, including the ability to obtain lower bounds. The heuristic is based on ideas first introduced by D. Wedelin. Empirical testing shows that our heuristic is superior to the exact solution procedure of the MILP formulation with the commercial ILOG Cplex solver as it generally produces better solutions within a set time limit. Moreover, the heuristic can easily be parallelized to take advantage of multicore CPUs. The work presented here has been motivated by a real-life application and is currently being used by companies that rent out recreational vehicles. Testing on data sets from these companies shows the practicality of our algorithm.",2011,1588412,"Andreas T. Ernst,Elena O. Gavriliouk,Leorey Marquez",0
1588413,A computational study of parametric tabu search for 0-1 mixed integer programs.,"We present a computational study of parametric tabu search for solving 0-1 mixed integer programming (MIP) problems, a generic heuristic for general MIP problems proposed by Glover [Glover F. Parametric tabu-search for mixed integer programs. Computers and Operations Research 2006; 33: 2449-94.]. This approach solves a series of linear programming problems by incorporating branching inequalities as weighted terms in the objective function. New strategies are proposed for uncovering feasible and high-quality solutions and extensive computational tests are performed on instances from the literature.",2011,1588413,"Luís Henrique Sacchi,Vinícius Amaral Armentano",0
1588414,Single-allocation ordered median hub location problems.,"The discrete ordered median location model is a powerful tool in modeling classic and alternative location problems that have been applied with success to a large variety of discrete location problems. Nevertheless, although hub location models have been analyzed from the sum, maximum and coverage point of views, as far as we know, they have never been considered under an alternative unifying point of view. In this paper we consider new formulations, based on the ordered median objective function, for hub location problems with new distribution patterns induced by the different users' roles within the supply chain network. This approach introduces some penalty factors associated with the position of an allocation cost with respect to the sorted sequence of these costs. First we present basic formulations for this problem, and then develop stronger formulations by exploiting properties of the model. The performance of all these formulations is compared by means of a computational analysis.",2011,1588414,"Justo Puerto,A. B. Ramos,Antonio M. Rodríguez-Chía",0
1588415,Reactive scheduling in the multi-mode RCPSP.,"The multi-mode resource-constrained project scheduling problem (MRCPSP) involves the determination of a baseline schedule of the project activities, which can be executed in multiple modes, satisfying the precedence relations and resource constraints while minimizing the project duration. During the execution of the project, the baseline schedule may become infeasible due to activity duration and resource disruptions. We propose and evaluate a number of dedicated exact reactive scheduling procedures as well as a tabu search heuristic for repairing a disrupted schedule, under the assumption that no activity can be started before its baseline starting time. We report on promising computational results obtained on a set of benchmark problems.",2011,1588415,"Filip Deblaere,Erik Demeulemeester,Willy Herroelen",0
1588416,Integrated task assignment and path optimization for cooperating uninhabited aerial vehicles using genetic algorithms.,"The problem of integrating task assignment and planning paths for a group of cooperating uninhabited aerial vehicles, servicing multiple targets, is addressed. In the problem of interest the uninhabited aerial vehicles need to perform multiple consecutive tasks cooperatively on each ground target. A Dubins car model is used for motion planning, taking into account each vehicle's specific constraint of minimum turn radius. By using a finite set to define the visitation angle of a vehicle over a target we pose the integrated problem of task assignment and path optimization in the form of a graph. This new approach results in suboptimal trajectory assignments. Refining the visitation angle discretization allows for an improved solution. Due to the computational complexity of the resulting combinatorial optimization problem, we propose genetic algorithms for the stochastic search of the space of solutions. We distinguish between two cases of vehicle group composition: homogeneous, where all vehicles are identical; and heterogeneous, where the vehicles may have different operational capabilities and kinematic constraints. The performance of the genetic algorithms is demonstrated through sample runs and a Monte Carlo simulation study. Results show that the algorithms quickly provide good feasible solutions, and find the optimal solution for small sized problems.",2011,1588416,"Eugene Edison,Tal Shima",0
1588417,Minimum cost path problems with relays.,"The minimum cost path problem with relays (MCPPR) consists of finding a minimum cost path from a source to a destination, along which relay nodes are located at a certain cost, subject to a weight constraint. This paper first models the MCPPR as a particular bicriteria path problem involving an aggregated function of the path and relay costs, as well as a weight function. A variant of this problem which takes into account all three functions separately is then considered. Formulating the MCPPR as a part of a bicriteria path problem allows the development of labeling algorithms in which the bound on the weight of paths controls the number of node labels. The algorithm for this constrained single objective function version of the problem has a time complexity of O(Wm+Wnlog(max{W,n})), where n is the number of nodes, m is the number of arcs and W is the weight upper bound. Computational results on random instances with up to 10000 nodes and 100000 arcs, are reported.",2011,1588417,"Gilbert Laporte,Marta M. B. Pascoal",0
1588418,Slacks-based measure of efficiency of airports with airplanes delays as undesirable outputs.,"This paper reports the slacks-based measure (SBM) of efficiency of 39 Spanish airports for years 2006 and 2007. In addition to the conventional outputs (namely aircraft traffic movements, passenger movements and cargo handled), two undesirable outputs have been considered: percentage of delayed flights and average conditional delay of delayed flights. The inputs considered quantify the physical infrastructure of the airports and are considered non-discretionary. The proposed Data Envelopment Analysis (DEA) approach assumes variable returns to scale and joint weak disposability of the desirable and undesirable outputs. The SBM model used has been found to have more discriminatory power than the common directional distance function approach. Also, the inclusion in the analysis of the undesirable effects of airport operations leads to more valid results. The results show that in both years more than half of the airports are technical efficient with the rest showing in general large inefficiencies due to slacks in the different outputs, slacks that the proposed SBM approach is able to identify and quantify. Overall, the system has significant improvement potential in cargo and to a less extent in passengers and percentage of delayed flights.",2011,1588418,"Sebastián Lozano,Ester Gutiérrez",0
1588419,Event-based MILP models for resource-constrained project scheduling problems.,"In this paper we make a comparative study of several mixed integer linear programming (MILP) formulations for resource-constrained project scheduling problems (RCPSPs). First, we present three discrete and continuous time MILP formulations issued from the literature. Second, instead of relying on the traditional discretization of the time horizon, we propose MILP formulations for the RCPSP based on the concept of event: the Start/End formulation and the On/Off formulation. These formulations present the advantage of involving fewer variables than the formulations indexed by time. Because the variables of this type of formulations are not function of the time horizon, we have a better capacity to deal with instances of very large scheduling horizon. Finally, we illustrate our contribution with a series of tests on various types of instance with the MILP formulations issued from the literature, together with our new formulations. We also compare our results with a recent RCPSP-specific exact method. We show that, in terms of exact solving, no MILP formulation class dominates the other ones and that a state-of-the art specialized (non-MILP) method for the RCPSP is even outperformed by MILP on a set of hard instances. Furthermore, on another set of hard ''highly cumulative'' RCPSP instances with a high processing time range, our On/Off formulation outperforms all the others MILP formulations and obtains results close to the ones of the specialized method.",2011,1588419,"Oumar Koné,Christian Artigues,Pierre Lopez,Marcel Mongeau",0
1588420,Simultaneous location of a service facility and a rapid transit line.,"This paper presents a location problem on the plane where a single service facility and a rapid transit line have to be simultaneously located. The rapid transit line represents an alternative transportation line which can be used by clients whenever it provides a cost-saving or time-saving service, and it is given by a segment with fixed and known length. This type of problems has not previously been considered in the Location Theory literature, as we are only aware of the existence of models that consider the location of service facilities in the presence of an already located alternative transportation system or models dealing with the location of rapid transit lines to minimize the travelling time among a set of points. To solve this problem we will develop an algorithm based on some characterizations of the objective function behavior.",2011,1588420,"Inmaculada Espejo,Antonio M. Rodríguez-Chía",0
1588421,Planning working time accounts under demand uncertainty.,"Working time accounts (WTAs) are employer-oriented flexibility systems that have been applied in industry but could be used far more. WTAs enable capacity to be adapted to fluctuations in demand. The required capacity, which is needed to plan WTAs, usually depends on several factors. It is often impossible to reliably predict the required capacity or unrealistic to adjust it to a probability distribution. In some cases, a set of required-capacity scenarios can be determined, each with a related probability. This paper presents a multistage stochastic optimisation model that is robust (i.e., provides a solution that is feasible for any possible scenario) and minimises the expected total cost (which includes the cost of overtime and the cost of the capacity shortage).",2011,1588421,"Amaia Lusa,Rafael Pastor",0
1588422,A large neighbourhood search heuristic for ship routing and scheduling with split loads.,"The purpose of this paper is to present and solve a new, important planning problem faced by many shipping companies dealing with the transport of bulk products. These shipping companies are committed to carrying some contract cargoes and will try to derive additional revenue from optional spot cargoes. In most of the literature on ship routing and scheduling problems a cargo cannot be transported by more than one ship. By introducing split loads this restriction is removed and each cargo can be transported by several ships. In this paper we propose a large neighbourhood search heuristic for the ship routing and scheduling problem with split loads. Computational results show that the heuristic provides good solutions to real-life instances within reasonable time. It is also shown that introducing split loads can yield significant improvements.",2011,1588422,"Jarl Eirik Korsvik,Kjetil Fagerholt,Gilbert Laporte",0
1588423,Flow shop scheduling to minimize the total completion time with a permanently present operator: Models and ant colony optimization metaheuristic.,"This paper studies the one-operator m-machine flow shop scheduling problem with the objective of minimizing the total completion time. In this problem, the processing of jobs and setup of machines require the continuous presence of a single operator. We compare three different mathematical formulations and propose an ant colony optimization based metaheuristic to solve this flow shop scheduling problem. A series of experiments are carried out to compare the properties of three formulations and to investigate the performance of the proposed ant colony optimization metaheuristic. The computational results show that (1) an assignment-based formulation performs best, and (2) the ant colony optimization based metaheuristic is a computationally efficient algorithm.",2011,1588423,"Xiangyong Li,Md. Fazle Baki,Yash P. Aneja",0
1588424,Combining contemporary and traditional project management tools to resolve a project scheduling problem.,"In this paper we examine a construction project involving the building of large concrete slabs for three buildings in an office park complex. There are finish-to-start (FS) as well as start-to-start (SS) and finish-to-finish (FF) precedence relationships among the project activities. We prepare an initial project schedule using Microsoft Project and manually validate the results using the precedence diagramming method (PDM) procedure. When the client informs us that the schedule must be shortened we find that Microsoft Project does not have the capability for resolving our particular time/cost tradeoff issues. So we revert to the traditional approach for resolving time/cost tradeoffs in projects and develop an original linear programming formulation for the time/cost tradeoff problem when a project is modeled as a precedence diagram. By combining contemporary (Microsoft Project) and traditional (a linear programming time/cost tradeoff model) project management tools we are able to successfully resolve the scheduling issues associated with the slab construction project. Further, we demonstrate the anomalous effects of start-to-start (SS) and finish-to-finish (FF) relationships via our construction project example in which the solution to the time/cost tradeoff problem requires that certain activities be lengthened in order to shorten the project duration.",2011,1588424,"John E. Hebert,Richard F. Deckro",0
1602922,Regularizing multiple kernel learning using response surface methodology.,"In recent years, several methods have been proposed to combine multiple kernels using a weighted linear sum of kernels. These different kernels may be using information coming from multiple sources or may correspond to using different notions of similarity on the same source. We note that such methods, in addition to the usual ones of the canonical support vector machine formulation, introduce new regularization parameters that affect the solution quality and, in this work, we propose to optimize them using response surface methodology on cross-validation data. On several bioinformatics and digit recognition benchmark data sets, we compare multiple kernel learning and our proposed regularized variant in terms of accuracy, support vector count, and the number of kernels selected. We see that our proposed variant achieves statistically similar or higher accuracy results by using fewer kernel functions and/or support vectors through suitable regularization; it also allows better knowledge extraction because unnecessary kernels are pruned and the favored kernels reflect the properties of the problem at hand.",2011,1602922,"Mehmet Gönen,Ethem Alpaydin",0
1603061,A discrete geometry approach for dominant point detection.,"We propose two fast methods for dominant point detection and polygonal representation of noisy and possibly disconnected curves based on a study of the decomposition of the curve into the sequence of maximal blurred segments [2]. Starting from results of discrete geometry [3,4], the notion of maximal blurred segment of width @n[2] has been proposed, well adapted to possibly noisy curves. The first method uses a fixed parameter that is the width of considered maximal blurred segments. The second method is deduced from the first one based on a multi-width approach to obtain a non-parametric method that uses no threshold for working with noisy curves. Comparisons with other methods in the literature prove the efficiency of our approach. Thanks to a recent result [5] concerning the construction of the sequence of maximal blurred segments, the complexity of the proposed methods is O(nlogn). An application of vectorization is also given in this paper.",2011,1603061,"Thanh Phuong Nguyen,Isabelle Debled-Rennesson",0
1603062,Long distance bigram models applied to word clustering.,"Two novel word clustering techniques are proposed which employ long distance bigram language models. The first technique is built on a hierarchical clustering algorithm and minimizes the sum of Mahalanobis distances of all words after a cluster merger from the centroid of the class created by merging. The second technique resorts to probabilistic latent semantic analysis (PLSA). Next, interpolated long distance bigrams are considered in the context of the aforementioned clustering techniques. Experiments conducted on the English Gigaword corpus (second edition) demonstrate that: (1) the long distance bigrams, when employed in the two clustering techniques under study, yield word clusters of better quality than the baseline bigrams; (2) the interpolated long distance bigrams outperform the long distance bigrams in the same respect; (3) the long distance bigrams perform better than the bigrams, which incorporate trigger-pairs selected at various distances; and (4) the best word clustering is achieved by the PLSA that employs interpolated long distance bigrams. Both proposed techniques outperform spectral clustering based on k-means. To assess objectively the quality of the created clusters, relative cluster validity indices are estimated as well as the average cluster sense precision, the average cluster sense recall, and the F-measure are computed by exploiting ground truth extracted from the WordNet.",2011,1603062,"Nikoletta Bassiou,Constantine Kotropoulos",0
1603063,Sparse ensembles using weighted combination methods based on linear programming.,"An ensemble of multiple classifiers is widely considered to be an effective technique for improving accuracy and stability of a single classifier. This paper proposes a framework of sparse ensembles and deals with new linear weighted combination methods for sparse ensembles. Sparse ensemble is to sparsely combine the outputs of multiple classifiers by using a sparse weight vector. When the continuous outputs of multiple classifiers are provided in our methods, the problem of solving sparse weight vector can be formulated as linear programming problems in which the hinge loss or/and the 1-norm regularization are exploited. Both the hinge loss and the 1-norm regularization are techniques inducing sparsity used in machine learning. We only ensemble classifiers with nonzero weight coefficients. In these LP-based methods, the ensemble training error is minimized while the weight vector of ensemble learning is controlled, which can be thought as implementing the structure risk minimization rule and naturally explains good performance of these methods. The promising experimental results over UCI data sets and the radar high-resolution range profile data are presented.",2011,1603063,"Li Zhang,Weida Zhou",0
1603064,Facial expression recognition on multiple manifolds.,"Manifold learning has been successfully applied to facial expression recognition by modeling different expressions as a smooth manifold embedded in a high dimensional space. However, the assumption of single manifold is still arguable and therefore does not necessarily guarantee the best classification accuracy. In this paper, a generalized framework for modeling and recognizing facial expressions on multiple manifolds is presented which assumes that different expressions may reside on different manifolds of possibly different dimensionalities. The intrinsic features of each expression are firstly learned separately and the genetic algorithm (GA) is then employed to obtain the nearly optimal dimensionality of each expression manifold from the classification viewpoint. Classification is performed under a newly defined criterion that is based on the minimum reconstruction error on manifolds. Extensive experiments on both the Cohn-Kanade and Feedtum databases show the effectiveness of the proposed multiple manifold based approach.",2011,1603064,"Rui Xiao,Qijun Zhao,David Zhang,Pengfei Shi",0
1603065,Arabic script web page language identifications using decision tree neural networks.,"In this paper, we propose a hybrid approach of Arabic scripts web page language identification based on decision tree and ARTMAP approaches. We use the decision tree approach to find the general identities of a web document, be it an Arabic script-based or a non-Arabic-based. Then, we use the selected representations of identified pages from the decision tree approach as an input to the ARTMAP neural network for further verification of the diversity of languages detected by the algorithm. From our initial experiments, we found that, although the decision tree approach may achieve a higher accuracy than ARTMAP, the former may not be as reliable as the ARTMAP approach if the language used is extended to other types of Arabic script web documents in different languages (e.g., Urdu, Arabic, Persian, etc.). Therefore, we propose this hybrid decision tree-ARTMAP approach in order to improve the performance of the Arabic script language identification on web documents in a variety of languages. The result shows that the proposed approach has outperformed both decision tree and the default ARTMAP approaches.",2011,1603065,"Ali Selamat,Choon-Ching Ng",0
1603066,Detecting and discriminating behavioural anomalies.,"This paper aims to address the problem of anomaly detection and discrimination in complex behaviours, where anomalies are subtle and difficult to detect owing to the complex temporal dynamics and correlations among multiple objects' behaviours. Specifically, we decompose a complex behaviour pattern according to its temporal characteristics or spatial-temporal visual contexts. The decomposed behaviour is then modelled using a cascade of Dynamic Bayesian Networks (CasDBNs). In contrast to existing standalone models, the proposed behaviour decomposition and cascade modelling offers distinct advantage in simplicity for complex behaviour modelling. Importantly, the decomposition and cascade structure map naturally to the structure of complex behaviour, allowing for a more effective detection of subtle anomalies in surveillance videos. Comparative experiments using both indoor and outdoor data are carried out to demonstrate that, in addition to the novel capability of discriminating different types of anomalies, the proposed framework outperforms existing methods in detecting durational anomalies in complex behaviours and subtle anomalies that are difficult to detect when objects are viewed in isolation.",2011,1603066,"Chen Change Loy,Tao Xiang,Shaogang Gong",0
1603067,Color image segmentation using histogram thresholding - Fuzzy C-means hybrid approach.,"This paper presents a novel histogram thresholding - fuzzy C-means hybrid (HTFCM) approach that could find different application in pattern recognition as well as in computer vision, particularly in color image segmentation. The proposed approach applies the histogram thresholding technique to obtain all possible uniform regions in the color image. Then, the Fuzzy C-means (FCM) algorithm is utilized to improve the compactness of the clusters forming these uniform regions. Experimental results have demonstrated that the low complexity of the proposed HTFCM approach could obtain better cluster quality and segmentation results than other segmentation approaches that employing ant colony algorithm.",2011,1603067,"Khang Siang Tan,Nor Ashidi Mat Isa",0
1603068,Solving the minimum sum-of-squares clustering problem by hyperbolic smoothing and partition into boundary and gravitational regions.,"This article considers the minimum sum-of-squares clustering (MSSC) problem. The mathematical modeling of this problem leads to a min-sum-min formulation which, in addition to its intrinsic bi-level nature, has the significant characteristic of being strongly nondifferentiable. To overcome these difficulties, the proposed resolution method, called hyperbolic smoothing, adopts a smoothing strategy using a special C^~ differentiable class function. The final solution is obtained by solving a sequence of low dimension differentiable unconstrained optimization subproblems which gradually approach the original problem. This paper introduces the method of partition of the set of observations into two nonoverlapping groups: ''data in frontier'' and ''data in gravitational regions''. The resulting combination of the two methodologies for the MSSC problem has interesting properties, which drastically simplify the computational tasks.",2011,1603068,"Adilson Elias Xavier,Vinicius Layter Xavier",0
1603069,Choice of a pertinent color space for color texture characterization using parametric spectral analysis.,"This article presents a comparison of different color spaces including RGB, IHLS and L@?a*b* for color texture characterization. This comparison is based on the fusion of the independent spatial structure and color feature cues. In IHLS and L*a*b*, two channel complex color images are created from the luminance and the chrominance values. For such images, two dimensional complex multichannel linear prediction models are used to perform parametric power spectrum estimation and the structure feature cues are computed from this estimated power spectrum. Quantitative comparison of auto spectra of luminance and combined chrominance channels for different color spaces is done. This comparison is based on the degree of decorrelation between luminance and chrominance information provided by different color space transformations. Three dimensional histograms are used as color feature cues. Then, to classify color textures, Kullback-Leibler divergence based symmetric distance measures are calculated for pure color, luminance structure and chrominance structure feature cues. Individual as well as combined effect of information from all feature cues on classification results is then compared for different color spaces and different color texture data sets. The proposed color texture classification method performs better than the state of the art methods in certain cases. The L*a*b* color space gives us a better characterization of the chrominance spatial structure as well as the overall spatial structure for all of the chosen data sets. Experimental results on pixel classification of color textures are also presented and discussed.",2011,1603069,"Imtnan-Ul-Haque Qazi,Olivier Alata,Jean-Christophe Burie,Ahmed Moussa,Christine Fernandez-Maloigne",0
1603070,Clustering ellipses for anomaly detection.,"Comparing, clustering and merging ellipsoids are problems that arise in various applications, e.g., anomaly detection in wireless sensor networks and motif-based patterned fabrics. We develop a theory underlying three measures of similarity that can be used to find groups of similar ellipsoids in p-space. Clusters of ellipsoids are suggested by dark blocks along the diagonal of a reordered dissimilarity image (RDI). The RDI is built with the recursive iVAT algorithm using any of the three (dis) similarity measures as input and performs two functions: (i) it is used to visually assess and estimate the number of possible clusters in the data; and (ii) it offers a means for comparing the three similarity measures. Finally, we apply the single linkage and CLODD clustering algorithms to three two-dimensional data sets using each of the three dissimilarity matrices as input. Two data sets are synthetic, and the third is a set of real WSN data that has one known second order node anomaly. We conclude that focal distance is the best measure of elliptical similarity, iVAT images are a reliable basis for estimating cluster structures in sets of ellipsoids, and single linkage can successfully extract the indicated clusters.",2011,1603070,"Masud Moshtaghi,Timothy C. Havens,James C. Bezdek,Laurence A. F. Park,Christopher Leckie,Sutharshan Rajasegarar,James M. Keller,Marimuthu Palaniswami",0
1603071,A new measurement for assessing polygonal approximation of curves.,"This paper presents a novel method for assessing the accuracy of unsupervised polygonal approximation algorithms. This measurement relies on a polygonal approximation called the ''reference approximation''. The reference approximation is obtained using the method of Perez and Vidal [11] by an iterative method that optimizes an objective function. Then, the proposed measurement is calculated by comparing the reference approximation with the approximation to be evaluated, taking into account the similarity between the polygonal approximation and the original contour, and penalizing polygonal approximations with an excessive number of points. A comparative experiment by using polygonal approximations obtained with commonly used algorithms showed that the proposed measurement is more efficient than other proposed measurements at comparing polygonal approximations with different number of points.",2011,1603071,"Ángel Carmona Poyato,Rafael Medina Carnicer,Francisco José Madrid-Cuevas,Rafael Muñoz-Salinas,Nicolás Luis Fernández García",0
1607915,Expression of affect in spontaneous speech: Acoustic correlates and automatic detection of irritation and resignation.,"The majority of previous studies on vocal expression have been conducted on posed expressions. In contrast, we utilized a large corpus of authentic affective speech recorded from real-life voice controlled telephone services. Listeners rated a selection of 200 utterances from this corpus with regard to level of perceived irritation, resignation, neutrality, and emotion intensity. The selected utterances came from 64 different speakers who each provided both neutral and affective stimuli. All utterances were further automatically analyzed regarding a comprehensive set of acoustic measures related to F0, intensity, formants, voice source, and temporal characteristics of speech. Results first showed that several significant acoustic differences were found between utterances classified as neutral and utterances classified as irritated or resigned using a within-persons design. Second, listeners' ratings on each scale were associated with several acoustic measures. In general the acoustic correlates of irritation, resignation, and emotion intensity were similar to previous findings obtained with posed expressions, though the effect sizes were smaller for the authentic expressions. Third, automatic classification (using LDA classifiers both with and without speaker adaptation) of irritation, resignation, and neutral performed at a level comparable to human performance, though human listeners and machines did not necessarily classify individual utterances similarly. Fourth, clearly perceived exemplars of irritation and resignation were rare in our corpus. These findings were discussed in relation to future research.",2011,1607915,"Petri Laukka,Daniel Neiberg,Mimmi Forsell,Inger Karlsson,Kjell Elenius",0
1607916,Fiction support for realistic portrayals of fear-type emotional manifestations.,"The present paper aims at filling the lack that currently exists with respect to databases containing emotional manifestations. Emotions, such as strong emotions, are indeed difficult to collect in real-life. They occur during contexts, which are generally unpredictable, and some of them such as anger are less frequent in public life than in private. Even though such emotions are not so present in existing databases, the need for applications, which target them (crisis management, surveillance, strategic intelligence, etc.), and the need for emotional recordings is even more acute. We propose here to use fictional media to compensate for the difficulty of collecting strong emotions. Emotions in realistic fictions are portrayed by skilled actors in interpersonal interactions. The mise-en-scene of the actors tends to stir genuine emotions. In addition, fiction offers an overall view of emotional manifestations in various real-life contexts: face-to-face interactions, phone calls, interviews, emotional event reporting vs. in situ emotional manifestations. A fear-type emotion recognition system has been developed, that is based on acoustic models learnt from the fiction corpus. This paper aims at providing an in-depth analysis of the various factors that may influence the system behaviour: the annotation issue and the acoustic features behaviour. These two aspects emphasize the main feature of fiction: the variety of the emotional manifestations and of their context.",2011,1607916,"Chloé Clavel,I. Vasilescu,Laurence Devillers",0
1607917,Automatic inference of complex affective states.,"Affective states and their non-verbal expressions are an important aspect of human reasoning, communication and social life. Automated recognition of affective states can be integrated into a wide variety of applications for various fields. Therefore, it is of interest to design systems that can infer the affective states of speakers from the non-verbal expressions in speech, occurring in real scenarios. This paper presents such a system and the framework for its design and validation. The framework defines a representation method that comprises a set of affective-state groups or archetypes that often appear in everyday life. The inference system is designed to infer combinations of affective states that can occur simultaneously and whose level of expression can change over time. The framework considers also the validation and generalisation of the system. The system was built of 36 independent pair-wise comparison machines, with average accuracy (tenfold cross-validation) of 75%. The accumulated inference system yielded total accuracy of 83% and recognised combinations for different nuances within the affective-state groups. In addition to the ability to recognise these affective-state groups, the inference system was applied to characterisation of a very large variety of affective state concepts (549 concepts) as combinations of the affective-state groups. The system was also applied to annotation of affective states that were naturally evoked during sustained human-computer interactions and multi-modal analysis of the interactions, to new speakers and to a different language, with no additional training. The system provides a powerful tool for recognition, characterisation, annotation (interpretation) and analysis of affective states. In addition, the results inferred from speech in both English and Hebrew, indicate that the vocal expressions of complex affective states such as thinking, certainty and interest transcend language boundaries.",2011,1607917,Tal Sobol Shikler,0
1607918,Whodunnit - Searching for the most important feature types signalling emotion-related user states in speech.,"In this article, we describe and interpret a set of acoustic and linguistic features that characterise emotional/emotion-related user states - confined to the one database processed: four classes in a German corpus of children interacting with a pet robot. To this end, we collected a very large feature vector consisting of more than 4000 features extracted at different sites. We performed extensive feature selection (Sequential Forward Floating Search) for seven acoustic and four linguistic types of features, ending up in a small number of 'most important' features which we try to interpret by discussing the impact of different feature and extraction types. We establish different measures of impact and discuss the mutual influence of acoustics and linguistics.",2011,1607918,"Anton Batliner,Stefan Steidl,Björn Schuller,Dino Seppi,Thurid Vogt,Johannes Wagner,Laurence Devillers,Laurence Vidrascu,Vered Aharonson,Loïc Kessous,Noam Amir",0
1607919,Detecting emotional state of a child in a conversational computer game.,"The automatic recognition of user's communicative style within a spoken dialog system framework, including the affective aspects, has received increased attention in the past few years. For dialog systems, it is important to know not only what was said but also how something was communicated, so that the system can engage the user in a richer and more natural interaction. This paper addresses the problem of automatically detecting ''frustration'', ''politeness'', and ''neutral'' attitudes from a child's speech communication cues, elicited in spontaneous dialog interactions with computer characters. Several information sources such as acoustic, lexical, and contextual features, as well as, their combinations are used for this purpose. The study is based on a Wizard-of-Oz dialog corpus of 103 children, 7-14 years of age, playing a voice activated computer game. Three-way classification experiments, as well as, pairwise classification between polite vs. others and frustrated vs. others were performed. Experimental results show that lexical information has more discriminative power than acoustic and contextual cues for detection of politeness, whereas context and acoustic features perform best for frustration detection. Furthermore, the fusion of acoustic, lexical and contextual information provided significantly better classification results. Results also showed that classification performance varies with age and gender. Specifically, for the ''politeness'' detection task, higher classification accuracy was achieved for females and 10-11 years-olds, compared to males and other age groups, respectively.",2011,1607919,"Serdar Yildirim,Shrikanth Narayanan,Alexandros Potamianos",0
1607921,Designing and evaluating a wizarded uncertainty-adaptive spoken dialogue tutoring system.,"We describe the design and evaluation of two different dynamic student uncertainty adaptations in wizarded versions of a spoken dialogue tutoring system. The two adaptive systems adapt to each student turn based on its uncertainty, after an unseen human ''wizard'' performs speech recognition and natural language understanding and annotates the turn for uncertainty. The design of our two uncertainty adaptations is based on a hypothesis in the literature that uncertainty is an ''opportunity to learn''; both adaptations use additional substantive content to respond to uncertain turns, but the two adaptations vary in the complexity of these responses. The evaluation of our two uncertainty adaptations represents one of the first controlled experiments to investigate whether substantive dynamic responses to student affect can significantly improve performance in computer tutors. To our knowledge we are the first study to show that dynamically responding to uncertainty can significantly improve learning during computer tutoring. We also highlight our ongoing evaluation of our uncertainty-adaptive systems with respect to other important performance metrics, and we discuss how our corpus can be used by the wider computer speech and language community as a linguistic resource supporting further research on effective affect-adaptive spoken dialogue systems in general.",2011,1607921,"Katherine Forbes-Riley,Diane J. Litman",0
1609757,A statistical approach to high-throughput screening of predicted orthologs.,"Orthologs are genes in different species that have diverged from a common ancestral gene after speciation. In contrast, paralogs are genes that have diverged after a gene duplication event. For many comparative analyses, it is of interest to identify orthologs with similar functions. Such orthologs tend to support species divergence (ssd-orthologs) in the sense that they have diverged only due to speciation, to the same relative degree as their species. However, due to incomplete sequencing or gene loss in a species, predicted orthologs can sometimes be paralogs or other non-ssd-orthologs. To increase the specificity of ssd-ortholog prediction, Fulton et al. [Fulton, D., Li, Y., Laird, M., Horsman, B., Roche, F., Brinkman, F., 2006. Improving the specificity of high-throughput ortholog prediction. BMC Bioinformatics 7 (1), 270] developed Ortholuge, a bioinformatics tool that identifies predicted orthologs with atypical genetic divergence. However, when the initial list of putative orthologs contains a non-negligible number of non-ssd-orthologs, the cut-off values that Ortholuge generates for orthology classification are difficult to interpret and can be too high, leading to decreased specificity of ssd-ortholog prediction. Therefore, we propose a complementary statistical approach to determining cut-off values. A benefit of the proposed approach is that it gives the user an estimated conditional probability that a predicted ortholog pair is unusually diverged. This enables the interpretation and selection of cut-off values based on a direct measure of the relative composition of ssd-orthologs versus non-ssd-orthologs. In a simulation comparison of the two approaches, we find that the statistical approach provides more stable cut-off values and improves the specificity of ssd-ortholog prediction for low-quality data sets of predicted orthologs.",2011,1609757,"Jeong Eun Min,Matthew D. Whiteside,Fiona S. L. Brinkman,Brad McNeney,Jinko Graham",0
1609761,Bayesian analysis of robust Poisson geometric process model using heavy-tailed distributions.,We propose a robust Poisson geometric process model with heavy-tailed distributions to cope with the problem of outliers as it may lead to an overestimation of mean and variance resulting in inaccurate interpretations of the situations. Two heavy-tailed distributions namely Student's t and exponential power distributions with different tailednesses and kurtoses are used and they are represented in scale mixture of normal and scale mixture of uniform respectively. The proposed model is capable of describing the trend and meanwhile the mixing parameters in the scale mixture representations can detect the outlying observations. Simulations and real data analysis are performed to investigate the properties of the models.,2011,1609761,"Wai-Yin Wan,Jennifer So-Kuen Chan",0
1609762,Analysis of zero-inflated clustered count data: A marginalized model approach.,"Min and Agresti (2005) proposed random effect hurdle models for zero-inflated clustered count data with two-part random effects for a binary component and a truncated count component. In this paper, we propose new marginalized models for zero-inflated clustered count data using random effects. The marginalized models are similar to Dobbie and Welsh's (2001) model in which generalized estimating equations were exploited to find estimates. However, our proposed models are based on a likelihood-based approach. A Quasi-Newton algorithm is developed for estimation. We use these methods to carefully analyze two real datasets.",2011,1609762,"Keunbaik Lee,Yongsung Joo,Joon Jin Song,Dee Wood Harper",0
1609763,The bivariate generalized linear failure rate distribution and its multivariate extension.,"The two-parameter linear failure rate distribution has been used quite successfully to analyze lifetime data. Recently, a new three-parameter distribution, known as the generalized linear failure rate distribution, has been introduced by exponentiating the linear failure rate distribution. The generalized linear failure rate distribution is a very flexible lifetime distribution, and the probability density function of the generalized linear failure rate distribution can take different shapes. Its hazard function also can be increasing, decreasing and bathtub shaped. The main aim of this paper is to introduce a bivariate generalized linear failure rate distribution, whose marginals are generalized linear failure rate distributions. It is obtained using the same approach as was adopted to obtain the Marshall-Olkin bivariate exponential distribution. Different properties of this new distribution are established. The bivariate generalized linear failure rate distribution has five parameters and the maximum likelihood estimators are obtained using the EM algorithm. A data set is analyzed for illustrative purposes. Finally, some generalizations to the multivariate case are proposed.",2011,1609763,"Ammar M. Sarhan,David C. Hamilton,Bruce Smith,Debasis Kundu",0
1609764,Estimating inter-group interaction radius for point processes with nested spatial structures.,"A statistical procedure is proposed in order to estimate the interaction radius between points of a non-stationary point process when the process can present local aggregated and regular patterns. The model under consideration is a hierarchical process with two levels, points and clusters of points. Points will represent individuals, clusters will represent groups of individuals. Points or clusters do not interact as soon as they are located beyond a given interaction radius, and are assumed to interact if their distance is less than this interaction radius. Interaction radius estimation is performed in the following way. For a given distance, observations are split into several clusters whose in-between distances are larger than this distance. For each cluster, a neighbourhood and an area in which this cluster is randomly located is defined under the assumption that the distance between the cluster and its neighbourhood is larger than the interaction radius. The p-value of a test of this assumption is then computed for each cluster. Modelling the expectation of this p-value as a function of the distance leads to an estimate of the interaction radius by a least-square method. This approach is shown to be robust against non-stationarity. Unlike most classical approaches, this method makes no assumption on the point spatial distribution inside the clusters. Two applications are presented in animal and plant ecology.",2011,1609764,"Joël Chadoeuf,G. Certain,E. Bellier,A. Bar-Hen,P. Couteron,P. Monestiez,V. Bretagnolle",0
1609765,Practical simulation and estimation for Gibbs Delaunay-Voronoi tessellations with geometric hardcore interaction.,"General models of Gibbs Delaunay-Voronoi tessellations, which can be viewed as extensions of Ord's process, are considered. The interaction may occur on each cell of the tessellation and between neighbour cells. The tessellation may also be subjected to a geometric hardcore interaction, forcing the cells not to be too large, too small, or too flat. This setting, natural for applications, introduces some theoretical difficulties since the interaction is not necessarily hereditary. Mathematical results available for studying these models are reviewed and further outcomes are provided. They concern the existence, the simulation and the estimation of such tessellations. Based on these results, tools to handle these objects in practice are presented: how to simulate them, estimate their parameters and validate the fitted model. Some examples of simulated tessellations are studied in detail.",2011,1609765,"D. Dereudre,Frédéric Lavancier",0
1609766,A space-time filter for panel data models containing random effects.,A space-time filter structure is introduced that can be used to accommodate dependence across space and time in the error components of panel data models that contain random effects. This specification provides insights regarding several space-time structures that have been used recently in the panel data literature. Markov Chain Monte Carlo methods are set forth for estimating the model which allow simple treatment of initial period observations as endogenous or exogenous. The performance of the approach is demonstrated using both Monte Carlo experiments and an applied illustration.,2011,1609766,"Olivier Parent,James P. LeSage",0
1609767,Dimensionality reduction when data are density functions.,"Functional Data Analysis deals with samples where a whole function is observed for each individual. A relevant case of FDA is when the observed functions are density functions. Among the particular characteristics of density functions, the most of the fact that they are an example of infinite dimensional compositional data (parts of some whole which only carry relative information) is made. Several dimensionality reduction methods for this particular type of data are compared: functional principal components analysis with or without a previous data transformation, and multidimensional scaling for different inter-density distances, one of them taking into account the compositional nature of density functions. The emphasis is on the steps previous and posterior to the application of a particular dimensionality reduction method: care must be taken in choosing the right density function transformation and/or the appropriate distance between densities before performing dimensionality reduction; subsequently the graphical representation of dimensionality reduction results must take into account that the observed objects are density functions. The different methods are applied to artificial and real data (population pyramids for 223 countries in year 2000). As a global conclusion, the use of multidimensional scaling based on compositional distance is recommended.",2011,1609767,P. Delicado,0
1609768,Bayesian inference for additive mixed quantile regression models.,"Quantile regression problems in practice may require flexible semiparametric forms of the predictor for modeling the dependence of responses on covariates. Furthermore, it is often necessary to add random effects accounting for overdispersion caused by unobserved heterogeneity or for correlation in longitudinal data. We present a unified approach for Bayesian quantile inference on continuous response via Markov chain Monte Carlo (MCMC) simulation and approximate inference using integrated nested Laplace approximations (INLA) in additive mixed models. Different types of covariate are all treated within the same general framework by assigning appropriate Gaussian Markov random field (GMRF) priors with different forms and degrees of smoothness. We applied the approach to extensive simulation studies and a Munich rental dataset, showing that the methods are also computationally efficient in problems with many covariates and large datasets.",2011,1609768,"Yu Ryan Yue,Håvard Rue",0
1609769,A note on mean-field variational approximations in Bayesian probit models.,We correct some conclusions presented by Consonni and Marin (2007) on the performance of mean-field variational approximations to Bayesian inferences in the case of a simple probit model. We show that some of their presentations are misleading and thus their results do not fairly present the performance of such approximations in terms of point estimation under the specified model.,2011,1609769,"Artin Armagan,Russell L. Zaretzki",0
1609770,A fast procedure for calculating importance weights in bootstrap sampling.,"Importance sampling is an efficient strategy for reducing the variance of certain bootstrap estimates. It has found wide applications in bootstrap quantile estimation, proportional hazards regression, bootstrap confidence interval estimation, and other problems. Although estimation of the optimal sampling weights is a special case of convex programming, generic optimization methods are frustratingly slow on problems with large numbers of observations. For instance, interior point and adaptive barrier methods must cope with forming, storing, and inverting the Hessian of the objective function. In this paper, we present an efficient procedure for calculating the optimal importance weights and compare its performance to standard optimization methods on a representative data set. The procedure combines several potent ideas for large-scale optimization.",2011,1609770,"Hua Zhou,Kenneth Lange",0
1609771,Likelihood-based confidence intervals for the risk ratio using double sampling with over-reported binary data.,"In this article we derive likelihood-based confidence intervals for the risk ratio using over-reported two-sample binary data obtained using a double-sampling scheme. The risk ratio is defined as the ratio of two proportion parameters. By maximizing the full likelihood function, we obtain closed-form maximum likelihood estimators for all model parameters. In addition, we derive four confidence intervals: a naive Wald interval, a modified Wald interval, a Fieller-type interval, and an Agresti-Coull interval. All four confidence intervals are illustrated using cervical cancer data. Finally, we conduct simulation studies to assess and compare the coverage probabilities and average lengths of the four interval estimators. We conclude that the modified Wald interval, unlike the other three intervals, produces close-to-nominal confidence intervals under various simulation scenarios examined here and, therefore, is preferred in practice.",2011,1609771,"Dewi Rahardja,Dean M. Young",0
1609772,Generalized additive models and inflated type I error rates of smoother significance tests.,"Generalized additive models (GAMs) have distinct advantages over generalized linear models as they allow investigators to make inferences about associations between outcomes and predictors without placing parametric restrictions on the associations. The variable of interest is often smoothed using a locally weighted scatterplot smoothing (LOESS) and the optimal span (degree of smoothing) can be determined by minimizing the Akaike Information Criterion (AIC). A natural hypothesis when using GAMs is to test whether the smoothing term is necessary or if a simpler model would suffice. The statistic of interest is the difference in deviances between models including and excluding the smoothed term. As approximate chi-square tests of this hypothesis are known to be biased, permutation tests are a reasonable alternative. We compare the type I error rates of the chi-square test and of three permutation test methods using synthetic data generated under the null hypothesis. In each permutation method a distribution of differences in deviances is obtained from 999 permuted datasets and the null hypothesis is rejected if the observed statistic falls in the upper 5% of the distribution. One test is a conditional permutation test using the optimal span size for the observed data; this span size is held constant for all permutations. This test is shown to have an inflated type I error rate. Alternatively, the span size can be fixed a priori such that the span selection technique is not reliant on the observed data. This test is shown to be unbiased; however, the choice of span size is not clear. A third method is an unconditional permutation test where the optimal span size is selected for observed and permuted datasets. This test is unbiased though computationally intensive.",2011,1609772,"Robin L. Young,Janice Weinberg,Verónica Vieira,Al Ozonoff,Thomas F. Webster",0
1609773,Semiparametric stochastic frontier models for clustered data.,"The mixed model approach to semiparametric regression is considered for stochastic frontier models, with focus on clustered data. Standard assumptions about the model component representing the inefficiency effect lead to a closed skew normal distribution for the response. Model parameters are estimated by a generalization of restricted maximum likelihood, and random effects are estimated by an orthodox best linear unbiased prediction procedure. The method is assessed by means of Monte Carlo studies, and illustrated by an empirical application on hospital productivity.",2011,1609773,"Ruggero Bellio,Luca Grassetti",0
1609774,Simulation smoothing for state-space models: A computational efficiency analysis.,"Simulation smoothing involves drawing state variables (or innovations) in discrete time state-space models from their conditional distribution given parameters and observations. Gaussian simulation smoothing is of particular interest, not only for the direct analysis of Gaussian linear models, but also for the indirect analysis of more general models. Several methods for Gaussian simulation smoothing exist, most of which are based on the Kalman filter. Since states in Gaussian linear state-space models are Gaussian Markov random fields, it is also possible to apply the Cholesky Factor Algorithm (CFA) to draw states. This algorithm takes advantage of the band diagonal structure of the Hessian matrix of the log density to make efficient draws. We show how to exploit the special structure of state-space models to draw latent states even more efficiently. We analyse the computational efficiency of Kalman-filter-based methods, the CFA, and our new method using counts of operations and computational experiments. We show that for many important cases, our method is most efficient. Gains are particularly large for cases where the dimension of observed variables is large or where one makes repeated draws of states for the same parameter values. We apply our method to a multivariate Poisson model with time-varying intensities, which we use to analyse financial market transaction count data.",2011,1609774,"William J. McCausland,Shirley Miller,Denis Pelletier",0
1609775,Quadratic approximation on SCAD penalized estimation.,"In this paper, we propose a method of quadratic approximation that unifies various types of smoothly clipped absolute deviation (SCAD) penalized estimations. For convenience, we call it the quadratically approximated SCAD penalized estimation (Q-SCAD). We prove that the proposed Q-SCAD estimator achieves the oracle property and requires only the least angle regression (LARS) algorithm for computation. Numerical studies including simulations and real data analysis confirm that the Q-SCAD estimator performs as efficient as the original SCAD estimator.",2011,1609775,"Sunghoon Kwon,Hosik Choi,Yongdai Kim",0
1609776,Detecting random-effects model misspecification via coarsened data.,Mixed effects models provide a suitable framework for statistical inference in a wide range of applications. The validity of likelihood inference for this class of models usually depends on the assumptions on random effects. We develop diagnostic tools for detecting random-effects model misspecification in a rich class of mixed effects models. These methods are illustrated via simulation and application to soybean growth data.,2011,1609776,Xianzheng Huang,0
1609777,Similarity analysis in Bayesian random partition models.,"This work proposes a method to assess the influence of individual observations in the clustering generated by any process that involves random partitions. We call it Similarity Analysis. It basically consists of decomposing the estimated similarity matrix into an intrinsic and an extrinsic part, coupled with a new approach for representing and interpreting partitions. Individual influence is associated with the particular ordering induced by individual covariates, which in turn provides an interpretation of the underlying clustering mechanism. We present applications in the context of Species Sampling Mixture Models (SSMMs), including Bayesian density estimation and dependent linear regression models.",2011,1609777,"Carlos A. Navarrete,Fernando A. Quintana",0
1609778,The hierarchical-likelihood approach to autoregressive stochastic volatility models.,"Many volatility models used in financial research belong to a class of hierarchical generalized linear models with random effects in the dispersion. Therefore, the hierarchical-likelihood (h-likelihood) approach can be used. However, the dimension of the Hessian matrix is often large, so techniques of sparse matrix computation are useful to speed up the procedure of computing the inverse matrix. Using numerical studies we show that the h-likelihood approach gives better long-term prediction for volatility than the existing MCMC method, while the MCMC method gives better short-term prediction. We show that the h-likelihood approach gives comparable estimations of fixed parameters to those of existing methods.",2011,1609778,"Woojoo Lee,Johan Lim,Youngjo Lee,Joan del Castillo",0
1609779,Inferences on Weibull parameters with conventional type-I censoring.,"In this article we consider the statistical inferences of the unknown parameters of a Weibull distribution when the data are Type-I censored. It is well known that the maximum likelihood estimators do not always exist, and even when they exist, they do not have explicit expressions. We propose a simple fixed point type algorithm to compute the maximum likelihood estimators, when they exist. We also propose approximate maximum likelihood estimators of the unknown parameters, which have explicit forms. We construct the confidence intervals of the unknown parameters using asymptotic distribution and also by using the bootstrapping technique. Bayes estimates and the corresponding highest posterior density credible intervals of the unknown parameters are also obtained under fairly general priors on the unknown parameters. The Bayes estimates cannot be obtained explicitly. We propose to use the Gibbs sampling technique to compute the Bayes estimates and also to construct the highest posterior density credible intervals. Different methods have been compared by Monte Carlo simulations. One real data set has been analyzed for illustrative purposes.",2011,1609779,"Avijit Joarder,Hare Krishna,Debasis Kundu",0
1609780,Hierarchical multilinear models for multiway data.,"Reduced-rank decompositions provide descriptions of the variation among the elements of a matrix or array. In such decompositions, the elements of an array are expressed as products of low-dimensional latent factors. This article presents a model-based version of such a decomposition, extending the scope of reduced-rank methods to accommodate a variety of data types such as longitudinal social networks and continuous multivariate data that are cross-classified by categorical variables. The proposed model-based approach is hierarchical, in that the latent factors corresponding to a given dimension of the array are not a priori independent, but exchangeable. Such a hierarchical approach allows more flexibility in the types of patterns that can be represented.",2011,1609780,Peter D. Hoff,0
1609781,Inference in HIV dynamics models via hierarchical likelihood.,"HIV dynamical models are often based on non-linear systems of ordinary differential equations (ODE), which do not have an analytical solution. Introducing random effects in such models leads to very challenging non-linear mixed-effects models. To avoid the numerical computation of multiple integrals involved in the likelihood, a hierarchical likelihood (h-likelihood) approach, treated in the spirit of a penalized likelihood is proposed. The asymptotic distribution of the maximum h-likelihood estimators (MHLE) for fixed effects is given. The MHLE are slightly biased but the bias can be made negligible by using a parametric bootstrap procedure. An efficient algorithm for maximizing the h-likelihood is proposed. A simulation study, based on a classical HIV dynamical model, confirms the good properties of the MHLE. The method is applied to the analysis of a clinical trial.",2011,1609781,"Daniel Commenges,D. Jolly,J. Drylewicz,Hein Putter,Rodolphe Thiébaut",0
1609782,A new and practical influence measure for subsets of covariance matrix sample principal components with applications to high dimensional datasets.,"Principal Component Analysis (PCA) is an important tool in multivariate analysis, in particular when faced with high dimensional data. There has been much done with regard to sensitivity analysis and the development of influence diagnostics for the eigenvector estimators that define the sample principal components. However, little, if any, has been done in this setting with regard to the sample principal components themselves. In this paper we develop a sensitivity measure for principal components associated with the covariance matrix that is very much related to the influence function (Hampel, 1974). This influence measure is based on the average squared canonical correlation and differs from the existing measures in that it assesses the influence of certain observational types on the sample principal components. We use this measure to derive an influence diagnostic that satisfies two key criteria being (i) it detects influential observations with respect to subsets of sample principal components and (ii) is efficient to calculate even in high dimensions. We use several microarray datasets to show that our measure satisfies both criteria.",2011,1609782,"Luke A. Prendergast,Connie Li Wai Suen",0
1609783,A smoothing principle for the Huber and other location M-estimators.,"A smoothing principle for M-estimators is proposed. The smoothing depends on the sample size so that the resulting smoothed M-estimator coincides with the initial M-estimator when n->~. The smoothing principle is motivated by an analysis of the requirements in the proof of the Cramer-Rao bound. The principle can be applied to every M-estimator. A simulation study is carried out where smoothed Huber, ML-, and Bisquare M-estimators are compared with their non-smoothed counterparts and with Pitman estimators on data generated from several distributions with and without estimated scale. This leads to encouraging results for the smoothed estimators, and particularly the smoothed Huber estimator, as they improve upon the initial M-estimators particularly in the tail areas of the distributions of the estimators. The results are backed up by small sample asymptotics.",2011,1609783,"Frank Hampel,Christian Hennig,Elvezio Ronchetti",0
1609784,Robust weighted kernel logistic regression in imbalanced and rare events data.,"Recent developments in computing and technology, along with the availability of large amounts of raw data, have contributed to the creation of many effective techniques and algorithms in the fields of pattern recognition and machine learning. The main objectives for developing these algorithms include identifying patterns within the available data or making predictions, or both. Great success has been achieved with many classification techniques in real-life applications. With regard to binary data classification in particular, analysis of data containing rare events or disproportionate class distributions poses a great challenge to industry and to the machine learning community. This study examines rare events (REs) with binary dependent variables containing many more non-events (zeros) than events (ones). These variables are difficult to predict and to explain as has been evidenced in the literature. This research combines rare events corrections to Logistic Regression (LR) with truncated Newton methods and applies these techniques to Kernel Logistic Regression (KLR). The resulting model, Rare Event Weighted Kernel Logistic Regression (RE-WKLR), is a combination of weighting, regularization, approximate numerical methods, kernelization, bias correction, and efficient implementation, all of which are critical to enabling RE-WKLR to be an effective and powerful method for predicting rare events. Comparing RE-WKLR to SVM and TR-KLR, using non-linearly separable, small and large binary rare event datasets, we find that RE-WKLR is as fast as TR-KLR and much faster than SVM. In addition, according to the statistical significance test, RE-WKLR is more accurate than both SVM and TR-KLR.",2011,1609784,"Maher Maalouf,Theodore B. Trafalis",0
1609785,A Bayesian approach to model-based clustering for binary panel probit models.,"Considering latent heterogeneity is of special importance in nonlinear models in order to gauge correctly the effect of explanatory variables on the dependent variable. A stratified model-based clustering approach is adapted for modeling latent heterogeneity in binary panel probit models. Within a Bayesian framework an estimation algorithm dealing with the inherent label switching problem is provided. Determination of the number of clusters is based on the marginal likelihood and a cross-validation approach. A simulation study is conducted to assess the ability of both approaches to determine on the correct number of clusters indicating high accuracy for the marginal likelihood criterion, with the cross-validation approach performing similarly well in most circumstances. Different concepts of marginal effects incorporating latent heterogeneity at different degrees arise within the considered model setup and are directly at hand within Bayesian estimation via MCMC methodology. An empirical illustration of the methodology developed indicates that consideration of latent heterogeneity via latent clusters provides the preferred model specification over a pooled and a random coefficient specification.",2011,1609785,"Christian Aßmann,Jens Boysen-Hogrefe",0
1609786,2D wavelet-based spectra with applications.,"A wavelet-based spectral method for estimating the (directional) Hurst parameter in isotropic and anisotropic non-stationary fractional Gaussian fields is proposed. The method can be applied to self-similar images and, in general, to d-dimensional data which scale. In the application part, the problems of denoising 2D fractional Brownian fields and classification of digital mammograms to benign and malignant are considered. In the first application, a Bayesian inference calibrated by information from the wavelet-spectral domain is used to separate the signal from the noise. In the second application, digital mammograms are classified into benign and malignant based on the directional Hurst exponents which prove to be discriminatory summaries.",2011,1609786,"Orietta Nicolis,Pepa Ramírez-Cobo,Brani Vidakovic",0
1609787,Bayesian nonlinear regression models with scale mixtures of skew-normal distributions: Estimation and case influence diagnostics.,"The purpose of this paper is to develop a Bayesian analysis for nonlinear regression models under scale mixtures of skew-normal distributions. This novel class of models provides a useful generalization of the symmetrical nonlinear regression models since the error distributions cover both skewness and heavy-tailed distributions such as the skew-t, skew-slash and the skew-contaminated normal distributions. The main advantage of these class of distributions is that they have a nice hierarchical representation that allows the implementation of Markov chain Monte Carlo (MCMC) methods to simulate samples from the joint posterior distribution. In order to examine the robust aspects of this flexible class, against outlying and influential observations, we present a Bayesian case deletion influence diagnostics based on the Kullback-Leibler divergence. Further, some discussions on the model selection criteria are given. The newly developed procedures are illustrated considering two simulations study, and a real data previously analyzed under normal and skew-normal nonlinear regression models.",2011,1609787,"Vicente G. Cancho,Dipak K. Dey,Victor H. Lachos,Marinho G. Andrade",0
1609788,Using capture-recapture data and hybrid Monte Carlo sampling to estimate an animal population affected by an environmental catastrophe.,"We propose a dynamic model for the evolution of an open animal population that is subject to an environmental catastrophe. The model incorporates a capture-recapture experiment often conducted for studying wildlife population, and enables inferences on the population size and possible effect of the catastrophe. A Bayesian approach is used to model unobserved quantities in the problem as latent variables and Markov chain Monte Carlo (MCMC) is used for posterior computation. Because the particular interrelationship between observed and latent variables negates the feasibility of standard MCMC methods, we propose a hybrid Monte Carlo approach that integrates a Gibbs sampler with the strategies of sequential importance sampling (SIS) and acceptance-rejection (AR) sampling for model estimation. We develop results on how to construct effective proposal densities for the SIS scheme. The approach is illustrated through a simulation study, and is applied to data from a mountain pygmy possum (Burramys Parvus) population that was affected by a bushfire.",2011,1609788,"Guoqi Qian,Ning Li,Richard Huggins",0
1609789,On testing the equivalence of treatments using the measure of range.,"A studentized range test is proposed to test the hypothesis of average equivalence of treatments in terms of the distance between means against the alternative hypothesis of inequivalence. A least favorable configuration (LFC) of means to guarantee the maximum level at a null hypothesis and a LFC of means to guarantee the minimum power at an alternative hypothesis are obtained. The level and power of the test are fully independent of the unknown means and variances. For a given level and a given power, the critical value and the required sample size for an experiment can be simultaneously determined, and the tables of critical values and sample sizes are provided for practitioners. A real numerical example to demonstrate the use of the test procedure is provided. In situations where the common population variance is unknown and the equivalence is the actual distance between means without standardization, a two-stage sampling procedure can be employed to find these solutions. It proves to be a quite feasible solution for practitioners.",2011,1609789,"Hubert J. Chen,Miin-Jye Wen,Chia-Jui Chuang",0
1609790,Computational issues with fitting joint location/dispersion models in unreplicated 2 factorials.,Maximum likelihood estimation for a joint location/dispersion model has been found occasionally to experience convergence problems when applied to experiments of the 2^k factorial series. We explore these problems and identify models for which the likelihood diverges or is multimodal. We derive the conditions under which this occurs and provide simple ways to check for problems both before and during computation.,2011,1609790,"Thomas M. Loughin,Jorge E. Rodríguez",0
1609791,Understanding and comparisons of different sampling approaches for the Fourier Amplitudes Sensitivity Test (FAST).,"Fourier Amplitude Sensitivity Test (FAST) is one of the most popular uncertainty and sensitivity analysis techniques. It uses a periodic sampling approach and a Fourier transformation to decompose the variance of a model output into partial variances contributed by different model parameters. Until now, the FAST analysis is mainly confined to the estimation of partial variances contributed by the main effects of model parameters, but does not allow for those contributed by specific interactions among parameters. In this paper, we theoretically show that FAST analysis can be used to estimate partial variances contributed by both main effects and interaction effects of model parameters using different sampling approaches (i.e., traditional search-curve based sampling, simple random sampling and random balance design sampling). We also analytically calculate the potential errors and biases in the estimation of partial variances. Hypothesis tests are constructed to reduce the effect of sampling errors on the estimation of partial variances. Our results show that compared to simple random sampling and random balance design sampling, sensitivity indices (ratios of partial variances to variance of a specific model output) estimated by search-curve based sampling generally have higher precision but larger underestimations. Compared to simple random sampling, random balance design sampling generally provides higher estimation precision for partial variances contributed by the main effects of parameters. The theoretical derivation of partial variances contributed by higher-order interactions and the calculation of their corresponding estimation errors in different sampling schemes can help us better understand the FAST method and provide a fundamental basis for FAST applications and further improvements.",2011,1609791,"Chonggang Xu,George Z. Gertner",0
1609793,Nonlinear methods for inverse statistical problems.,"In the uncertainty treatment framework considered, the intrinsic variability of the inputs of a physical simulation model is modelled by a multivariate probability distribution. The objective is to identify this probability distribution-the dispersion of which is independent of the sample size since intrinsic variability is at stake-based on observation of some model outputs. Moreover, in order to limit the number of (usually burdensome) physical model runs inside the inversion algorithm to a reasonable level, a nonlinear approximation methodology making use of Kriging and a stochastic EM algorithm is presented. It is compared with iterated linear approximation on the basis of numerical experiments on simulated data sets coming from a simplified but realistic modelling of a dyke overflow. Situations where this nonlinear approach is to be preferred to linearisation are highlighted.",2011,1609793,"Pierre Barbillon,Gilles Celeux,Agnès Grimaud,Yannick Lefebvre,Etienne de Rocquigny",0
1609794,Acceleration of the alternating least squares algorithm for principal components analysis.,"Principal components analysis (PCA) is a popular descriptive multivariate method for handling quantitative data and it can be extended to deal with qualitative data and mixed measurement level data. The existing algorithms for extended PCA are PRINCIPALS of Young et al. (1978) and PRINCALS of Gifi (1989) in which the alternating least squares algorithm is utilized. These algorithms based on the least squares estimation may require many iterations in their application to very large data sets and variable selection problems and may take a long time to converge. In this paper, we derive a new iterative algorithm for accelerating the convergence of PRINCIPALS and PRINCALS by using the vector @e algorithm of Wynn (1962). The proposed acceleration algorithm speeds up the convergence of the sequence of the parameter estimates obtained from PRINCIPALS or PRINCALS. Numerical experiments illustrate the potential of the proposed acceleration algorithm.",2011,1609794,"Masahiro Kuroda,Yuichi Mori,Masaya Iizuka,Michio Sakakihara",0
1609795,Mapping electron density in the ionosphere: A principal component MCMC algorithm.,"The outer layers of the Earth's atmosphere are known as the ionosphere, a plasma of free electrons and positively charged atomic ions. The electron density of the ionosphere varies considerably with time of day, season, geographical location and the sun's activity. Maps of electron density are required because local changes in this density can produce inaccuracies in the Navy Navigation Satellite System (NNSS) and Global Positioning System (GPS). Satellite to ground based receiver measurements produce tomographic information about the density in the form of path integrated snapshots of the total electron content which must be inverted to generate electron density maps. A Bayesian approach is proposed for solving the inversion problem using spatial priors in a parsimonious model for the variation of electron density with height. The Bayesian approach to modelling and inference provides estimates of electron density along with a measure of uncertainty for these estimates, leading to credible intervals for all quantities of interest. The standard parameterisation does not lend itself well to standard Metropolis-Hastings algorithms. A much more efficient form of Markov chain Monte Carlo sampler is developed using a transformation of variables based on a principal components analysis of initial output.",2011,1609795,"Eman Khorsheed,Merrilee Hurn,Christopher Jennison",0
1609796,A latent class selection model for nonignorably missing data.,"When we have data with missing values, the assumption that data are missing at random is very convenient. It is, however, sometimes questionable because some of the missing values could be strongly related to the underlying true values. We introduce methods for nonignorable multivariate missing data, which assume that missingness is related to the variables in question, and to the additional covariates, through a latent variable measured by the missingness indicators. The methodology developed here is useful for investigating the sensitivity of one's estimates to untestable assumptions about the missing-data mechanism. A simulation study and data analysis are conducted to evaluate the performance of the proposed method and to compare to that of MAR-based alternatives.",2011,1609796,"Hyekyung Jung,Joseph L. Schafer,Byungtae Seo",0
1609797,Efficiently sampling nested Archimedean copulas.,"Efficient sampling algorithms for both Archimedean and nested Archimedean copulas are presented. First, efficient sampling algorithms for the nested Archimedean families of Ali-Mikhail-Haq, Frank, and Joe are introduced. Second, a general strategy how to build a nested Archimedean copula from a given Archimedean generator is presented. Sampling this copula involves sampling an exponentially tilted stable distribution. A fast rejection algorithm is developed for the more general class of tilted Archimedean generators. It is proven that this algorithm reduces the complexity of the standard rejection algorithm to logarithmic complexity. As an application it is shown that the fast rejection algorithm outperforms existing algorithms for sampling exponentially tilted stable distributions involved, e.g., in nested Clayton copulas. Third, with the additional help of randomization of generator parameters, explicit sampling algorithms for several nested Archimedean copulas based on different Archimedean families are found. Additional results include approximations and some dependence properties, such as Kendall's tau and tail dependence parameters. The presented ideas may also apply in the more general context of sampling distributions given by their Laplace-Stieltjes transforms.",2011,1609797,Marius Hofert,0
1609798,Cutpoint selection for discretizing a continuous covariate for generalized estimating equations.,"We consider the problem of dichotomizing a continuous covariate when performing a regression analysis based on a generalized estimation approach. The problem involves estimation of the cutpoint for the covariate and testing the hypothesis that the binary covariate constructed from the continuous covariate has a significant impact on the outcome. Due to the multiple testing used to find the optimal cutpoint, we need to make an adjustment to the usual significance test to preserve the type-I error rates. We illustrate the techniques on one data set of patients given unrelated hematopoietic stem cell transplantation. Here the question is whether the CD34 cell dose given to patient affects the outcome of the transplant and what is the smallest cell dose which is needed for good outcomes.",2011,1609798,"Gisela Tunes-da-Silva,John P. Klein",0
1609799,An extension of an over-dispersion test for count data.,"While over-dispersion in capture-recapture studies is well known to lead to poor estimation of population size, current diagnostic tools to detect the presence of heterogeneity have not been specifically developed for capture-recapture studies. To address this, a simple and efficient method of testing for over-dispersion in zero-truncated count data is developed and evaluated. The proposed method generalizes an over-dispersion test previously suggested for un-truncated count data and may also be used for testing residual over-dispersion in zero-inflation data. Simulations suggest that the asymptotic distribution of the test statistic is standard normal and that this approximation is also reasonable for small sample sizes. The method is also shown to be more efficient than an existing test for over-dispersion adapted for the capture-recapture setting. Studies with zero-truncated and zero-inflated count data are used to illustrate the test procedures.",2011,1609799,"M. Fazil Baksh,Dankmar Böhning,Rattana Lerdsuwansri",0
1609800,Stochastic volatility models with leverage and heavy-tailed distributions: A Bayesian approach using scale mixtures.,"This paper studies a heavy-tailed stochastic volatility (SV) model with leverage effect, where a bivariate Student-t distribution is used to model the error innovations of the return and volatility equations. Choy et al. (2008) studied this model by expressing the bivariate Student-t distribution as a scale mixture of bivariate normal distributions. We propose an alternative formulation by first deriving a conditional Student-t distribution for the return and a marginal Student-t distribution for the log-volatility and then express these two Student-t distributions as a scale mixture of normal (SMN) distributions. Our approach separates the sources of outliers and allows for distinguishing between outliers generated by the return process or by the volatility process, and hence is an improvement over the approach of Choy et al. (2008). In addition, it allows an efficient model implementation using the WinBUGS software. A simulation study is conducted to assess the performance of the proposed approach and its comparison with the approach by Choy et al. (2008). In the empirical study, daily exchange rate returns of the Australian dollar to various currencies and daily stock market index returns of various international stock markets are analysed. Model comparison relies on the Deviance Information Criterion and convergence diagnostic is monitored by Geweke's convergence test.",2011,1609800,"Joanna J. J. Wang,Jennifer S. K. Chan,S. T. Boris Choy",0
1609801,A comparison of block and semi-parametric bootstrap methods for variance estimation in spatial statistics.,"Efron (1979) introduced the bootstrap method for independent data but it cannot be easily applied to spatial data because of their dependency. For spatial data that are correlated in terms of their locations in the underlying space the moving block bootstrap method is usually used to estimate the precision measures of the estimators. The precision of the moving block bootstrap estimators is related to the block size which is difficult to select. In the moving block bootstrap method also the variance estimator is underestimated. In this paper, first the semi-parametric bootstrap is used to estimate the precision measures of estimators in spatial data analysis. In the semi-parametric bootstrap method, we use the estimation of the spatial correlation structure. Then, we compare the semi-parametric bootstrap with a moving block bootstrap for variance estimation of estimators in a simulation study. Finally, we use the semi-parametric bootstrap to analyze the coal-ash data.",2011,1609801,"N. Iranpanah,M. Mohammadzadeh,C. C. Taylor",0
1609802,Interval estimation for response adaptive clinical trials.,In this paper we examine a new method for constructing confidence intervals for the difference of success probabilities to analyze dependent data from response adaptive designs with binary responses. Specifically we investigate the feasibility of the Jeffreys-Perks procedure for interval estimation. Simulation results are derived to demonstrate the performance of the Jeffreys-Perks procedure compared with the profile likelihood method. It is found that both asymptotic methods perform well for small sample sizes despite being approximate procedures.,2011,1609802,"David Tolusso,Xikui Wang",0
1609803,A weighted quantile regression for randomly truncated data.,"Quantile regression offers great flexibility in assessing covariate effects on the response. In this article, based on the weights proposed by He and Yang (2003), we develop a new quantile regression approach for left truncated data. Our method leads to a simple algorithm that can be conveniently implemented with R software. It is shown that the proposed estimator is strongly consistent and asymptotically normal under appropriate conditions. We evaluate the finite sample performance of the proposed estimators through extensive simulation studies.",2011,1609803,Weihua Zhou,0
1609804,Some exact tests for manifest properties of latent trait models.,"Item response theory is one of the modern test theories with applications in educational and psychological testing. Recent developments made it possible to characterize some desired properties in terms of a collection of manifest ones, so that hypothesis tests on these traits can, in principle, be performed. But the existing test methodology is based on asymptotic approximation, which is impractical in most applications since the required sample sizes are often unrealistically huge. To overcome this problem, a class of tests is proposed for making exact statistical inference about four manifest properties: covariances given the sum are non-positive (CSN), manifest monotonicity (MM), conditional association (CA), and vanishing conditional dependence (VCD). One major advantage is that these exact tests do not require large sample sizes. As a result, tests for CSN and MM can be routinely performed in empirical studies. For testing CA and VCD, the exact methods are still impractical in most applications, due to the unusually large number of parameters to be tested. However, exact methods are still derived for them as an exploration toward practicality. Some numerical examples with applications of the exact tests for CSN and MM are provided.",2011,1609804,"Jan G. De Gooijer,Ao Yuan",0
1609805,Bayesian inference for a skew-normal IRT model under the centred parameterization.,"Item response theory (IRT) comprises a set of statistical models which are useful in many fields, especially when there is interest in studying latent variables. These latent variables are directly considered in the Item Response Models (IRM) and they are usually called latent traits. A usual assumption for parameter estimation of the IRM, considering one group of examinees, is to assume that the latent traits are random variables which follow a standard normal distribution. However, many works suggest that this assumption does not apply in many cases. Furthermore, when this assumption does not hold, the parameter estimates tend to be biased and misleading inference can be obtained. Therefore, it is important to model the distribution of the latent traits properly. In this paper we present an alternative latent traits modeling based on the so-called skew-normal distribution; see Genton (2004). We used the centred parameterization, which was proposed by Azzalini (1985). This approach ensures the model identifiability as pointed out by Azevedo et al. (2009b). Also, a Metropolis-Hastings within Gibbs sampling (MHWGS) algorithm was built for parameter estimation by using an augmented data approach. A simulation study was performed in order to assess the parameter recovery in the proposed model and the estimation method, and the effect of the asymmetry level of the latent traits distribution on the parameter estimation. Also, a comparison of our approach with other estimation methods (which consider the assumption of symmetric normality for the latent traits distribution) was considered. The results indicated that our proposed algorithm recovers properly all parameters. Specifically, the greater the asymmetry level, the better the performance of our approach compared with other approaches, mainly in the presence of small sample sizes (number of examinees). Furthermore, we analyzed a real data set which presents indication of asymmetry concerning the latent traits distribution. The results obtained by using our approach confirmed the presence of strong negative asymmetry of the latent traits distribution.",2011,1609805,"Caio L. N. Azevedo,Heleno Bolfarine,Dalton F. Andrade",0
1609806,On the index of dissimilarity for lack of fit in loglinear and log-multiplicative models.,"The index of dissimilarity, often denoted by Delta, is commonly used, especially in social science and with large datasets, to describe the lack of fit of models for categorical data. The definition and sampling properties of the index for general loglinear and log-multiplicative models are investigated. It is argued that in some applications a standardized version of the index is appropriate for interpretation. A simple, approximate variance formula is derived for the index, whether standardized or not. A simple bias reduction formula is also given. The accuracy of these formulae and of confidence intervals based upon them is investigated in a simulation study based on large-scale social mobility data.",2011,1609806,"Jouni Kuha,David Firth",0
1609807,Estimation from aggregate data.,"A statistical methodology to handle aggregate data is proposed. Aggregate data arise in many fields such as medical science, ecology, social science, reliability, etc. They can be described as follows: individuals are moving progressively along a finite set of states and observations are made in a time window split into several intervals. At each observation time, the only available information is the number of individuals in each state and the history of each item viewed as a stochastic process is thus lost. The time spent in a given state is unknown. Using a data completion technique, an estimation of the hazard rate in each state based on sojourn times is obtained and an estimation of the survival function is deduced. These methods are studied through simulations and applied to a data set. The simulation study shows that the algorithms involved in the methods converge and are robust.",2011,1609807,"E. Gouno,L. Courtrai,M. Fredette",0
1609808,A generalized false discovery rate in microarray studies.,"The problem of identifying differentially expressed genes is considered in a microarray experiment. This motivates us to involve an appropriate multiple testing setup to high dimensional and low sample size testing problems in highly nonstandard setups. Family-wise error rate (FWER) is too conservative to control the type I error, whereas a less conservative false discovery rate has received considerable attention in a wide variety of research areas such as genomics and large biological systems. Recently, a less conservative method than FDR, the k-FDR, which generalizes the FDR has been proposed by Sarkar (2007). Most of the current FDR procedures assume restrictive dependence structures, resulting in being less reliable. The purpose of this paper is to address these very large multiplicity problems by adopting a proposed k-FDR controlling procedure under suitable dependence structures and based on a Poisson distributional approximation in a unified framework. We compare the performance of the proposed k-FDR procedure with that of other FDR controlling procedures, with an illustration of the leukemia microarray study of Golub et al. (1999) and simulated data. For power consideration, different FDR procedures are assessed using false negative rate (FNR). An unbiased property is appraised by FDR@?@a and a higher value of 1-(FDR+FNR). The proposed k-FDR procedure is characterized by greater power without much elevation of k-FDR.",2011,1609808,"Moonsu Kang,Heuiju Chun",0
1609809,Comparative study of ROC regression techniques - Applications for the computer-aided diagnostic system in breast cancer detection.,"The receiver operating characteristic (ROC) curve is the most widely used measure for statistically evaluating the discriminatory capacity of continuous biomarkers. It is well known that, in certain circumstances, the markers' discriminatory capacity can be affected by factors, and several ROC regression methodologies have been proposed to incorporate covariates in the ROC framework. An in-depth simulation study of different ROC regression models and their application in the emerging field of automatic detection of tumour masses is presented. In the simulation study different scenarios were considered and the models were compared to each other on the basis of the mean squared error criterion. The application of the reviewed ROC regression techniques in evaluating computer-aided diagnostic (CAD) schemes can become a major factor in the development of such systems in Radiology.",2011,1609809,"María Xosé Rodríguez-Álvarez,Pablo G. Tahoces,Carmen Cadarso-Suárez,María José Lado",0
1609810,The joint model of the logistic model and linear random effect model - An application to predict orthostatic hypertension for subacute stroke patients.,"Stroke is a common acute neurologic and disabling disease. Orthostatic hypertension (OH) is one of the catastrophic cardiovascular conditions. If a stroke patient has OH, he/she has higher chance to fall or syncope during the following courses of treatment. This can result in possible bone fracture and the burden of medical cost therefore increases. How to early diagnose OH is clinically important. However, there is no obvious time-saving method for clinical evaluation except to check the postural blood pressure. This paper uses clinical data to identify potential clinical factors that are associated with OH. The data include repeatedly observed blood pressure, and the patient's basic characteristics and clinical symptoms. A traditional logistic regression is not appropriate for such data. The paper modifies the two-stage model proposed by Tsiatis et al. (1995) and the joint model proposed by Wulfsohn and Tsiatis (1997) to take into account of a sequence of repeated measures to predict OH. The large sample properties of estimators of modified models are derived. Monte Carlo simulations are performed to evaluate the accuracy of these estimators. A case study is presented.",2011,1609810,"Yi-Ting Hwang,Hao-Yun Tsai,Yeu-Jhy Chang,Hsun-Chih Kuo,Chun-Chao Wang",0
1609811,Approximate forward-backward algorithm for a switching linear Gaussian model.,"A hidden Markov model with two hidden layers is considered. The bottom layer is a Markov chain and given this the variables in the second hidden layer are assumed conditionally independent and Gaussian distributed. The observation process is Gaussian with mean values that are linear functions of the second hidden layer. The forward-backward algorithm is not directly feasible for this model as the recursions result in a mixture of Gaussian densities where the number of terms grows exponentially with the length of the Markov chain. By dropping the less important Gaussian terms an approximate forward-backward algorithm is defined. Thereby one gets a computationally feasible algorithm that generates samples from an approximation to the conditional distribution of the unobserved layers given the data. The approximate algorithm is also used as a proposal distribution in a Metropolis-Hastings setting, and this gives high acceptance rates and good convergence and mixing properties. The model considered is related to what is known as switching linear dynamical systems. The proposed algorithm can in principle also be used for these models and the potential use of the algorithm is therefore large. In simulation examples the algorithm is used for the problem of seismic inversion. The simulations demonstrate the effectiveness and quality of the proposed approximate algorithm.",2011,1609811,"Hugo Hammer,Håkon Tjelmeland",0
1609812,Nonlinear tracking in a diffusion process with a Bayesian filter and the finite element method.,"A new approach to nonlinear state estimation and object tracking from indirect observations of a continuous time process is examined. Stochastic differential equations (SDEs) are employed to model the dynamics of the unobservable state. Tracking problems in the plane subject to boundaries on the state-space do not in general provide analytical solutions. A widely used numerical approach is the sequential Monte Carlo (SMC) method which relies on stochastic simulations to approximate state densities. For off-line analysis, however, accurate smoothed state density and parameter estimation can become complicated using SMC because Monte Carlo randomness is introduced. The finite element (FE) method solves the Kolmogorov equations of the SDE numerically on a triangular unstructured mesh for which boundary conditions to the state-space are simple to incorporate. The FE approach to nonlinear state estimation is suited for off-line data analysis because the computed smoothed state densities, maximum a posteriori parameter estimates and state sequence are deterministic conditional on the finite element mesh and the observations. The proposed method is conceptually similar to existing point-mass filtering methods, but is computationally more advanced and generally applicable. The performance of the FE estimators in relation to SMC and to the resolution of the spatial discretization is examined empirically through simulation. A real-data case study involving fish tracking is also analysed.",2011,1609812,"M. W. Pedersen,U. H. Thygesen,H. Madsen",0
1609813,Model selection for zero-inflated regression with missing covariates.,"Count data are widely existed in the fields of medical trials, public health, surveys and environmental studies. In analyzing count data, it is important to find out whether the zero-inflation exists or not and how to select the most suitable model. However, the classic AIC criterion for model selection is invalid when the observations are missing. In this paper, we develop a new model selection criterion in line with AIC for the zero-inflated regression models with missing covariates. This method is a modified version of Monte Carlo EM algorithm which is based on the data augmentation scheme. One of the main attractions of this new method is that it is applicable for comparison of candidate models regardless of whether there are missing data or not. What is more, it is very simple to compute as it is just a by-product of Monte Carlo EM algorithm when the estimations of parameters are obtained. A simulation study and a real example are used to illustrate the proposed methodologies.",2011,1609813,"Xue-Dong Chen,Ying-Zi Fu",0
1609814,Error rates for multivariate outlier detection.,"Multivariate outlier identification requires the choice of reliable cut-off points for the robust distances that measure the discrepancy from the fit provided by high-breakdown estimators of location and scatter. Multiplicity issues affect the identification of the appropriate cut-off points. It is described how a careful choice of the error rate which is controlled during the outlier detection process can yield a good compromise between high power and low swamping, when alternatives to the Family Wise Error Rate are considered. Multivariate outlier detection rules based on the False Discovery Rate and the False Discovery Exceedance criteria are proposed. The properties of these rules are evaluated through simulation. The rules are then applied to real data examples. The conclusion is that the proposed approach provides a sensible strategy in many situations of practical interest.",2011,1609814,"Andrea Cerioli,Alessio Farcomeni",0
1609815,Generating generalized inverse Gaussian random variates by fast inversion.,"The inversion method for generating non-uniformly distributed random variates is a crucial part in many applications of Monte Carlo techniques, e.g., when low discrepancy sequences or copula based models are used. Unfortunately, closed form expressions of quantile functions of important distributions are often not available. The (generalized) inverse Gaussian distribution is a prominent example. It is shown that algorithms that are based on polynomial approximation are well suited for this distribution. Their precision is close to machine precision and they are much faster than root finding methods like the bisection method that has been recently proposed.",2011,1609815,"Josef Leydold,Wolfgang Hörmann",0
1609816,Model-based classification via mixtures of multivariate t-distributions.,"A novel model-based classification technique is introduced based on mixtures of multivariate t-distributions. A family of four mixture models is defined by constraining, or not, the covariance matrices and the degrees of freedom to be equal across mixture components. Parameters for each of the resulting four models are estimated using a multicycle expectation-conditional maximization algorithm, where convergence is determined using a criterion based on the Aitken acceleration. A straightforward, but very effective, technique for the initialization of the unknown component memberships is introduced and compared with a popular, more sophisticated, initialization procedure. This novel four-member family is applied to real and simulated data, where it gives good classification performance, even when compared with more established techniques.",2011,1609816,"Jeffrey L. Andrews,Paul D. McNicholas,Sanjeena Subedi",0
1609817,Modelling using an extended Yule distribution.,A biparametric discrete distribution that extends the Yule distribution is presented. It belongs to the family of distributions generated by the Gaussian hypergeometric function and it can be expressed as a generalized beta mixture of a geometric distribution. The introduction of a new parameter makes the model very suitable to fit the empiric distribution tails and the effect of infinite variance is not possible. Several examples show more accurate fits when the extended distribution is used and the results are compared with other biparametric extensions of the Yule distribution.,2011,1609817,"A. M. Martínez-Rodríguez,A. J. Sáez-Castillo,A. Conde-Sánchez",0
1609818,Cost-efficiency considerations in the choice of a microarray platform for time course experimental designs.,"Customarily, the choice between one- or two-colour microarray platforms is based on their respective practical and technical merits, contingent on objectives and constraints of the study at stake. Statistical efficiency, if accounted for, plays a secondary role. A cost-efficiency comparison of the one- and two-colour designs for a 2x4 time course experiment was conducted. It is shown that differences in costs between the platforms' designs, once adjusted for statistical efficiency, are not always negligible. The extent of these differences is largely influenced by subjects and array prices as well as by biological and error variances in their relative magnitude. Circumstances are described, in which cost-efficiency considerations will have an added value in motivating the selection of a microarray platform at the design stage.",2011,1609818,"Valéria Lima Passos,Frans E. S. Tan,Martijn P. F. Berger",0
1609819,Grid based variational approximations.,"Variational methods for approximate Bayesian inference provide fast, flexible, deterministic alternatives to Monte Carlo methods. Unfortunately, unlike Monte Carlo methods, variational approximations cannot, in general, be made to be arbitrarily accurate. This paper develops grid-based variational approximations which endeavor to approximate marginal posterior densities in a spirit similar to the Integrated Nested Laplace Approximation (INLA) of Rue et al. (2009) but which may be applied in situations where INLA cannot be used. The method can greatly increase the accuracy of a base variational approximation, although not in general to arbitrary accuracy. The methodology developed is at least reasonably accurate on all of the examples considered in the paper.",2011,1609819,John T. Ormerod,0
1609820,Partially varying coefficient single index proportional hazards regression models.,"In this paper, the partially varying coefficient single index proportional hazards regression models are discussed. All unknown functions are fitted by polynomial B splines. The index parameters and B-spline coefficients are estimated by the partial likelihood method and a two-step Newton-Raphson algorithm. Consistency and asymptotic normality of the estimators of all the parameters are derived. Through a simulation study and the VA data example, we illustrate that the proposed estimation procedure is accurate, rapid and stable.",2011,1609820,"Jianbo Li,Riquan Zhang",0
1609821,Smooth semiparametric and nonparametric Bayesian estimation of bivariate densities from bivariate histogram data.,"Penalized B-splines combined with the composite link model are used to estimate a bivariate density from a histogram with wide bins. The goals are multiple: they include the visualization of the dependence between the two variates, but also the estimation of derived quantities like Kendall's tau, conditional moments and quantiles. Two strategies are proposed: the first one is semiparametric with flexible margins modeled using B-splines and a parametric copula for the dependence structure; the second one is nonparametric and is based on Kronecker products of the marginal B-spline bases. Frequentist and Bayesian estimations are described. A large simulation study quantifies the performances of the two methods under different dependence structures and for varying strengths of dependence, sample sizes and amounts of grouping. It suggests that Schwarz's BIC is a good tool for classifying the competing models. The density estimates are used to evaluate conditional quantiles in two applications in social and in medical sciences.",2011,1609821,Philippe Lambert,0
1609822,A semi-parametric generalization of the Cox proportional hazards regression model: Inference and applications.,"The assumption of proportional hazards (PH) fundamental to the Cox PH model sometimes may not hold in practice. In this paper, we propose a generalization of the Cox PH model in terms of the cumulative hazard function taking a form similar to the Cox PH model, with the extension that the baseline cumulative hazard function is raised to a power function. Our model allows for interaction between covariates and the baseline hazard and it also includes, for the two sample problem, the case of two Weibull distributions and two extreme value distributions differing in both scale and shape parameters. The partial likelihood approach can not be applied here to estimate the model parameters. We use the full likelihood approach via a cubic B-spline approximation for the baseline hazard to estimate the model parameters. A semi-automatic procedure for knot selection based on Akaike's information criterion is developed. We illustrate the applicability of our approach using real-life data.",2011,1609822,"Karthik Devarajan,Nader Ebrahimi",0
1609823,Hidden Markov models with arbitrary state dwell-time distributions.,"A hidden Markov model (HMM) with a special structure that captures the 'semi'-property of hidden semi-Markov models (HSMMs) is considered. The proposed model allows arbitrary dwell-time distributions in the states of the Markov chain. For dwell-time distributions with finite support the HMM formulation is exact while for those that have infinite support, e.g. the Poisson, the distribution can be approximated with arbitrary accuracy. A benefit of using the HMM formulation is that it is easy to incorporate covariates, trend and seasonal variation particularly in the hidden component of the model. In addition, the formulae and methods for forecasting, state prediction, decoding and model checking that exist for ordinary HMMs are applicable to the proposed class of models. An HMM with explicitly modeled dwell-time distributions involving seasonality is used to model daily rainfall occurrence for sites in Bulgaria.",2011,1609823,"R. Langrock,W. Zucchini",0
1609824,On power and sample size computation for multiple testing procedures.,"Power and sample size determination has been a challenging issue for multiple testing procedures, especially stepwise procedures, mainly because (1) there are several power definitions, (2) power calculation usually requires multivariate integration involving order statistics, and (3) expansion of these power expressions in terms of ordinary statistics, instead of order statistics, is generally a difficult task. Traditionally power and sample size calculations rely on either simulations or some recursive algorithm; neither is straightforward and computationally economic. In this paper we develop explicit formulas for minimal power and r-power of stepwise procedures as well as complete power of single-step procedures for exchangeable and non-exchangeable bivariate and trivariate test statistics. With the explicit power expressions, we were able to directly calculate the desired power, given sample size and correlation. Numerical examples are presented to illustrate the relationship among power, sample size and correlation.",2011,1609824,"Jie Chen,Jianfeng Luo,Kenneth Liu,Devan V. Mehrotra",0
1609825,The Poisson-exponential lifetime distribution.,"In this paper we proposed a new two-parameters lifetime distribution with increasing failure rate. The new distribution arises on a latent complementary risk problem base. The properties of the proposed distribution are discussed, including a formal proof of its probability density function and explicit algebraic formulae for its reliability and failure rate functions, quantiles and moments, including the mean and variance. A simple EM-type algorithm for iteratively computing maximum likelihood estimates is presented. The Fisher information matrix is derived analytically in order to obtaining the asymptotic covariance matrix. The methodology is illustrated on a real data set.",2011,1609825,"Vicente G. Cancho,Francisco Louzada-Neto,Gladys D. C. Barriga",0
1609826,Floating prioritized subset analysis: A powerful method to detect differentially expressed genes.,"Controlling the false discovery rate (FDR) is a powerful approach to deal with a large number of hypothesis tests, such as in gene expression data analyses and genome-wide association studies. To further boost power, here we propose a floating prioritized subset analysis (floating PSA) that can more effectively use prior knowledge and detect more genes that are differentially expressed. Genes are first allocated into two subsets: a prioritized subset and a non-prioritized subset, according to investigators' prior biological knowledge. We allow the FDRs of the two subsets to vary freely (to float) but aim to control the overall FDR at a desired level. An algorithm for the floating PSA is developed to detect the largest number of true positives. Theoretical justifications of the algorithm are given, and computer simulation studies show that the method has good statistical properties. We apply this method to detect genes that are differentially expressed between acute lymphoblastic leukemia and acute myeloid leukemia patients. The result shows that our floating PSA identifies 32 more genes (permutation-based FDR=0.0427) than the conventional (fixed) FDR control. Another example is a colon cancer study, and our floating PSA identifies 43 more genes (permutation-based FDR=0.0502). The floating PSA method is to be recommended for the detection of differentially expressed genes, in light of its power, robustness, and ease of implementation.",2011,1609826,"Wan-Yu Lin,Wen-Chung Lee",0
1609827,Exact and approximate algorithms for variable selection in linear discriminant analysis.,"Variable selection is a venerable problem in multivariate statistics. In the context of discriminant analysis, the goal is to select a subset of variables that accomplishes one of two objectives: (1) the provision of a parsimonious, yet descriptive, representation of group structure, or (2) the ability to correctly allocate new cases to groups. We present an exact (branch-and-bound) algorithm for variable selection in linear discriminant analysis that identifies subsets of variables that minimize Wilks' @L. An important feature of this algorithm is a variable reordering scheme that greatly reduces computation time. We also present an approximate procedure based on tabu search, which can be implemented for a variety of objective criteria designed for either the descriptive or allocation goals associated with discriminant analysis. The tabu search heuristic is especially useful for maximizing the hit ratio (i.e., the percentage of correctly classified cases). Computational results for the proposed methods are provided for two data sets from the literature.",2011,1609827,"Michael J. Brusco,Douglas Steinley",0
1609828,"Supervised multidimensional scaling for visualization, classification, and bipartite ranking.","Least squares multidimensional scaling (MDS) is a classical method for representing a nxn dissimilarity matrix D. One seeks a set of configuration points z""1,...,z""n@?R^S such that D is well approximated by the Euclidean distances between the configuration points: D""i""j~@?z""i-z""j@?""2. Suppose that in addition to D, a vector of associated binary class labels y@?{1,2}^n corresponding to the n observations is available. We propose an extension to MDS that incorporates this outcome vector. Our proposal, supervised multidimensional scaling (SMDS), seeks a set of configuration points z""1,...,z""n@?R^S such that D""i""j~@?z""i-z""j@?""2, and such that z""i""s>z""j""s for s=1,...,S tends to occur when y""i>y""j. This results in a new way to visualize the observations. In addition, we show that SMDS leads to a method for the classification of test observations, which can also be interpreted as a solution to the bipartite ranking problem. This method is explored in a simulation study, as well as on a prostate cancer gene expression data set and on a handwritten digits data set.",2011,1609828,"Daniela M. Witten,Robert Tibshirani",0
1609829,A unified Bayesian inference on treatment means with order constraints.,"In some applications involving comparison of treatment means, it is known a priori that population means are ordered in a certain way. In such situations, imposing constraints on the treatment means can greatly increase the effectiveness of statistical procedures. This paper proposes a unified Bayesian method which performs a simultaneous comparison of treatment means and parameter estimation in ANOVA models with order constraints on the means. A continuous prior restricted to order constraints is employed, and posterior samples of parameters are generated using a Markov chain Monte Carlo method. Posterior probabilities of all possible hypotheses on the equality/inequality of treatment means are obtained using Savage-Dickey density ratios, for which we propose a simple and computationally efficient estimation method. Posterior densities and HPD intervals of parameters of interest are estimated with almost no extra cost, given some by-products from the test procedure. Simulation study results show that the proposed method outperforms the test without constraints and that the method is powerful in detecting the true hypothesis. The method is applied to the ramus bone sizes of 20 boys, which were measured at four time points. The proposed Bayesian test reveals that there are two growth spurts in the ramus bone size during the observed period, which could not be detected by pairwise comparisons of the means.",2011,1609829,"Man-Suk Oh,Dong Wan Shin",0
1609830,Post-stratified calibration method for estimating quantiles.,"The estimation of quantiles in the presence of auxiliary information is discussed. Calibration and poststratification techniques provide simple and practical procedures for incorporating auxiliary information into the estimation of distribution functions, which can offer some useful gains in efficiency. The estimator proposed combines these techniques and possesses a number of desirable properties, including yielding a genuine distribution function, providing simplicity of computation and generalizing Silva and Skinner's estimator. This proposed procedure is compared to alternative methods. On the basis of simulation studies, the proposed post-stratified calibration estimator presents a good level of performance and comprises a valid alternative to other estimators of the distribution function.",2011,1609830,"S. Martínez,María del Mar Rueda,Antonio Arcos,H. Martínez,I. Sánchez-Borrego",0
1609831,Optimal allocation of change points in simple step-stress experiments under Type-II censoring.,"In simple step-stress experiments under Type-II censoring with the cumulative exposure model and exponentially distributed lifetimes, maximum likelihood estimates (MLE) of the expected lifetimes may not exist due to the absence of failure times either before or after the stress change point. For this reason, when planning a step-stress experiment, the change point could be chosen so as to minimize the probability of non-existence of the MLE. These non-existence probabilities are examined and compared in the one- as well as the two-sample situations. Moreover, the optimal allocations of the change points are discussed and the effects of the use of non-optimal choices for the change points are assessed.",2011,1609831,"Maria Kateri,Udo Kamps,Narayanaswamy Balakrishnan",0
1609832,Generalized weighted likelihood density estimators with application to finite mixture of exponential family distributions.,"The family of weighted likelihood estimators largely overlaps with minimum divergence estimators. They are robust to data contaminations compared to MLE. We define the class of generalized weighted likelihood estimators (GWLE), provide its influence function and discuss the efficiency requirements. We introduce a new truncated cubic-inverse weight, which is both first and second order efficient and more robust than previously reported weights. We also discuss new ways of selecting the smoothing bandwidth and weighted starting values for the iterative algorithm. The advantage of the truncated cubic-inverse weight is illustrated in a simulation study of three-component normal mixtures model with large overlaps and heavy contaminations. A real data example is also provided.",2011,1609832,"Tingting Zhan,Inna Chevoneva,Boris Iglewicz",0
1609833,Feature selection in the Laplacian support vector machine.,"Traditional classifiers including support vector machines use only labeled data in training. However, labeled instances are often difficult, costly, or time consuming to obtain while unlabeled instances are relatively easy to collect. The goal of semi-supervised learning is to improve the classification accuracy by using unlabeled data together with a few labeled data in training classifiers. Recently, the Laplacian support vector machine has been proposed as an extension of the support vector machine to semi-supervised learning. The Laplacian support vector machine has drawbacks in its interpretability as the support vector machine has. Also it performs poorly when there are many non-informative features in the training data because the final classifier is expressed as a linear combination of informative as well as non-informative features. We introduce a variant of the Laplacian support vector machine that is capable of feature selection based on functional analysis of variance decomposition. Through synthetic and benchmark data analysis, we illustrate that our method can be a useful tool in semi-supervised learning.",2011,1609833,"Sangjun Lee,Changyi Park,Ja-Yong Koo",0
1609834,Derivation of centralized and distributed filters using covariance information.,"The problem of estimating a degraded image using observations acquired from multiple sensors is addressed when the image degradation is modelled by white multiplicative and additive noise. Assuming the state-space model is unknown, the centralized and distributed filtering algorithms are derived using the information provided by the covariance functions of the processes involved in the measurement equation. The filters obtained are applied to an image affected by multiplicative and additive noise, and the goodness of the centralized and distributed filters is compared by examining the respective filtering error variances.",2011,1609834,"María J. García-Ligero,Aurora Hermoso-Carazo,Josefa Linares-Pérez",0
1609835,Alternating imputation posterior estimation of models with crossed random effects.,"Generalized linear mixed models or latent variable models for categorical data are difficult to estimate if the random effects or latent variables vary at non-nested levels, such as persons and test items. Clayton and Rasbash (1999) suggested an Alternating Imputation Posterior (AIP) algorithm for approximate maximum likelihood estimation. For item response models with random item effects, the algorithm iterates between an item wing in which the item mean and variance are estimated for given person effects and a person wing in which the person mean and variance are estimated for given item effects. The person effects used for the item wing are sampled from the conditional posterior distribution estimated in the person wing and vice versa. Clayton and Rasbash (1999) used marginal quasi-likelihood (MQL) and penalized quasi-likelihood (PQL) estimation within the AIP algorithm, but this method has been shown to produce biased estimates in many situations, so we use maximum likelihood estimation with adaptive quadrature. We apply the proposed algorithm to the famous salamander mating data, comparing the estimates with many other methods, and to an educational testing dataset. We also present a simulation study to assess performance of the AIP algorithm and the Laplace approximation with different numbers of items and persons and a range of item and person variances.",2011,1609835,"S.-J. Cho,S. Rabe-Hesketh",0
1609836,Semiparametric regression analysis of panel count data with informative observation times.,"This paper discusses regression analysis of panel count data that arise naturally when recurrent events are considered. For the analysis of panel count data, most of the existing methods have assumed that observation times are completely independent of recurrent events or given covariates, which may not be true in practice. We propose a joint modeling approach that uses an unobserved random variable and a completely unspecified link function to characterize the correlations between the response variable and the observation times. For inference about regression parameters, estimating equation approaches are developed without involving any estimation for latent variables, and the asymptotic properties of the resulting estimators are established. In addition, a technique is provided for assessing the adequacy of the model. The performance of the proposed estimation procedures are evaluated by means of Monte Carlo simulations, and a data set from a bladder tumor study is analyzed as an illustrative example.",2011,1609836,"Xingqiu Zhao,Xingwei Tong",0
1609837,MCMC-based estimation methods for continuous longitudinal data with non-random (non)-monotone missingness.,"The analysis of incomplete longitudinal data requires joint modeling of the longitudinal outcomes (observed and unobserved) and the response indicators. When non-response does not depend on the unobserved outcomes, within a likelihood framework, the missingness is said to be ignorable, obviating the need to formally model the process that drives it. For the non-ignorable or non-random case, estimation is less straightforward, because one must work with the observed data likelihood, which involves integration over the missing values, thereby giving rise to computational complexity, especially for high-dimensional missingness. The stochastic EM algorithm is a variation of the expectation-maximization (EM) algorithm and is particularly useful in cases where the E (expectation) step is intractable. Under the stochastic EM algorithm, the E-step is replaced by an S-step, in which the missing data are simulated from an appropriate conditional distribution. The method is appealing due to its computational simplicity. The SEM algorithm is used to fit non-random models for continuous longitudinal data with monotone or non-monotone missingness, using simulated, as well as case study, data. Resulting SEM estimates are compared with their direct likelihood counterparts wherever possible.",2011,1609837,"Cristina Sotto,Caroline Beunckens,Geert Molenberghs,Michael G. Kenward",0
1609838,Robust accelerated failure time regression.,"Robust estimators for accelerated failure time models with asymmetric (or symmetric) error distribution and censored observations are proposed. It is assumed that the error model belongs to a log-location-scale family of distributions and that the mean response is the parameter of interest. Since scale is a main component of mean, scale is not treated as a nuisance parameter. A three steps procedure is proposed. In the first step, an initial high breakdown point S estimate is computed. In the second step, observations that are unlikely under the estimated model are rejected or down weighted. Finally, a weighted maximum likelihood estimate is computed. To define the estimates, functions of censored residuals are replaced by their estimated conditional expectation given that the response is larger than the observed censored value. The rejection rule in the second step is based on an adaptive cut-off that, asymptotically, does not reject any observation when the data are generated according to the model. Therefore, the final estimate attains full efficiency at the model, with respect to the maximum likelihood estimate, while maintaining the breakdown point of the initial estimator. Asymptotic results are provided. The new procedure is evaluated with the help of Monte Carlo simulations. Two examples with real data are discussed.",2011,1609838,"Isabella Locatelli,Alfio Marazzi,Victor J. Yohai",0
